---
title: "7个流行的强化学习算法及代码实现！"
date: 2023-02-02T10:33:16Z
draft: ["false"]
tags: [
  "fetched",
  "法纳斯特"
]
categories: ["Acdemic"]
---
7个流行的强化学习算法及代码实现！ by 法纳斯特
------
<div><p data-mpa-powered-by="yiban.io"><img data-copyright="0" data-ratio="0.16245006657789615" data-src="https://mmbiz.qpic.cn/mmbiz_gif/y0SBuxfLhaly8za5kQolL29ibZEM13gCSgBzGEYrqmlAjARVibQEMlGVQLmnNBUSamVczvRPz9sSRmiatSPsYu02g/640?wx_fmt=gif" data-type="gif" data-w="751" src="https://mmbiz.qpic.cn/mmbiz_gif/y0SBuxfLhaly8za5kQolL29ibZEM13gCSgBzGEYrqmlAjARVibQEMlGVQLmnNBUSamVczvRPz9sSRmiatSPsYu02g/640?wx_fmt=gif"><br></p><section><br></section><p><span>作者<span>丨</span>Siddhartha Pramanik </span></p><p><span>来源丨Deephub Imba</span></p><p><span md-inline="plain"></span></p><p cid="n2" mdtype="paragraph"><span md-inline="plain">大家好，我是小F～</span></p><p cid="n2" mdtype="paragraph"><span md-inline="plain">目前流行的强化学习算法包括 Q-learning、SARSA、DDPG、A2C、PPO、DQN 和 TRPO。这些算法已被用于在游戏、机器人和决策制定等各种应用中，并且这些流行的算法还在不断发展和改进，本文我们将对其做一个简单的介绍。</span></p><p><img data-ratio="0.5227272727272727" data-s="300,640" data-type="png" data-w="1100" data-src="https://mmbiz.qpic.cn/mmbiz_png/6wQyVOrkRNLia6E8LzSQWYVwFtoJzeA6jIL9rTkC6WBbWADtjrKgHCc1w8bQ9icZiaTC3kw4sdsgia3MaiaCj3LjaZA/640?wx_fmt=png" src="https://mmbiz.qpic.cn/mmbiz_png/6wQyVOrkRNLia6E8LzSQWYVwFtoJzeA6jIL9rTkC6WBbWADtjrKgHCc1w8bQ9icZiaTC3kw4sdsgia3MaiaCj3LjaZA/640?wx_fmt=png"></p><h2 cid="n7" mdtype="heading"><span md-inline="plain">1、Q-learning</span></h2><p cid="n4" mdtype="paragraph"><span md-inline="plain">Q-learning：Q-learning 是一种无模型、非策略的强化学习算法。它使用 Bellman 方程估计最佳动作值函数，该方程迭代地更新给定状态动作对的估计值。Q-learning 以其简单性和处理大型连续状态空间的能力而闻名。</span></p><p cid="n9" mdtype="paragraph"><span md-inline="plain">下面是一个使用 Python 实现 Q-learning 的简单示例：</span></p><section><pre><code><span>import</span> numpy <span>as</span> np<br><br><span># Define the Q-table and the learning rate</span><br>Q = np.zeros((state_space_size, action_space_size))<br>alpha = <span>0.1</span><br><br><span># Define the exploration rate and discount factor</span><br>epsilon = <span>0.1</span><br>gamma = <span>0.99</span><br><br><span>for</span> episode <span>in</span> range(num_episodes):<br>    current_state = initial_state<br>    <span>while</span> <span>not</span> done:<br>        <span># Choose an action using an epsilon-greedy policy</span><br>        <span>if</span> np.random.uniform(<span>0</span>, <span>1</span>) &lt; epsilon:<br>            action = np.random.randint(<span>0</span>, action_space_size)<br>        <span>else</span>:<br>            action = np.argmax(Q[current_state])<br><br>        <span># Take the action and observe the next state and reward</span><br>        next_state, reward, done = take_action(current_state, action)<br><br>        <span># Update the Q-table using the Bellman equation</span><br>        Q[current_state, action] = Q[current_state, action] + alpha * (<br>                reward + gamma * np.max(Q[next_state]) - Q[current_state, action])<br><br>        current_state = next_state<br></code></pre></section><p cid="n9" mdtype="paragraph"><span>上面的示例中，state_space_size 和 actio</span><span>n_space_siz</span><span>e 分别是环境中的状态数和动作数。</span><span>num_episodes 是要为运行算法的轮次数。</span><span>initial_state 是环境的起始状态。</span><span>take_action(current_state, action) 是一个函数，它将当前状态和一个动作作为输入，并返回下一个状态、奖励和一个指示轮次是否完成的布尔值。</span></p><p cid="n43" mdtype="paragraph"><span md-inline="plain">在 while 循环中，使用 epsilon-greedy 策略根据当前状态选择一个动作。使用概率 epsilon选择一个随机动作，使用概率 1-epsilon选择对当前状态具有最高 Q 值的动作。</span></p><p cid="n44" mdtype="paragraph"><span md-inline="plain">采取行动后，观察下一个状态和奖励，使用Bellman方程更新q。并将当前状态更新为下一个状态。这只是 Q-learning 的一个简单示例，并未考虑 Q-table 的初始化和要解决的问题的具体细节。</span></p><h2 cid="n15" mdtype="heading"><span md-inline="plain">2、SARSA</span></h2><p cid="n17" mdtype="paragraph"><span md-inline="plain"> SARSA：SARSA 是一种无模型、基于策略的强化学习算法。它也使用Bellman方程来估计动作价值函数，但它是基于下一个动作的期望值，而不是像 Q-learning 中的最优动作。SARSA 以其处理随机动力学问题的能力而闻名。</span></p><section><pre><code><span>import</span> numpy <span>as</span> np<br><br><span># Define the Q-table and the learning rate</span><br>Q = np.zeros((state_space_size, action_space_size))<br>alpha = <span>0.1</span><br><br><span># Define the exploration rate and discount factor</span><br>epsilon = <span>0.1</span><br>gamma = <span>0.99</span><br><br><span>for</span> episode <span>in</span> range(num_episodes):<br>    current_state = initial_state<br>    action = epsilon_greedy_policy(epsilon, Q, current_state)<br>    <span>while</span> <span>not</span> done:<br>        <span># Take the action and observe the next state and reward</span><br>        next_state, reward, done = take_action(current_state, action)<br>        <span># Choose next action using epsilon-greedy policy</span><br>        next_action = epsilon_greedy_policy(epsilon, Q, next_state)<br>        <span># Update the Q-table using the Bellman equation</span><br>        Q[current_state, action] = Q[current_state, action] + alpha * (<br>                reward + gamma * Q[next_state, next_action] - Q[current_state, action])<br>        current_state = next_state<br>        action = next_action<span></span></code></pre></section><p cid="n17" mdtype="paragraph"><span>state_space_size</span><span>和action_space_size分别是环境中的状态和操作的数量。</span><span>num_episodes是您想要运行SARSA算法的轮次数。</span><span>Initial_state是环境的初始状态。</span><span>take_action(current_state, action)是一个将当前状态和作为操作输入的函数，并返回下一个状态、奖励和一个指示情节是否完成的布尔值。</span><br></p><p cid="n51" mdtype="paragraph"><span md-inline="plain">在while循环中，使用在单独的函数epsilon_greedy_policy(epsilon, Q, current_state)中定义的epsilon-greedy策略来根据当前状态选择操作。使用概率 epsilon选择一个随机动作，使用概率 1-epsilon对当前状态具有最高 Q 值的动作。</span></p><p cid="n52" mdtype="paragraph"><span md-inline="plain">上面与Q-learning相同，但是采取了一个行动后，在观察下一个状态和奖励时它然后使用贪心策略选择下一个行动。并使用Bellman方程更新q表。</span></p><h2 cid="n53" mdtype="heading"><span md-inline="plain">3、DDPG</span></h2><p cid="n23" mdtype="paragraph"><span md-inline="plain">DDPG 是一种用于连续动作空间的无模型、非策略算法。它是一种actor-critic算法，其中actor网络用于选择动作，而critic网络用于评估动作。DDPG 对于机器人控制和其他连续控制任务特别有用。</span></p><section><pre><code><span>import</span> numpy <span>as</span> np<br><span>from</span> keras.models <span>import</span> Model, Sequential<br><span>from</span> keras.layers <span>import</span> Dense, Input<br><span>from</span> keras.optimizers <span>import</span> Adam<br><br><span># Define the actor and critic models</span><br>actor = Sequential()<br>actor.add(Dense(<span>32</span>, input_dim=state_space_size, activation=<span>'relu'</span>))<br>actor.add(Dense(<span>32</span>, activation=<span>'relu'</span>))<br>actor.add(Dense(action_space_size, activation=<span>'tanh'</span>))<br>actor.compile(loss=<span>'mse'</span>, optimizer=Adam(lr=<span>0.001</span>))<br><br>critic = Sequential()<br>critic.add(Dense(<span>32</span>, input_dim=state_space_size, activation=<span>'relu'</span>))<br>critic.add(Dense(<span>32</span>, activation=<span>'relu'</span>))<br>critic.add(Dense(<span>1</span>, activation=<span>'linear'</span>))<br>critic.compile(loss=<span>'mse'</span>, optimizer=Adam(lr=<span>0.001</span>))<br><br><span># Define the replay buffer</span><br>replay_buffer = []<br><br><span># Define the exploration noise</span><br>exploration_noise = OrnsteinUhlenbeckProcess(size=action_space_size, theta=<span>0.15</span>, mu=<span>0</span>, sigma=<span>0.2</span>)<br><br><span>for</span> episode <span>in</span> range(num_episodes):<br>    current_state = initial_state<br>    <span>while</span> <span>not</span> done:<br>        <span># Select an action using the actor model and add exploration noise</span><br>        action = actor.predict(current_state)[<span>0</span>] + exploration_noise.sample()<br>        action = np.clip(action, <span>-1</span>, <span>1</span>)<br><br>        <span># Take the action and observe the next state and reward</span><br>        next_state, reward, done = take_action(current_state, action)<br><br>        <span># Add the experience to the replay buffer</span><br>        replay_buffer.append((current_state, action, reward, next_state, done))<br><br>        <span># Sample a batch of experiences from the replay buffer</span><br>        batch = sample(replay_buffer, batch_size)<br><br>        <span># Update the critic model</span><br>        states = np.array([x[<span>0</span>] <span>for</span> x <span>in</span> batch])<br>        actions = np.array([x[<span>1</span>] <span>for</span> x <span>in</span> batch])<br>        rewards = np.array([x[<span>2</span>] <span>for</span> x <span>in</span> batch])<br>        next_states = np.array([x[<span>3</span>] <span>for</span> x <span>in</span> batch])<br><br>        target_q_values = rewards + gamma * critic.predict(next_states)<br>        critic.train_on_batch(states, target_q_values)<br><br>        <span># Update the actor model</span><br>        action_gradients = np.array(critic.get_gradients(states, actions))<br>        actor.train_on_batch(states, action_gradients)<br><br>        current_state = next_state<br></code></pre></section><p cid="n23" mdtype="paragraph"><span>在本例中，state_space_size和action_space_size分别是环境中的状态和操作的数量。</span><span>num_episodes是轮次数。</span><span>Initial_state是环境的初始状态。</span><span>Take_action (current_state, action)是一个函数，它接受当前状态和操作作为输入，并返回下一个操作。</span></p><h2 cid="n29" mdtype="heading"><span md-inline="plain">4、A2C</span></h2><p cid="n31" mdtype="paragraph"><span md-inline="plain">A2C（Advantage Actor-Critic）是一种有策略的actor-critic算法，它使用Advantage函数来更新策略。该算法实现简单，可以处理离散和连续的动作空间。</span></p><section><pre><code><span>import</span> numpy <span>as</span> np<br><span>from</span> keras.models <span>import</span> Model, Sequential<br><span>from</span> keras.layers <span>import</span> Dense, Input<br><span>from</span> keras.optimizers <span>import</span> Adam<br><span>from</span> keras.utils <span>import</span> to_categorical<br><br><span># Define the actor and critic models</span><br>state_input = Input(shape=(state_space_size,))<br>actor = Dense(<span>32</span>, activation=<span>'relu'</span>)(state_input)<br>actor = Dense(<span>32</span>, activation=<span>'relu'</span>)(actor)<br>actor = Dense(action_space_size, activation=<span>'softmax'</span>)(actor)<br>actor_model = Model(inputs=state_input, outputs=actor)<br>actor_model.compile(loss=<span>'categorical_crossentropy'</span>, optimizer=Adam(lr=<span>0.001</span>))<br><br>state_input = Input(shape=(state_space_size,))<br>critic = Dense(<span>32</span>, activation=<span>'relu'</span>)(state_input)<br>critic = Dense(<span>32</span>, activation=<span>'relu'</span>)(critic)<br>critic = Dense(<span>1</span>, activation=<span>'linear'</span>)(critic)<br>critic_model = Model(inputs=state_input, outputs=critic)<br>critic_model.compile(loss=<span>'mse'</span>, optimizer=Adam(lr=<span>0.001</span>))<br><br><span>for</span> episode <span>in</span> range(num_episodes):<br>    current_state = initial_state<br>    done = <span>False</span><br>    <span>while</span> <span>not</span> done:<br>        <span># Select an action using the actor model and add exploration noise</span><br>        action_probs = actor_model.predict(np.array([current_state]))[<span>0</span>]<br>        action = np.random.choice(range(action_space_size), p=action_probs)<br><br>        <span># Take the action and observe the next state and reward</span><br>        next_state, reward, done = take_action(current_state, action)<br><br>        <span># Calculate the advantage</span><br>        target_value = critic_model.predict(np.array([next_state]))[<span>0</span>][<span>0</span>]<br>        advantage = reward + gamma * target_value - critic_model.predict(np.array([current_state]))[<span>0</span>][<span>0</span>]<br><br>        <span># Update the actor model</span><br>        action_one_hot = to_categorical(action, action_space_size)<br>        actor_model.train_on_batch(np.array([current_state]), advantage * action_one_hot)<br><br>        <span># Update the critic model</span><br>        critic_model.train_on_batch(np.array([current_state]), reward + gamma * target_value)<br><br>        current_state = next_state<br></code></pre></section><p cid="n31" mdtype="paragraph"><span>在这个例子中，actor模型是一个神经网络，它有2个隐藏层，每个隐藏层有32个神经元，具有relu激活函数，输出层具有softmax激活函数。</span><span>critic模型也是一个神经网络，它有2个隐含层，每层32个神经元，具有relu激活函数，输出层具有线性激活函数。</span></p><p cid="n84" mdtype="paragraph"><span md-inline="plain">使用分类交叉熵损失函数训练actor模型，使用均方误差损失函数训练critic模型。动作是根据actor模型预测选择的，并添加了用于探索的噪声。</span></p><h2 cid="n72" mdtype="heading"><span md-inline="plain">5、PPO</span></h2><p cid="n74" mdtype="paragraph"><span md-inline="plain">PPO（Proximal Policy Optimization）是一种策略算法，它使用信任域优化的方法来更新策略。它在具有高维观察和连续动作空间的环境中特别有用。PPO 以其稳定性和高样品效率而著称。</span></p><section><pre><code><span>import</span> numpy <span>as</span> np<br><span>from</span> keras.models <span>import</span> Model, Sequential<br><span>from</span> keras.layers <span>import</span> Dense, Input<br><span>from</span> keras.optimizers <span>import</span> Adam<br><br><span># Define the policy model</span><br>state_input = Input(shape=(state_space_size,))<br>policy = Dense(<span>32</span>, activation=<span>'relu'</span>)(state_input)<br>policy = Dense(<span>32</span>, activation=<span>'relu'</span>)(policy)<br>policy = Dense(action_space_size, activation=<span>'softmax'</span>)(policy)<br>policy_model = Model(inputs=state_input, outputs=policy)<br><br><span># Define the value model</span><br>value_model = Model(inputs=state_input, outputs=Dense(<span>1</span>, activation=<span>'linear'</span>)(policy))<br><br><span># Define the optimizer</span><br>optimizer = Adam(lr=<span>0.001</span>)<br><br><span>for</span> episode <span>in</span> range(num_episodes):<br>    current_state = initial_state<br>    <span>while</span> <span>not</span> done:<br>        <span># Select an action using the policy model</span><br>        action_probs = policy_model.predict(np.array([current_state]))[<span>0</span>]<br>        action = np.random.choice(range(action_space_size), p=action_probs)<br><br>        <span># Take the action and observe the next state and reward</span><br>        next_state, reward, done = take_action(current_state, action)<br><br>        <span># Calculate the advantage</span><br>        target_value = value_model.predict(np.array([next_state]))[<span>0</span>][<span>0</span>]<br>        advantage = reward + gamma * target_value - value_model.predict(np.array([current_state]))[<span>0</span>][<span>0</span>]<br><br>        <span># Calculate the old and new policy probabilities</span><br>        old_policy_prob = action_probs[action]<br>        new_policy_prob = policy_model.predict(np.array([next_state]))[<span>0</span>][action]<br><br>        <span># Calculate the ratio and the surrogate loss</span><br>        ratio = new_policy_prob / old_policy_prob<br>        surrogate_loss = np.minimum(ratio * advantage, np.clip(ratio, <span>1</span> - epsilon, <span>1</span> + epsilon) * advantage)<br><br>        <span># Update the policy and value models</span><br>        policy_model.trainable_weights = value_model.trainable_weights<br>        policy_model.compile(optimizer=optimizer, loss=-surrogate_loss)<br>        policy_model.train_on_batch(np.array([current_state]), np.array([action_one_hot]))<br>        value_model.train_on_batch(np.array([current_state]), reward + gamma * target_value)<br><br>        current_state = next_state</code></pre></section><h2 cid="n80" mdtype="heading"><span md-inline="plain">6、DQN</span></h2><p cid="n92" mdtype="paragraph"><span md-inline="plain">DQN（深度 Q 网络）是一种无模型、非策略算法，它使用神经网络来逼近 Q 函数。DQN 特别适用于 Atari 游戏和其他类似问题，其中状态空间是高维的，并使用神经网络近似 Q 函数。</span></p><section><pre><code><span>import</span> numpy <span>as</span> np<br><span>from</span> keras.models <span>import</span> Sequential<br><span>from</span> keras.layers <span>import</span> Dense, Input<br><span>from</span> keras.optimizers <span>import</span> Adam<br><span>from</span> collections <span>import</span> deque<br><br><span># Define the Q-network model</span><br>model = Sequential()<br>model.add(Dense(<span>32</span>, input_dim=state_space_size, activation=<span>'relu'</span>))<br>model.add(Dense(<span>32</span>, activation=<span>'relu'</span>))<br>model.add(Dense(action_space_size, activation=<span>'linear'</span>))<br>model.compile(loss=<span>'mse'</span>, optimizer=Adam(lr=<span>0.001</span>))<br><br><span># Define the replay buffer</span><br>replay_buffer = deque(maxlen=replay_buffer_size)<br><br><span>for</span> episode <span>in</span> range(num_episodes):<br>    current_state = initial_state<br>    <span>while</span> <span>not</span> done:<br>        <span># Select an action using an epsilon-greedy policy</span><br>        <span>if</span> np.random.rand() &lt; epsilon:<br>            action = np.random.randint(<span>0</span>, action_space_size)<br>        <span>else</span>:<br>            action = np.argmax(model.predict(np.array([current_state]))[<span>0</span>])<br><br>        <span># Take the action and observe the next state and reward</span><br>        next_state, reward, done = take_action(current_state, action)<br><br>        <span># Add the experience to the replay buffer</span><br>        replay_buffer.append((current_state, action, reward, next_state, done))<br><br>        <span># Sample a batch of experiences from the replay buffer</span><br>        batch = random.sample(replay_buffer, batch_size)<br><br>        <span># Prepare the inputs and targets for the Q-network</span><br>        inputs = np.array([x[<span>0</span>] <span>for</span> x <span>in</span> batch])<br>        targets = model.predict(inputs)<br>        <span>for</span> i, (state, action, reward, next_state, done) <span>in</span> enumerate(batch):<br>            <span>if</span> done:<br>                targets[i, action] = reward<br>            <span>else</span>:<br>                targets[i, action] = reward + gamma * np.max(model.predict(np.array([next_state]))[<span>0</span>])<br><br>        <span># Update the Q-network</span><br>        model.train_on_batch(inputs, targets)<br><br>        current_state = next_state<br></code></pre></section><p cid="n92" mdtype="paragraph"><span>上面的代码，Q-network有2个隐藏层，每个隐藏层有32个神经元，使用relu激活函数。</span><span>该网络使用均方误差损失函数和Adam优化器进行训练。</span></p><h2 cid="n96" mdtype="heading"><span md-inline="plain">7、TRPO</span></h2><p cid="n100" mdtype="paragraph"><span md-inline="plain">TRPO （Trust Region Policy Optimization）是一种无模型的策略算法，它使用信任域优化方法来更新策略。它在具有高维观察和连续动作空间的环境中特别有用。</span></p><p cid="n103" mdtype="paragraph"><span md-inline="plain">TRPO 是一个复杂的算法，需要多个步骤和组件来实现。TRPO不是用几行代码就能实现的简单算法。</span></p><p cid="n105" mdtype="paragraph"><span md-inline="plain">所以我们这里使用实现了TRPO的现有库，例如OpenAI Baselines，它提供了包括TRPO在内的各种预先实现的强化学习算法，。</span></p><p cid="n106" mdtype="paragraph"><span md-inline="plain">要在OpenAI Baselines中使用TRPO，我们需要安装:</span></p><section><pre><code>pip install baselines<br></code></pre></section><p cid="n106" mdtype="paragraph"><span>然后</span><span>可以使用baselines库中的trpo_mpi模块在你的环境中训练TRPO代理，这里有一个简单的例子:</span></p><section><pre><code><span>import</span> gym<br><span>from</span> baselines.common.vec_env.dummy_vec_env <span>import</span> DummyVecEnv<br><span>from</span> baselines.trpo_mpi <span>import</span> trpo_mpi<br><br><span># Initialize the environment</span><br>env = gym.make(<span>"CartPole-v1"</span>)<br>env = DummyVecEnv([<span>lambda</span>: env])<br><br><span># Define the policy network</span><br>policy_fn = mlp_policy<br><br><span># Train the TRPO model</span><br>model = trpo_mpi.learn(env, policy_fn, max_iters=<span>1000</span>)<span></span></code></pre></section><p cid="n106" mdtype="paragraph"><span>我们使用Gym库初始化环境。</span><span>然后定义策略网络，并调用TRPO模块中的learn()函数来训练模型。</span></p><p cid="n116" mdtype="paragraph"><span md-inline="plain">还有许多其他库也提供了TRPO的实现，例如TensorFlow、PyTorch和RLLib。下面时一个使用TF 2.0实现的样例</span></p><section><pre><code><span>import</span> tensorflow <span>as</span> tf<br><span>import</span> gym<br><br><br><span># Define the policy network</span><br><span><span>class</span> <span>PolicyNetwork</span><span>(tf.keras.Model)</span>:</span><br>    <span><span>def</span> <span>__init__</span><span>(self)</span>:</span><br>        super(PolicyNetwork, self).__init__()<br>        self.dense1 = tf.keras.layers.Dense(<span>16</span>, activation=<span>'relu'</span>)<br>        self.dense2 = tf.keras.layers.Dense(<span>16</span>, activation=<span>'relu'</span>)<br>        self.dense3 = tf.keras.layers.Dense(<span>1</span>, activation=<span>'sigmoid'</span>)<br><br>    <span><span>def</span> <span>call</span><span>(self, inputs)</span>:</span><br>        x = self.dense1(inputs)<br>        x = self.dense2(x)<br>        x = self.dense3(x)<br>        <span>return</span> x<br><br><br><span># Initialize the environment</span><br>env = gym.make(<span>"CartPole-v1"</span>)<br><br><span># Initialize the policy network</span><br>policy_network = PolicyNetwork()<br><br><span># Define the optimizer</span><br>optimizer = tf.optimizers.Adam()<br><br><span># Define the loss function</span><br>loss_fn = tf.losses.BinaryCrossentropy()<br><br><span># Set the maximum number of iterations</span><br>max_iters = <span>1000</span><br><br><span># Start the training loop</span><br><span>for</span> i <span>in</span> range(max_iters):<br>    <span># Sample an action from the policy network</span><br>    action = tf.squeeze(tf.random.categorical(policy_network(observation), <span>1</span>))<br><br>    <span># Take a step in the environment</span><br>    observation, reward, done, _ = env.step(action)<br><br>    <span>with</span> tf.GradientTape() <span>as</span> tape:<br>        <span># Compute the loss</span><br>        loss = loss_fn(reward, policy_network(observation))<br><br>    <span># Compute the gradients</span><br>    grads = tape.gradient(loss, policy_network.trainable_variables)<br><br>    <span># Perform the update step</span><br>    optimizer.apply_gradients(zip(grads, policy_network.trainable_variables))<br><br>    <span>if</span> done:<br>        <span># Reset the environment</span><br>        observation = env.reset()<br></code></pre></section><p cid="n116" mdtype="paragraph"><span>在这个例子中，我们首先使用TensorFlow的Keras API定义一个策略网络。</span><span>然后使用Gym库和策略网络初始化环境。</span><span>然后定义用于训练策略网络的优化器和损失函数。</span></p><p cid="n131" mdtype="paragraph"><span md-inline="plain">在训练循环中，从策略网络中采样一个动作，在环境中前进一步，然后使用TensorFlow的GradientTape计算损失和梯度。然后我们使用优化器执行更新步骤。</span></p><p cid="n132" mdtype="paragraph"><span md-inline="plain">这是一个简单的例子，只展示了如何在TensorFlow 2.0中实现TRPO。TRPO是一个非常复杂的算法，这个例子没有涵盖所有的细节，但它是试验TRPO的一个很好的起点。</span></p><h2 cid="n117" mdtype="heading"><span md-inline="plain">总结</span></h2><p cid="n133" mdtype="paragraph"><span md-inline="plain">以上就是我们总结的7个常用的强化学习算法，这些算法并不相互排斥，通常与其他技术(如值函数逼近、基于模型的方法和集成方法)结合使用，可以获得更好的结果。</span><span></span></p><section><strong><span><strong><span>万水千山总是情，点个 👍 行不行</span></strong>。</span></strong></section><section><br></section><section><br></section><section><span><strong>推荐阅读</strong></span></section><section><br></section><section><a href="http://mp.weixin.qq.com/s?__biz=MzU4OTYzNjE2OQ==&amp;mid=2247484115&amp;idx=1&amp;sn=447d90d9e9d17b963c6dc76f7d691735&amp;chksm=fdcb35f5cabcbce3bb4318fa7271a367a2beca95e6aaa02670a61844c6497b6aa2a258588b90&amp;scene=21#wechat_redirect" target="_blank" data-linktype="1"><span data-positionback="static"><img data-copyright="0" data-ratio="0.36171875" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhamlEIJicJlZiaLAHOaiaykJXY28yiangUibwP8pQia0dVVRnyBEcB8NEeCgbtzSbGiaIhwlmSWVNAbQNkPPw/640?wx_fmt=jpeg" data-type="jpeg" data-w="1280" src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhamlEIJicJlZiaLAHOaiaykJXY28yiangUibwP8pQia0dVVRnyBEcB8NEeCgbtzSbGiaIhwlmSWVNAbQNkPPw/640?wx_fmt=jpeg"></span></a></section><section><a href="http://mp.weixin.qq.com/s?__biz=MzU4OTYzNjE2OQ==&amp;mid=2247484095&amp;idx=1&amp;sn=a284ba324550829c969fa2cda0da723c&amp;chksm=fdcb3599cabcbc8f96df521eb5ce50910da152de627e47a69b26e9f2da5920dc6afe4ad2642f&amp;scene=21#wechat_redirect" target="_blank" data-linktype="1"><span data-positionback="static"><img data-copyright="0" data-ratio="0.3613963039014374" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhamlEIJicJlZiaLAHOaiaykJXY2NGWbAsoex55wOX7AaeSCTpaHYibpcv0btGvriaGxNlbHTsIAtVAiceg2w/640?wx_fmt=jpeg" data-type="jpeg" data-w="1948" src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhamlEIJicJlZiaLAHOaiaykJXY2NGWbAsoex55wOX7AaeSCTpaHYibpcv0btGvriaGxNlbHTsIAtVAiceg2w/640?wx_fmt=jpeg"></span></a></section><section><a href="http://mp.weixin.qq.com/s?__biz=MzU4OTYzNjE2OQ==&amp;mid=2247484076&amp;idx=1&amp;sn=fac8b75e2fbdb8eecff16b1faa4dc7f2&amp;chksm=fdcb358acabcbc9c20769d3800f77f7cafb16844486e2c3fd35b0d567923eeac352189cc6285&amp;scene=21#wechat_redirect" target="_blank" data-linktype="1"><span data-positionback="static"><img data-copyright="0" data-ratio="0.3613963039014374" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhakn3PDXd3AZqsndDkz0N8f1aOSbmwb81qkW1ZfyibRVP300ESWyibhjIicWuw5gV7gw3tSz2omeia6Ftw/640?wx_fmt=jpeg" data-type="jpeg" data-w="1948" src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhakn3PDXd3AZqsndDkz0N8f1aOSbmwb81qkW1ZfyibRVP300ESWyibhjIicWuw5gV7gw3tSz2omeia6Ftw/640?wx_fmt=jpeg"></span></a></section><section><br></section><section><strong><span>···  END  ···</span></strong></section><section><br></section><section><img data-copyright="0" data-ratio="0.4618249534450652" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhakhhYibk7icTEGoX7VZ2R8g9CwnITeJvFL2R3IeQIenSuFibD9ibNO6AWXqMpgOWbS05P1vic3tafbP0qQ/640?wx_fmt=jpeg" data-type="jpeg" data-w="1074" src="https://mmbiz.qpic.cn/mmbiz_png/y0SBuxfLhakhhYibk7icTEGoX7VZ2R8g9CwnITeJvFL2R3IeQIenSuFibD9ibNO6AWXqMpgOWbS05P1vic3tafbP0qQ/640?wx_fmt=jpeg"></section><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/DAPirChUTKZ9yLExJw86Tg",target="_blank" rel="noopener noreferrer">原文链接</a>
