---
title: "7.图解机器学习 | 逻辑回归算法详解"
date: 2023-12-16T02:58:09Z
draft: ["false"]
tags: [
  "fetched",
  "生信小知识"
]
categories: ["Acdemic"]
---
7.图解机器学习 | 逻辑回归算法详解 by 生信小知识
------
<div><section><h2><span>7.图解机器学习 | 逻辑回归算法详解</span></h2><blockquote><p>微信公众号：<strong>生信小知识</strong><br>关注可了解更多的生物信息学教程及知识。问题或建议，请公众号留言;</p></blockquote><h3><span>目录</span></h3><p><span><span>前言</span></span><span><span>1.机器学习与分类问题</span></span><span><span><span>1）分类问题</span></span></span><span><span><span>2）分类问题的数学抽象</span></span></span><span><span>2.逻辑回归算法核心思想</span></span><span><span><span>1）线性回归与分类</span></span></span><span><span><span>2）逻辑回归核心思想</span></span></span><span><span>3. Sigmoid 函数与决策边界</span></span><span><span><span>1）分类与决策边界</span></span></span><span><span><span>2）线性决策边界生成</span></span></span><span><span><span>3）非线性决策边界生成</span></span></span><span><span>4.梯度下降与优化</span></span><span><span><span>1）损失函数</span></span></span><span><span><span>2）梯度下降</span></span></span><span><span>5.正则化与缓解过拟合</span></span><span><span><span>1）过拟合现象</span></span></span><span><span><span>2）正则化处理</span></span></span><span><span>6.特征变换与非线性表达</span></span><span><span><span>1）多项式特征</span></span></span><span><span><span>2）非线性切分</span></span></span><span><span>后记</span></span></p><h3><span>前言</span></h3><p>本系列内容均来自 <strong>ShowMeAI</strong> 论坛：https://www.showmeai.tech/</p><p>该论坛中内容详实，部分系列非常适合数学功底不那么好的同学学习，这里仅对 【<strong>图解机器学习</strong>】系列进行学习，并同时整理为笔记。</p><p>过往已发布笔记：</p><ul><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489371&amp;idx=1&amp;sn=8d61e3d8cde6acb55361e4eeade4fbb6&amp;scene=21#wechat_redirect" data-linktype="2">1.图解机器学习 | 机器学习基础知识</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489408&amp;idx=1&amp;sn=159692dc7f34d89293cca28172c2e86b&amp;scene=21#wechat_redirect" data-linktype="2">2.图解机器学习 | 模型评估方法与准则</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489441&amp;idx=1&amp;sn=2e5ae676ade3a10a6b3615ad84ef7735&amp;scene=21#wechat_redirect" data-linktype="2">3.图解机器学习 | 决策树模型详解</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489455&amp;idx=1&amp;sn=903797623f07b3ae01e37b52dbce5159&amp;scene=21#wechat_redirect" data-linktype="2">4.图解机器学习-随机森林分类模型详解</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489473&amp;idx=1&amp;sn=eff20c55c32236c4724aebd1da1ab99a&amp;scene=21#wechat_redirect" data-linktype="2">5.图解机器学习 | 回归树模型详解</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489524&amp;idx=1&amp;sn=b216b9aa17b77da413e5843c2b52f3e6&amp;scene=21#wechat_redirect" data-linktype="2">6.图解机器学习 | GBDT模型详解</a></p></li></ul><p>今天主要介绍<strong>逻辑回归算法的原理知识</strong>。</p><blockquote><p>本公众号已与ShowMeAI论坛负责人联系，并得到授权转载。在次给做机器学习的朋友们推荐下他们的微信公众号，有兴趣的同学可以自行关注。</p></blockquote><h3><span>1.机器学习与分类问题</span></h3><h4><span>1）分类问题</span></h4><p>分类问题是机器学习非常重要的一个组成部分，它的目标是根据已知样本的某些特征，判断一个样本属于哪个类别。分类问题可以细分如下：</p><ul><li><p><strong>二分类问题</strong>：表示分类任务中有<strong>两种类别</strong>新的样本属于哪种已知的样本类。</p></li><li><p><strong>多类分类（Multiclass Classification）问题</strong>：表示分类任务中有<strong>多种类别</strong>。</p></li><li><p><strong>多标签分类（Multilabel Classification）问题</strong>：给<strong>每个样本多种</strong>目标标签。</p></li></ul><p><img data-galleryid="" data-imgfileid="100005878" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczxYutNlMoWQCZiaxsgpwnXG7aHqOnWTXCQpzGTGkdKZO6qhCkic1BGKcw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczxYutNlMoWQCZiaxsgpwnXG7aHqOnWTXCQpzGTGkdKZO6qhCkic1BGKcw/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 机器学习算法分类;</figcaption></figure><h4><span>2）分类问题的数学抽象</span></h4><p>从算法的角度解决一个分类问题，我们的训练数据会被映射成 n 维空间的样本点（这里的 n 就是特征维度），我们需要做的事情是对 n 维样本空间的点进行类别区分，某些点会归属到某个类别。</p><p>下图所示的是二维平面中的两类样本点，我们的模型（分类器）在学习一种区分不同类别的方法，比如这里是使用一条直线去对2类不同的样本点进行切分：</p><p><img data-galleryid="" data-imgfileid="100005879" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczrJJicX7uW8AsBrgzDmictS2uqWp5f4B1g1GZ4PwMIrjJUCGbZS34ojLQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczrJJicX7uW8AsBrgzDmictS2uqWp5f4B1g1GZ4PwMIrjJUCGbZS34ojLQ/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 机器学习算法分类; 分类问题;</figcaption></figure><p>常见的分类问题应用场景很多，我们选择几个进行举例说明：</p><ul><li><p><strong>垃圾邮件识别</strong>：可以作为<strong>二分类</strong>问题，将邮件分为你「垃圾邮件」或者「正常邮件」。</p></li><li><p><strong>图像内容识别</strong>：因为图像的内容种类不止一个，图像内容可能是猫、狗、人等等，因此是<strong>多类分类</strong>问题。</p></li><li><p><strong>文本情感分析</strong>：既可以作为<strong>二分类</strong>问题，将情感分为褒贬两种，还可以作为<strong>多类分类</strong>问题，将情感种类扩展，比如分为：十分消极、消极、积极、十分积极等。</p></li></ul><h3><span>2.逻辑回归算法核心思想</span></h3><p>下面介绍本次要讲解的算法——逻辑回归（ Logistic Regression）。逻辑回归是线性回归的一种扩展，用来处理分类问题。</p><h4><span>1）线性回归与分类</span></h4><p>分类问题和回归问题有一定的相似性，都是通过对数据集的学习来对未知结果进行预测，<strong>区别在于输出值不同</strong>。</p><ul><li><p><strong>分类问题</strong>的输出值是<strong>离散值</strong>（如垃圾邮件和正常邮件）。</p></li><li><p><strong>回归问题</strong>的输出值是<strong>连续值</strong>（例如房子的价格）。</p></li></ul><p><strong>既然分类问题和回归问题有一定的相似性，那么我们能不能在回归的基础上进行分类呢</strong>？</p><p>可以想到的一种尝试思路是，先用线性拟合，然后<strong>对线性拟合的预测结果值进行量化</strong>，即将连续值量化为离散值——即使用<strong>『线性回归+阈值』</strong>解决分类问题。</p><p>我们来看一个例子。假如现在有一个关于肿瘤大小的数据集，需要根据肿瘤的大小来判定是<strong>良性（用数字0表示）</strong>还是<strong>恶性（用数字1表示)</strong>，这是一个很典型的二分类问题。</p><p>如上图，目前这个简单的场景我们得到一个直观的判定：肿瘤的大小大于5，即为恶性肿瘤（输出为1）；肿瘤的大小等于5，即为良性肿瘤（输出为0)：</p><p><img data-galleryid="" data-imgfileid="100005880" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczjH3zUPia1aPEkVgAFagzRYia9XWsiahpVYTxrBPQyz0bwRoXg8AT241UQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczjH3zUPia1aPEkVgAFagzRYia9XWsiahpVYTxrBPQyz0bwRoXg8AT241UQ/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 算法核心思想; 线性回归+阈值;</figcaption></figure><p>下面我们尝试之前提到的思路，使用一元线性函数 h(x)=θ<sub>1</sub>x+θ<sub>0</sub> 去进行拟合数据，函数体现在图片中就是这条黑色直线。</p><p>这样分类问题就可以转化为：对于这个线性拟合的假设函数，给定一个肿瘤的大小，只要将其带入假设函数，并将其输出值和0.5进行比较：</p><ul><li><p><span>如果线性回归值大于0.5，就输出1（恶性肿瘤)。</span></p></li><li><p><span>如果线性回归值小于0.5，就输出0（良性肿瘤)。</span></p></li></ul><p><img data-galleryid="" data-imgfileid="100005881" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczCUXXx8DvY6Cfr1HPLndRTYRwCbB52M0T0SrqyaJTvKicicgtscYvPFow/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczCUXXx8DvY6Cfr1HPLndRTYRwCbB52M0T0SrqyaJTvKicicgtscYvPFow/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 算法核心思想; 线性回归+阈值;</figcaption></figure><p>上图的数据集中的分类问题被完美解决。但如果将数据集更改一下，如图所示，如果我们还是以0.5为判定阈值，那么就会把肿瘤大小为6的情况进行误判为良好：</p><p><img data-galleryid="" data-imgfileid="100005882" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczOaR3NyoDxh7IHl6aWQJtHk49O7q7sRvqSc3XVnXVmN8lZbGWd8gnyQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczOaR3NyoDxh7IHl6aWQJtHk49O7q7sRvqSc3XVnXVmN8lZbGWd8gnyQ/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 算法核心思想; 线性回归+阈值;</figcaption></figure><p>所以，单纯地通过将线性拟合的输出值与某一个阈值进行比较，<strong>这种方法用于分类非常不稳定</strong>。</p><h4><span>2）逻辑回归核心思想</span></h4><p>因为「线性回归+阈值」的方式很难得到鲁棒性好的分类器，我们对其进行拓展得到<strong>鲁棒性更好的逻辑回归</strong>（ Logistic Regression，有些地方也叫做<strong>「对数几率回归」</strong>）。</p><p>逻辑回归将数据拟合到一个<strong>logit函数</strong>中，从而完成对事件发生概率的预测。</p><p>如果线性回归的结果输出是一个连续值，而<strong>值的范围是无法限定的</strong>，这种情况下我们无法得到稳定的判定阈值。那是否可以<strong>把这个结果映射到一个固定大小的区间内</strong>（比如0~1)，进而判断呢？</p><p>当然可以，这就是逻辑回归做的事情，而其中用于对连续值压缩变换的函数叫做 <strong>Sigmoid 函数</strong>（也称 Logistic 函数，S函数）：</p><p><img data-galleryid="" data-imgfileid="100005883" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczDUBBT0ibic8Jt5JcX3bvQAdnYxXPEMDTClyrt3yhandLdhMCZtlDjZNg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczDUBBT0ibic8Jt5JcX3bvQAdnYxXPEMDTClyrt3yhandLdhMCZtlDjZNg/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 算法核心思想; Sigmoid函数;</figcaption></figure><p>可以看到 S 函数的输出值在 <code>[0,1]</code> 之间。</p><h3><span>3. Sigmoid 函数与决策边界</span></h3><p>刚才大家见到了 Sigmoid 函数，下面我们来讲讲它和线性拟合的结合，如何能够完成分类问题，并且得到清晰可解释的分类器判定「决策边界」。</p><h4><span>1）分类与决策边界</span></h4><p>决策边界就是分类器对于样本进行区分的边界，主要有<strong>线性决策边界</strong>（linear decision boundaries）和<strong>非线性决策边界</strong>（non-linear decision boundaries），如下图所示。</p><p><img data-galleryid="" data-imgfileid="100005884" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczWLXZic0TGgbuaeYcOsoDWIoGhspzpssDQUkvX59sXK6FEfMAzfslZzg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczWLXZic0TGgbuaeYcOsoDWIoGhspzpssDQUkvX59sXK6FEfMAzfslZzg/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; Sigmoid函数; 分类与决策边界;</figcaption></figure><h4><span>2）线性决策边界生成</span></h4><p>那么，逻辑回归是怎么得到决策边界的呢，它与 Sigmoid 函数又有什么关系呢？</p><p>如下图中的例子：</p><p>如果我们用函数 g 表示 Sigmoid 函数，逻辑回归的输出结果由假设函数 h<sub>θ</sub>(x)=g(θ<sub>1</sub>x<sub>1</sub>+θ<sub>2</sub>x<sub>2</sub>+θ<sub>0</sub>) 得到。</p><p>对于图中的例子，我们暂时取参数θ<sub>1</sub>、θ<sub>2</sub>、θ<sub>0</sub>分别为1、1和-3，那么我们可以得到<strong>函数 -3+x<sub>1</sub>+x<sub>2</sub></strong>。所以<strong>在由 x<sub>1</sub> 和 x<sub>2</sub> 构成的空间内，我们可以依赖 -3+x<sub>1</sub>+x<sub>2</sub> = 0 这条曲线（即： x<sub>2</sub> = -x<sub>1</sub>+3）当作边界</strong>。那么对于图上的两类样本点，我们代入一些坐标到h<sub>θ</sub>(x)，会得到什么结果值呢：</p><ul><li><p>对于直线上方的点 (x<sub>1</sub>,x<sub>2</sub>)（例如 (100,100)），代入 -3+x<sub>1</sub>+x<sub>2</sub>，得到大于0的取值，经过 Sigmoid 映射后得到的是大于0.5的取值。</p></li><li><p>对于直线上方的点 (x<sub>1</sub>,x<sub>2</sub>)（例如 (0,0)），代入 -3+x<sub>1</sub>+x<sub>2</sub>，得到小于0的取值，经过 Sigmoid 映射后得到的是小于0.5的取值。</p></li></ul><p><img data-galleryid="" data-imgfileid="100005885" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5Ccczj3uef7OSPDp8Xhq42Osp1ZqQBS3ss7yP4iarliaQu2oibWLFxHiag7qxiaQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5Ccczj3uef7OSPDp8Xhq42Osp1ZqQBS3ss7yP4iarliaQu2oibWLFxHiag7qxiaQ/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; Sigmoid函数; 线性决策边界生成;</figcaption></figure><p>如果我们以0.5为判定边界，则线性拟合的直线 -3+x<sub>1</sub>+x<sub>2</sub>=0 变换成了一条决策边界（这里是线性决策边界)。</p><h4><span>3）非线性决策边界生成</span></h4><p>其实，我们不仅仅可以得到线性决策边界，当 h<sub>θ</sub>(x) 更复杂的时候，我们甚至可以得到对样本非线性切分的非线性决策边界（这里的非线性指的是无法通过直线或者超平面把不同类别的样本很好地切分开）。</p><p>如下图中另外一个例子：如果我们用函数 g 表示 Sigmoid 函数，逻辑回归的输出结果由假设函数 h<sub>θ</sub>(x)=g(θ<sub>1</sub>x<sub>1</sub>+θ<sub>2</sub>x<sub>2</sub>+θ<sub>3</sub>x<sub>1</sub><sup>2</sup>+θ<sub>4</sub>x<sub>2</sub><sup>2</sup>+θ<sub>0</sub>) 得到。</p><p>对于图中的例子，我们暂时取参数θ<sub>1</sub>、θ<sub>2</sub>、θ<sub>3</sub>、θ<sub>4</sub>、θ<sub>0</sub>分别为0、0、1、1和-1，那么我们可以得到<strong>函数 x<sub>1</sub><sup>2</sup>+x<sub>2</sub><sup>2</sup>-1</strong>。所以<strong>在由 x<sub>1</sub> 和 x<sub>2</sub> 构成的空间内，我们可以依赖 x<sub>1</sub><sup>2</sup>+x<sub>2</sub><sup>2</sup>-1 = 0 这条曲线（即： x<sub>1</sub><sup>2</sup>+x<sub>2</sub><sup>2</sup> = 1）当作边界</strong>。那么对于图上的两类样本点，我们代入一些坐标到h<sub>θ</sub>(x)，会得到什么结果值呢：</p><ul><li><p>对于圆外部的点 (x<sub>1</sub>,x<sub>2</sub>)（例如 (100,100)），代入x<sub>1</sub><sup>2</sup>+x<sub>2</sub><sup>2</sup>-1，得到大于0的取值，经过 Sigmoid 映射后得到的是大于0.5的取值。</p></li><li><p>对于圆内部的点 (x<sub>1</sub>,x<sub>2</sub>)（例如 (0,0)），代入x<sub>1</sub><sup>2</sup>+x<sub>2</sub><sup>2</sup>-1，得到小于0的取值，经过 Sigmoid 映射后得到的是小于0.5的取值。</p></li></ul><p><img data-galleryid="" data-imgfileid="100005886" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5Cccz1ibgXeQxwg2HFibLmgnnXWVBc4Sia1NtG6icarExrPfDyaF76KpABTKhBQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5Cccz1ibgXeQxwg2HFibLmgnnXWVBc4Sia1NtG6icarExrPfDyaF76KpABTKhBQ/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; Sigmoid函数; 非线性决策边界生成;</figcaption></figure><p>如果我们以0.5为判定边界，则线性拟合的圆曲线x<sub>1</sub><sup>2</sup>+x<sub>2</sub><sup>2</sup>-1=0变换成了一条决策边界（这里是非线性决策边界)。</p><h3><span>4.梯度下降与优化</span></h3><h4><span>1）损失函数</span></h4><p>前一部分的例子中，我们手动取了一些参数θ的取值，最后得到了决策边界。但大家显然可以看到，<strong>取不同的参数时，可以得到不同的决策边界</strong>。</p><p><strong>哪一条决策边界是最好的呢</strong>？我们需要定义一个能量化衡量模型好坏的函数——<strong>损失函数</strong>（有时候也叫做「目标函数」或者「代价函数」）。我们的目标是<strong>使得损失函数最小化</strong>。</p><p><strong>我们如何衡量预测值和标准答案之间的差异呢</strong>？最简单直接的方式是数学中的均方误差。它的计算方式很简单，对于所有的样本点 x<sub>i</sub>，预测值 h<sub>θ</sub>(x<sub>i</sub>) 与标准答案 y<sub>i</sub> 作差后平方，求均值即可，这个取值越小代表差异度越小。</p><blockquote><p>详情可以参考：<a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489408&amp;idx=1&amp;sn=159692dc7f34d89293cca28172c2e86b&amp;scene=21#wechat_redirect" data-linktype="2">2.图解机器学习 | 模型评估方法与准则</a></p><p>在逻辑回归中需要注意，因为<strong>真实结果 y 的取值只有0和1</strong>，故在计算所谓的误差时，是用 Sigmoid 函数转化到【0-1】之间的 y<sub>i</sub> 与0/1进行比较。</p></blockquote><p>均方误差对应的损失函数：均方误差损失（MSE）在回归问题损失定义与优化中广泛应用，但是<strong>在逻辑回归问题中不太适用</strong>。</p><p>Sigmoid 函数的变换使得我们最终得到损失函数曲线如下图所示，是非常不光滑凹凸不平的，这种数学上叫做<strong>非凸的损失函数</strong>，我们要找到最优参数（使得函数取值最小的参数）是很困难的。</p><blockquote><p><strong>解释</strong>：在逻辑回归模型场景下，使用MSE得到的<strong>损失函数是非凸的，数学特性不太好</strong>，我们希望损失函数如下的凸函数。因为<strong>在凸优化问题中，局部最优解同时也是全局最优解</strong>，这一特性使得凸优化问题在一定意义上更易于解决，而一般的非凸最优化问题相比之下更难解决。</p></blockquote><p><img data-galleryid="" data-imgfileid="100005887" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5Cccz2icXAib8picxOS0Z4aZzq64MRF6TBwTnGIO8fh3lSH6ibWdCYwdAo1B3yg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5Cccz2icXAib8picxOS0Z4aZzq64MRF6TBwTnGIO8fh3lSH6ibWdCYwdAo1B3yg/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 梯度下降与优化; 均方差损失函数;</figcaption></figure><p>我们更希望我们的损失函数如下图所示，是凸函数，这样我们在数学上有很好优化方法可以对其进行优化。</p><p>在逻辑回归模型场景下，我们会改用<strong>对数损失函数</strong>（二元交叉熵损失），这个损失函数同样<strong>能很好地衡量参数好坏，又能保证凸函数的特性</strong>。对数损失函数的公式如下：</p><p><img data-galleryid="" data-imgfileid="100005888" data-ratio="0.11969111969111969" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczjdCvJheibicOZrqJ2n90A02RkW6XZoMGoMKDtnPyssVLwYO1geHmB05g/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1036" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczjdCvJheibicOZrqJ2n90A02RkW6XZoMGoMKDtnPyssVLwYO1geHmB05g/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>对数损失函数</figcaption></figure><blockquote><p>注意：<strong>y<sub>(i)</sub> 表示样本取值，在其为正样本时取值为1，负样本时取值为0</strong></p></blockquote><p><img data-galleryid="" data-imgfileid="100005889" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczldGRiaAo3CjVbXAI9KHOgvJJSTyxFsEepkWEBRLWeKwuZYI4cqlwUYA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczldGRiaAo3CjVbXAI9KHOgvJJSTyxFsEepkWEBRLWeKwuZYI4cqlwUYA/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 梯度下降与优化; 均方差损失函数;</figcaption></figure><p>这样的话，我们分这两种情况来看看上述的损失函数：</p><ul><li><p><strong>y<sub>(i)</sub> = 0</strong>：当一个样本为<strong>负样本</strong>时，若 <strong>h<sub>θ</sub>(x<sub>i</sub>) 的结果接近1（即预测为正样本）</strong>，那么 -log(1-h<sub>θ</sub>(x<sub>i</sub>)) 的值很大，那么<strong>损失函数的值就大</strong>。</p></li><li><p><strong>y<sub>(i)</sub> = 1</strong>：当一个样本为<strong>正样本</strong>时，若 <strong>h<sub>θ</sub>(x<sub>i</sub>) 的结果接近0（即预测为负样本）</strong>，那么 -log(h<sub>θ</sub>(x<sub>i</sub>)) 的值很大，那么<strong>损失函数的值就大</strong>。</p></li></ul><p><img data-galleryid="" data-imgfileid="100005890" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczX3VMLlFzzJ27TE3dUfS1GRQvB3ZVmp8JRccxTIjo2XIZoxBiaAy5wGw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczX3VMLlFzzJ27TE3dUfS1GRQvB3ZVmp8JRccxTIjo2XIZoxBiaAy5wGw/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 梯度下降与优化; 对数损失函数 ;</figcaption></figure><h4><span>2）梯度下降</span></h4><p>损失函数可以用于衡量模型参数好坏，但我们还需要一些优化方法找到最佳的参数（使得当前的损失函数值最小）。最常见的算法之一是<strong>「梯度下降法」</strong>，<strong>逐步迭代减小损失函数（在凸函数场景下非常容易使用）。如同下山，找准方向（斜率），每次迈进一小步，直至山底</strong>。</p><p><img data-galleryid="" data-imgfileid="100005891" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczoMQmfUF7WynicZMjDokeHd0IuNfz047RG5SUPyEUTmAm2zvrHb4ZOLw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczoMQmfUF7WynicZMjDokeHd0IuNfz047RG5SUPyEUTmAm2zvrHb4ZOLw/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 梯度下降与优化; 梯度下降;</figcaption></figure><p>梯度下降（Gradient Descent）法，是一个一阶最优化算法，通常也称为最速下降法。要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。</p><p><img data-galleryid="" data-imgfileid="100005892" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5Cccz5GsibgOY4cbYdqic6icagpkEr2RZjLmdt2ArE7s0lVPScbZGDHB6lKb3w/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5Cccz5GsibgOY4cbYdqic6icagpkEr2RZjLmdt2ArE7s0lVPScbZGDHB6lKb3w/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 梯度下降与优化; 梯度下降;</figcaption></figure><p>上图中，α 称为学习率（learning rate），直观的意义是，在函数向极小值方向前进时每步所走的步长。<strong>太大一般会错过极小值，太小会导致迭代次数过多</strong>。</p><p><img data-galleryid="" data-imgfileid="100005893" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczymQs6qwCTu0VfwQlYqOVNhdCAAkbucVRob0DRcicUQqcnlZXpq0vGicA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczymQs6qwCTu0VfwQlYqOVNhdCAAkbucVRob0DRcicUQqcnlZXpq0vGicA/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 梯度下降与优化; 梯度下降-学习率;</figcaption></figure><h3><span>5.正则化与缓解过拟合</span></h3><h4><span>1）过拟合现象</span></h4><p>在<strong>训练数据不够多</strong>，或者<strong>模型复杂又过度训练</strong>时，模型会陷入过拟合（Overfitting）状态。如下图所示，得到的不同拟合曲线（决策边界）代表不同的模型状态：</p><ul><li><p>拟合曲线1能够将部分样本正确分类，但是仍有<strong>较大量的样本未能正确分类，分类精度低</strong>，是<strong>「欠拟合」</strong>状态。</p></li><li><p>拟合曲线2能够将大部分样本正确分类，并且有<strong>足够的泛化能力</strong>，是<strong>较优的拟合曲线</strong>。</p></li><li><p>拟合曲线3能够很好的<strong>将当前样本区分开来</strong>，但是当<strong>新来一个样本时，有很大的可能不能将其正确区分</strong>，原因是该决策边界太努力地学习当前的样本点，甚至把它们直接「记」下来了。</p></li></ul><p><img data-galleryid="" data-imgfileid="100005894" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczB4Yo4oCqDcEe0kXbY0maibJCDtYjBeYEnV4cUkZHBA1UbXyvN4lcxsg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczB4Yo4oCqDcEe0kXbY0maibJCDtYjBeYEnV4cUkZHBA1UbXyvN4lcxsg/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 正则化&amp;缓解过拟合; 过拟合现象;</figcaption></figure><p><strong>拟合曲线中的「抖动」，表示拟合曲线不规则、不光滑（上图中的拟合曲线3），对数据的学习程度深，过拟合了</strong>。</p><p>因此，我们可以粗略地通过曲线的光滑程度判断曲线是否有过拟合。</p><h4><span>2）正则化处理</span></h4><p>过拟合的一种处理方式是正则化，我们通过对<strong>损失函数</strong>添加正则化项，可以约束参数的搜索空间，从而保证拟合的决策边界并不会抖动非常厉害。如下图为对数损失函数中加入正则化项（这里是一个<strong>L2正则化项</strong>）：</p><p><img data-galleryid="" data-imgfileid="100005895" data-ratio="0.10733333333333334" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CccziayF1YZjA1nbJbYZKIZT21epvjHkGRqMDRhyhicy2pO7ibYctDNn4ulQg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1500" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CccziayF1YZjA1nbJbYZKIZT21epvjHkGRqMDRhyhicy2pO7ibYctDNn4ulQg/640?wx_fmt=png&amp;from=appmsg"></p><p><img data-galleryid="" data-imgfileid="100005896" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczCygrBf6g5FIxXGGLpZKVkJTwT8EzdDe9Mafg57W8CxoJrFDV53b1UQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczCygrBf6g5FIxXGGLpZKVkJTwT8EzdDe9Mafg57W8CxoJrFDV53b1UQ/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 正则化&amp;缓解过拟合; 正则化处理;</figcaption></figure><p>其中 λ 表示正则化系数，表示<strong>惩罚程度</strong>， λ 的值越大，为使 J(θ) 的值小，则参数 θ 的绝对值就得越小，通常对应于越光滑的函数，也就是更加简单的函数，因此不易发生过拟合的问题。我们依然可以采用梯度下降对加正则化项的损失函数进行优化。</p><blockquote><p>这里对正则化的介绍不是特别详细，有兴趣的可以参考进行深入理解：</p><p>奇妙AI小知识系列二：正则化项 |15分钟彻底理解正则化项的物理意义：https://www.bilibili.com/video/BV1KV411z7L7/?buvid=XUE8450ED3ABA73C98FC056E64E409DB148AD</p></blockquote><h3><span>6.特征变换与非线性表达</span></h3><h4><span>1）多项式特征</span></h4><p>对于输入的特征，如果我们<strong>直接进行线性拟合再给到 Sigmoid 函数，得到的是线性决策边界</strong>。但添加多项式特征，可以对样本点进行多项式回归拟合，也能在后续得到更好的非线性决策边界。</p><blockquote><p>线性拟合：对于不同特征x而言，均以x原始值进行处理，没有进行各种复杂变换，例如x<sup>2</sup>，log(x) 等</p><p>多项式回归拟合：对于不同特征x而言，可以以x的不同变换形式进行处理，例如x<sup>2</sup>，log(x) 等</p></blockquote><p><img data-galleryid="" data-imgfileid="100005897" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczTff9frt5sEvETniacicNic8gXZ30Z9dhPQssGD8bOBLb3KSAb0gto0TRw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczTff9frt5sEvETniacicNic8gXZ30Z9dhPQssGD8bOBLb3KSAb0gto0TRw/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 特征变换; 多项式特征 ;</figcaption></figure><p>多项式回归，回归函数是回归变量多项式。多项式回归模型是线性回归模型的一种，此时回归函数关于回归系数是线性的。</p><p>在实际应用中，通过增加一些输入数据的非线性特征来增加模型的复杂度通常是有效的。一个简单通用的办法是使用多项式特征，这可以获得特征的更高维度和互相间关系的项，进而获得更好的实验结果。</p><h4><span>2）非线性切分</span></h4><p>如下图所示，在逻辑回归中，拟合得到的决策边界，可以<strong>通过添加多项式特征，调整为非线性决策边界</strong>，具备非线性切分能力。</p><ul><li><p>Z<sub>θ</sub>(x) 中 θ 是参数，当 Z<sub>θ</sub>(x) = θ<sub>0</sub>+θ<sub>1</sub>x 时，此时得到的是线性决策边界；</p></li><li><p>Z<sub>θ</sub>(x) = θ<sub>0</sub>+θ<sub>1</sub>x+θ<sub>2</sub>x<sup>2</sup> 时，使用了多项式特征，得到的是非线性决策边界。</p></li></ul><p><img data-galleryid="" data-imgfileid="100005898" data-ratio="0.38060249816311537" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczfUicaGibD5hBDickEmzhHiaVF2zOJSzHp3xSfz824yn6ZokCnibCBU5bf4g/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1361" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJoyQfhCbO48Pqoibylk5CcczfUicaGibD5hBDickEmzhHiaVF2zOJSzHp3xSfz824yn6ZokCnibCBU5bf4g/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>逻辑回归算法详解; 非线性表达; 非线性切分;</figcaption></figure><p>目的是低维线性不可分的数据转化到高维时，会变成线性可分。得到在高维空间下的线性分割参数映射回低维空间，形式上表现为低维的非线性切分。</p><h3><span>后记</span></h3><p>这部分内容详细介绍了逻辑回归的算法模型原理，还是比较容易理解，不过关于<strong>为什么使用Sigmoid 函数将线性结果转化为逻辑回归结果</strong>没有进行解释，但是不影响我们的使用，我们只需要知道，这个转换的数学家们根据数据特征以及模型原理得到的即可，我们放心使用就好。这对于后面的机器学习真的很有帮助，再次感叹 <strong>ShowMeAI</strong> 论坛整理的真的非常棒，对数学能力不太高的我真的非常友善了！</p><blockquote><p>https://www.showmeai.tech/tutorials/34?articleId=188</p></blockquote></section><p><br></p><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/Vy_Ybq-n1TwrNVlG29P00A",target="_blank" rel="noopener noreferrer">原文链接</a>
