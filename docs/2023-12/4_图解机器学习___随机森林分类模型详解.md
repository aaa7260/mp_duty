---
title: "4.图解机器学习 | 随机森林分类模型详解"
date: 2023-12-12T02:27:47Z
draft: ["false"]
tags: [
  "fetched",
  "生信小知识"
]
categories: ["Acdemic"]
---
4.图解机器学习 | 随机森林分类模型详解 by 生信小知识
------
<div><section><h2><span>4.图解机器学习 | 随机森林分类模型详解</span></h2><blockquote><p>微信公众号：<strong>生信小知识</strong><br>关注可了解更多的生物信息学教程及知识。问题或建议，请公众号留言;</p></blockquote><h3><span>目录</span></h3><p><span><span>前言</span></span><span><span>1.集成学习</span></span><span><span><span>1）集成学习</span></span></span><span><span><span>2）个体学习器</span></span></span><span><span><span>3）集成学习核心问题</span></span></span><span><span><span><span>（1）使用什么样的个体学习器？</span></span></span></span><span><span><span><span>（2）如何选择合适的结合策略构建强学习器？</span></span></span></span><span><span>2.Bagging</span></span><span><span><span>1）Bootstrap Sampling</span></span></span><span><span><span>2）Bagging</span></span></span><span><span>3.随机森林算法</span></span><span><span><span>1）随机森林算法介绍</span></span></span><span><span><span>2）随机森林核心特点</span></span></span><span><span><span>3）随机森林决策边界可视化</span></span></span><span><span><span>4）随机森林算法优点</span></span></span><span><span><span><span>（1）随机森林优点</span></span></span></span><span><span><span><span>（2）随机森林缺点</span></span></span></span><span><span>4.影响随机森林的参数与调优</span></span><span><span><span>1）核心影响参数</span></span></span><span><span><span><span>（1）生成单颗决策树时的特征数 max_features</span></span></span></span><span><span><span><span>（2）决策树的棵数 n_estimators</span></span></span></span><span><span><span><span>（3）树深 max_depth</span></span></span></span><span><span><span>2）参数调优</span></span></span><span><span><span><span>（1）RF划分时考虑的最大特征数 max_features</span></span></span></span><span><span><span><span>（2）决策树的棵树 n_estimators</span></span></span></span><span><span><span><span>（3）决策树最大深度 max_depth</span></span></span></span><span><span><span><span>（4）内部节点再划分所需最小样本数 min_samples_split</span></span></span></span><span><span><span><span>（5）叶子节点最少样本数 min_samples_leaf</span></span></span></span><span><span><span>后记</span></span></span></p><h3><span>前言</span></h3><p>本系列内容均来自 <strong>ShowMeAI</strong> 论坛：https://www.showmeai.tech/</p><p>该论坛中内容详实，部分系列非常适合数学功底不那么好的同学学习，这里仅对 【<strong>图解机器学习</strong>】系列进行学习，并同时整理为笔记。</p><p>过往已发布笔记：</p><ul><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489371&amp;idx=1&amp;sn=8d61e3d8cde6acb55361e4eeade4fbb6&amp;scene=21#wechat_redirect" data-linktype="2">1.图解机器学习 | 机器学习基础知识</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489408&amp;idx=1&amp;sn=159692dc7f34d89293cca28172c2e86b&amp;scene=21#wechat_redirect" data-linktype="2">2.图解机器学习 | 模型评估方法与准则</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489441&amp;idx=1&amp;sn=2e5ae676ade3a10a6b3615ad84ef7735&amp;scene=21#wechat_redirect" data-linktype="2">3.图解机器学习 | 决策树模型详解</a></p></li></ul><p>今天主要介绍<strong>随机森林的原理知识</strong>。</p><blockquote><p>本公众号已与ShowMeAI论坛负责人联系，并得到授权转载。在次给做机器学习的朋友们推荐下他们的微信公众号，有兴趣的同学可以自行关注。</p></blockquote><h3><span>1.集成学习</span></h3><h4><span>1）集成学习</span></h4><p>学习随机森林，我们需要先了解一些概念，比如第1个大家要了解的概念是<strong>集成学习（ensemble learning）</strong>：</p><p><img data-galleryid="" data-imgfileid="100005795" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJsFDJ9IYK89z6sfmnibIrddXleM7Hjnlblt0YNU6TfiaqPOPWVrucIUdw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJsFDJ9IYK89z6sfmnibIrddXleM7Hjnlblt0YNU6TfiaqPOPWVrucIUdw/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>随机森林分类模型; 集成学习; Ensemble Learning;</figcaption></figure><p><strong>对于训练数据集，我们训练一系列「个体学习器」，再通过「结合策略」将它们集成起来，形成一个更强的学习器，这就是「集成学习」在做的事情</strong>，内核思想类似<strong>「三个臭皮匠，顶个诸葛亮」</strong>。</p><h4><span>2）个体学习器</span></h4><p>个体学习器是相对于集成学习来说的，其实我们在之前了解到的很多模型，比如 C4.5 决策树算法、逻辑回归算法、朴素贝叶斯算法等，都是个体学习器。</p><ul><li><p>若集成中只包含<strong>同种类型</strong>的个体学习器，叫做<strong>「同质」集成</strong>，个体学习器称作<strong>「基学习器」</strong>。例如随机森林中全是决策树集成。</p></li><li><p>若集成中包含<strong>不同类型</strong>的个体学习器，叫做<strong>「异质」集成</strong>，个体学习器称作<strong>「组件学习器」</strong>。例如同时包含决策树和神经网络进行集成。</p></li></ul><blockquote><p>个体学习器代表的是单个学习器，集成学习代表的是多个学习器的结合。</p></blockquote><p><img data-galleryid="" data-imgfileid="100005796" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJ2cBggibAZDq9OBGlatPdmiaNRlib71hrHopLUJnGcTGBBWVRh1ttr89pA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJ2cBggibAZDq9OBGlatPdmiaNRlib71hrHopLUJnGcTGBBWVRh1ttr89pA/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>随机森林分类模型; 集成学习; 个体学习器;</figcaption></figure><h4><span>3）集成学习核心问题</span></h4><h5><span>（1）使用什么样的个体学习器？</span></h5><ul><li><p>个体学习器不能太「弱」，需要<strong>有一定的准确性</strong>。</p></li><li><p>个体学习器之间要具有「多样性」，即<strong>存在差异性</strong>。</p></li></ul><h5><span>（2）如何选择合适的结合策略构建强学习器？</span></h5><ul><li><p><strong>并行组合</strong>方式，例如随机森林。</p></li><li><p><strong>串行组合</strong>方式，例如boosting树模型。</p></li></ul><p><img data-galleryid="" data-imgfileid="100005797" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJMVHHRoQbsowWHO0rsO9vnoEnfWNSX9icnFCcoWyaq0GfKmbQIpcyvQA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJMVHHRoQbsowWHO0rsO9vnoEnfWNSX9icnFCcoWyaq0GfKmbQIpcyvQA/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>随机森林分类模型; 集成学习; 两个核心问题;</figcaption></figure><h3><span>2.Bagging</span></h3><p>我们在这里讲到的随机森林是<strong>并行集成模型</strong>，而Bagging是并行式集成学习方法最著名的代表。</p><h4><span>1）Bootstrap Sampling</span></h4><p>要理解bagging，首先要了解自助采样法（Bootstrap Sampling）：</p><ul><li><p>给定包含m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中【<strong>有放回抽样</strong>】。</p></li><li><p>上述过程重复m轮，我们得到m个样本的采样集，初始训练集中有的样本在采样集中多次出现，有的则从未出现。根据数学计算，可以证明：<strong>约63.2%的样本出现在采样集中</strong>，而<strong>未出现的约36.8%的样本可用作验证集来对后续的泛化性能进行「包外估计」</strong>。</p></li></ul><p><img data-galleryid="" data-imgfileid="100005798" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJm6bkjMS9VBhWLAwTUMC12pnFXQPVoG3Jn0YCaIegE0N8kMzdo76cSw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJm6bkjMS9VBhWLAwTUMC12pnFXQPVoG3Jn0YCaIegE0N8kMzdo76cSw/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>随机森林分类模型; Bagging; 自助采样法;</figcaption></figure><h4><span>2）Bagging</span></h4><p>Bagging是<strong>B</strong>ootstrap <strong>ag</strong>gregating的缩写，是在Boostrap Sampling基础上构建的：上述的采样过程我们可以重复T次，采样出<strong>T个含m个训练样本的采样集</strong>，然后<strong>基于每个采样集训练出一个基学习器</strong>，然后<strong>将这些基学习器进行结合</strong>。</p><p>在对预测输出进行结合时：</p><ul><li><p>Bagging 通常对<strong>分类任务</strong>使用<strong>简单投票法</strong></p></li><li><p>Bagging 通常对<strong>回归任务</strong>使用<strong>简单平均法</strong></p></li></ul><p>这就 是Bagging 的基本流程。</p><p><img data-galleryid="" data-imgfileid="100005799" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJ2W3Yg9qHQZVTpiaGH8C7WCRUuMaLvZJRfWFic8blZav07Po82I65ICwQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJ2W3Yg9qHQZVTpiaGH8C7WCRUuMaLvZJRfWFic8blZav07Po82I65ICwQ/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>随机森林分类模型; 参数调优;</figcaption></figure><p>从「偏差-方差分解」的角度看，<strong>Bagging主要关注降低方差</strong>，因此它在不剪枝的决策树、神经网络等<strong>易受到样本扰动的学习器上效用更明显</strong>。</p><h3><span>3.随机森林算法</span></h3><h4><span>1）随机森林算法介绍</span></h4><p>Random Forest（随机森林，简称RF）是一种基于树模型的Bagging的优化版本。<strong>核心思想依旧是Bagging</strong>，但是做了一些独特的改进——<strong>RF使用了CART决策树作为基学习器</strong>。</p><p>假设我们一共有<strong>N个样本</strong>，这些样本一共有<strong>M个特征</strong>，从这些样本中一共进行<strong>T次有放回抽样，每次抽样n个样本</strong>。具体过程如下：</p><ul><li><p>输入为样本集D =｛(x<sub>1</sub>,y<sub>1</sub>), (x<sub>2</sub>,y<sub>2</sub>), …, (x<sub>N</sub>,y<sub>N</sub>) ｝</p></li><li><p><span>对于每次抽样 t=1, 2, 3, …, T：</span></p></li><li><p>对训练集进行<strong>第t次</strong>随机采样，共采集得到<strong>包含n个样本</strong>的采样集D<sub>t</sub>。</p></li><li><p><strong>用采样集D<sub>t</sub>训练第t个决策树模型G<sub>t</sub>(x)</strong>：在训练决策树模型的节点的时候，在抽样得到的<strong>n个样本</strong>中，从全部M个特征中<strong>随机不放回</strong>选择m（<strong>m &lt;&lt; M</strong>）个特征，利用<strong>信息增益</strong>和<strong>基尼指数</strong>或者其他指标，从m个特征中<strong>选择一个最优的特征</strong>来做决策树的左右子树划分。</p></li><li><p>左右子树划分后，继续从全部M个特征中<strong>随机不放回</strong>选择m（<strong>m &lt;&lt; M</strong>）个特征，选择一个最优的特征来做决策树的左右子树划分。</p></li><li><p>每棵树都一直这样分裂下去，直到该节点的所有训练样例都属于同一类，<strong>不需要剪枝</strong>。</p></li></ul><p>这样运行的结果是：<strong>生成了T个决策树</strong>。</p><p>将模型应用到新的数据中时，会根据T个基模型（决策树）<strong>投出最多票数的类别</strong>为最终类别。</p><p><img data-galleryid="" data-imgfileid="100005800" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJzZicvovf22OMudxgZu3mMNicowhDKSSxAUfpfgtXZhnwmILA10iclj9Rg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJzZicvovf22OMudxgZu3mMNicowhDKSSxAUfpfgtXZhnwmILA10iclj9Rg/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>随机森林分类模型; 随机森林算法; Random Forest, RF;</figcaption></figure><h4><span>2）随机森林核心特点</span></h4><p>随机森林核心点是<strong>「随机」</strong>和<strong>「森林」</strong>，也是给它带来良好性能的最大支撑。</p><p><strong>「随机」主要体现在两个方面</strong>：</p><ul><li><p><strong>样本扰动</strong>：直接基于自助采样法（Bootstrap Sampling），使得<strong>初始训练集中约63.2%的样本出现在一个采样集中</strong>，并带来数据集的差异化。</p></li><li><p><strong>属性扰动</strong>：在随机森林中，对基决策树的每个结点，先在该结点的特征属性集合中<strong>随机选择k个属性</strong>，然后再从这k个属性中<strong>选择一个最优属性进行划分</strong>。这一重随机性也会带来基模型的差异性。</p></li></ul><p><strong>「集成」体现在</strong>：根据多个（差异化）采样集，训练得到多个（差异化）决策树，采用<strong>简单投票</strong>或者<strong>平均法</strong>来<strong>提高模型稳定性和泛化能力</strong>。</p><h4><span>3）随机森林决策边界可视化</span></h4><p>下面是对于同一份数据集（iris数据集），我们使用决策树和不同棵数的随机森林做分类的结果，我们对其决策边界做了可视化。</p><p><img data-galleryid="" data-imgfileid="100005801" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJr3Ric7PobajPwygB8SVAfxibnV4jdyUUtKeqv3t52IejiafJsXG8ibMRQQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJr3Ric7PobajPwygB8SVAfxibnV4jdyUUtKeqv3t52IejiafJsXG8ibMRQQ/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>随机森林分类模型; 决策边界可视化; iris数据集;</figcaption></figure><p>可以很明显地看到，<strong>随着随机森林中决策树数量的增多，模型的泛化能力逐渐增强，决策边界越来越趋于平滑</strong>（受到噪声点的影响越来越小）。</p><h4><span>4）随机森林算法优点</span></h4><p>下面我们来总结一下随机森林的优缺点：</p><h5><span>（1）随机森林优点</span></h5><ul><li><p><span>对于高维（特征很多）稠密型的数据适用，不用降维，无需做特征选择。</span></p></li><li><p>构建随机森林模型的过程，亦<strong>可帮助判断特征的重要程度</strong>。</p></li><li><p><span>可以借助模型构建组合特征。</span></p></li><li><p>并行集成，有效<strong>控制过拟合</strong>。</p></li><li><p>工程<strong>实现并行简单，训练速度快</strong>。</p></li><li><p>对于不平衡的数据集友好，可以<strong>平衡误差</strong>。</p></li><li><p>对于<strong>数据缺失</strong>的情况，<strong>鲁棒性仍然强，可以维持不错的准确度</strong>。</p></li></ul><h5><span>（2）随机森林缺点</span></h5><ul><li><p>在<strong>噪声过大</strong>的分类和回归数据集上还是可能会过拟合。</p></li><li><p>相比单一决策树，因其随机性，<strong>模型解释会更复杂一些</strong>。</p></li></ul><h3><span>4.影响随机森林的参数与调优</span></h3><p>上面我们已经系统了解了随机森林的原理与机制，下面我们一起来看看工程应用实践中的一些重点，比如随机森林模型有众多可调参数，它们有什么影响，又如何调优。</p><h4><span>1）核心影响参数</span></h4><p><img data-galleryid="" data-imgfileid="100005802" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJiaYavwcc6p7U4g8xIEsz6jpNsnRNcooCsKhR6F2slrgA9FicxxOg6SCg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJiaYavwcc6p7U4g8xIEsz6jpNsnRNcooCsKhR6F2slrgA9FicxxOg6SCg/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>随机森林分类模型; 核心影响参数;</figcaption></figure><h5><span>（1）生成单颗决策树时的特征数 max_features</span></h5><ul><li><p>增加 <code>max_features</code> 一般能<strong>提高单个决策树模型的性能</strong>，但<strong>降低了树和树之间的差异性</strong>，且可能<strong>降低算法的速度</strong>。</p></li><li><p>太小的 <code>max_features</code> 会<strong>影响单颗树的性能</strong>，进而影响整体的集成效果。</p></li><li><p>需要适当地平衡和选择最佳的 <code>max_features</code>。</p></li></ul><h5><span>（2）决策树的棵数 n_estimators</span></h5><ul><li><p><strong>较多的子树</strong>可以让模型有<strong>更好的稳定性和泛化能力</strong>，但同时让模型的<strong>学习速度变慢</strong>。</p></li><li><p>我们会在计算资源能支撑的情况下，<strong>选择稍大的子树棵树</strong>。</p></li></ul><h5><span>（3）树深 max_depth</span></h5><ul><li><p>太大的树深，因为每颗子树都过度学习，<strong>可能会有过拟合问题</strong>。</p></li><li><p>如果模型样本量多特征多，我们会<strong>限制最大树深</strong>，提高模型的泛化能力。</p></li></ul><h4><span>2）参数调优</span></h4><p><img data-galleryid="" data-imgfileid="100005803" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJUMhEqowicE6tCNdf7ibnWuicKWUbtMygncrUuLXPiayEgiaATnV8xO3nCCA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJqcbXynZpVNjPiaEviauDVQnJUMhEqowicE6tCNdf7ibnWuicKWUbtMygncrUuLXPiayEgiaATnV8xO3nCCA/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>随机森林分类模型; 参数调优;</figcaption></figure><h5><span>（1）RF划分时考虑的最大特征数 max_features</span></h5><ul><li><p>总数的百分比，常见的选择区间是<strong> [0.5-0.9]</strong>。</p></li></ul><h5><span>（2）决策树的棵树 n_estimators</span></h5><ul><li><p>可能会设置为 <strong>&gt;50</strong> 的取值，可根据计算资源调整。</p></li></ul><h5><span>（3）决策树最大深度 max_depth</span></h5><ul><li><p>常见的选择在 <strong>4-12</strong> 之间。</p></li></ul><h5><span>（4）内部节点再划分所需最小样本数 min_samples_split</span></h5><ul><li><p>如果<strong>样本量不大</strong>，不需要调整这个值。</p></li><li><p>如果<strong>样本量数量级非常大</strong>，我们可能会设置这个值为 <strong>16, 32, 64</strong> 等。</p></li></ul><h5><span>（5）叶子节点最少样本数 min_samples_leaf</span></h5><ul><li><p>为了提高泛化能力，我们可能会设置这个值 <strong>&gt;1</strong>。</p></li></ul><h4><span>后记</span></h4><p>这部分内容详细介绍了随机森林的算法模型原理，在理解了决策树的基础上，还是比较容易理解，并且对于后面的机器学习真的很有帮助，再次感叹 <strong>ShowMeAI</strong> 论坛整理的真的非常棒，对数学能力不太高的我真的非常友善了！</p><blockquote><p>https://www.showmeai.tech/tutorials/34?articleId=191</p></blockquote></section><p><br></p><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/yd_PBJKACVPHrem9ebt8uw",target="_blank" rel="noopener noreferrer">原文链接</a>
