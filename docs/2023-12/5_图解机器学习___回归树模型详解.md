---
title: "5.图解机器学习 | 回归树模型详解"
date: 2023-12-13T00:40:37Z
draft: ["false"]
tags: [
  "fetched",
  "生信小知识"
]
categories: ["Acdemic"]
---
5.图解机器学习 | 回归树模型详解 by 生信小知识
------
<div><section><h2><span>5.图解机器学习 | 回归树模型详解</span></h2><blockquote><p>微信公众号：<strong>生信小知识</strong><br>关注可了解更多的生物信息学教程及知识。问题或建议，请公众号留言;</p></blockquote><h3><span>目录</span></h3><p><span><span>前言</span></span><span><span>1.决策树回归算法核心思想</span></span><span><span><span>1）决策树结构回顾</span></span></span><span><span><span>2）回归树的核心思想</span></span></span><span><span>2.启发式切分与最优属性选择</span></span><span><span><span>1）回归树模型示例</span></span></span><span><span><span>2）回归树构建方法</span></span></span><span><span><span><span>（1）贪婪式递归</span></span></span></span><span><span><span><span>（2）递归二分</span></span></span></span><span><span>3.过拟合与正则化</span></span><span><span><span>1）过拟合问题</span></span></span><span><span><span>2）过拟合问题处理</span></span></span><span><span><span><span>（1）约束控制树的过度生长</span></span></span></span><span><span><span><span>（2）剪枝</span></span></span></span><span><span><span>3）正则化</span></span></span><span><span>后记</span></span></p><h3><span>前言</span></h3><p>本系列内容均来自 <strong>ShowMeAI</strong> 论坛：https://www.showmeai.tech/</p><p>该论坛中内容详实，部分系列非常适合数学功底不那么好的同学学习，这里仅对 【<strong>图解机器学习</strong>】系列进行学习，并同时整理为笔记。</p><p>过往已发布笔记：</p><ul><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489371&amp;idx=1&amp;sn=8d61e3d8cde6acb55361e4eeade4fbb6&amp;scene=21#wechat_redirect" data-linktype="2">1.图解机器学习 | 机器学习基础知识</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489408&amp;idx=1&amp;sn=159692dc7f34d89293cca28172c2e86b&amp;scene=21#wechat_redirect" data-linktype="2">2.图解机器学习 | 模型评估方法与准则</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489441&amp;idx=1&amp;sn=2e5ae676ade3a10a6b3615ad84ef7735&amp;scene=21#wechat_redirect" data-linktype="2">3.图解机器学习 | 决策树模型详解</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTk0Mzg2Nw==&amp;mid=2247489455&amp;idx=1&amp;sn=903797623f07b3ae01e37b52dbce5159&amp;scene=21#wechat_redirect" data-linktype="2">4.图解机器学习-随机森林分类模型详解</a></p></li></ul><p>今天主要介绍<strong>回归树的原理知识</strong>。</p><p>大家在前面的部分学习到了使用决策树进行<strong>分类</strong>，实际<strong>决策树也可以用作回归任务，我们叫作回归树</strong>。而回归树的结构还是树形结构，但是属性选择与生长方式和分类的决策树有不同。</p><section><mp-common-profile data-pluginname="mpprofile" data-id="Mzg2OTYyMTcwMw==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/dqVy90DEgE2LcsuIAv2rnmSJhzQUu74VuY9O5Cgne5rAJm4Mwz9BqK2Yd3aJb11bQykstichPwrxkkia161lNUfg/0?wx_fmt=png" data-nickname="ShowMeAI研究中心" data-alias="ShowMeAI-Hub" data-signature="为AI硬核资料库(cool)而生！" data-from="0" data-is_biz_ban="0"></mp-common-profile></section><blockquote><p>本公众号已与ShowMeAI论坛负责人联系，并得到授权转载。在次给做机器学习的朋友们推荐下他们的微信公众号，有兴趣的同学可以自行关注。</p></blockquote><h3><span>1.决策树回归算法核心思想</span></h3><h4><span>1）决策树结构回顾</span></h4><p>我们一起来回顾一下决策树的结构，<strong>决策树的典型结构如下图所示</strong>。</p><p><img data-galleryid="" data-imgfileid="100005809" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFooriaGTXP39ufgjR4Uvrz0Zv8lPDy8MeOk5ic0UraKCvG95tjm6Sv4kOA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFooriaGTXP39ufgjR4Uvrz0Zv8lPDy8MeOk5ic0UraKCvG95tjm6Sv4kOA/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 决策树回归算法; 结构回顾;</figcaption></figure><p>决策树的<strong>学习过程</strong>和<strong>预测过程</strong>如下图所示：</p><p><img data-galleryid="" data-imgfileid="100005810" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoKqghYLzdcQTiavd0KtsuGNq63iaerd82qNVhvaPvicVF1kPEamx2hQdnA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoKqghYLzdcQTiavd0KtsuGNq63iaerd82qNVhvaPvicVF1kPEamx2hQdnA/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 决策树回归算法; 结构回顾;</figcaption></figure><blockquote><p>详细内容可以参考[3.图解机器学习 | 决策树模型详解]()。</p></blockquote><p><strong>主流的决策树算法有</strong>：</p><ul><li><p><strong>ID3</strong>：基于<strong>信息增益</strong>来选择分裂属性（每步选择<strong>信息增益最大</strong>的属性作为分裂节点，树可能是多叉的）。</p></li><li><p><strong>C4.5</strong>：基于<strong>信息增益率</strong>来选择分裂属性（每步选择<strong>信息增益率最大</strong>的属性作为分裂节点，树可能是多叉的）。</p></li><li><p>CART：基于<strong>基尼系数</strong>来构建决策树（每步要求<strong>基尼系数最小，树是二叉的</strong>）。其中：CART树全称Classification And Regression Tree，<strong>即可以用于分类，也可以用于回归</strong>，这里指的<strong>回归树就是 CART 树，ID3和C4.5不能用于回归问题</strong>。</p></li></ul><h4><span>2）回归树的核心思想</span></h4><p>要讲回归树，我们一定会提到CART树，<strong>CART树全称Classification And Regression Trees，包括分类树与回归树</strong>。</p><p><strong>CART的特点是</strong>：假设决策树是<strong>二叉树</strong>，内部结点特征的取值为<strong>「是」和「否」</strong>，右分支是取值为「是」的分支，左分支是取值为「否」的分支。</p><p>这样的决策树等价于「<strong>递归地二分每个特征</strong>」，将输入空间（特征空间）划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。</p><p>设有数据集D，构建回归树的大体思路如下：</p><ul><li><p>① 考虑数据集 D 上的所有特征 j，遍历每一个特征下所有可能的取值或者切分点 s，将数据集 D 划分成两部分 D<sub>1</sub> 和 D<sub>2</sub> 。</p></li><li><p>② 分别计算 D<sub>1</sub> 和 D<sub>2</sub> 的<strong>平方误差和</strong>，选择<strong>最小的平方误差对应的特征与分割点</strong>，生成两个子节点（将数据划分为两部分）。</p></li><li><p><span>③ 对上述两个子节点递归调用步骤 ① ②，直到满足停止条件。</span></p></li></ul><p>回归树构建完成后，就完成了<strong>对整个输入空间的划分</strong>（即完成了回归树的建立）。将整个输入空间划分为多个子区域，<strong>每个子区域输出为该区域内所有训练样本的平均值</strong>。</p><p><img data-galleryid="" data-imgfileid="100005811" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFo251jebcCRg7bqZIjicR7OHbCaOloPpntPoM3GdWEJo26ib89BtiaOsTmg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFo251jebcCRg7bqZIjicR7OHbCaOloPpntPoM3GdWEJo26ib89BtiaOsTmg/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 回归树的核心思想; CART示例1;</figcaption></figure><p><img data-galleryid="" data-imgfileid="100005812" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoMNTCibtnibeQWaR0zLSibeCFYibumrRPJst58JyrK9ve8EBu0rkwK1wtfg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoMNTCibtnibeQWaR0zLSibeCFYibumrRPJst58JyrK9ve8EBu0rkwK1wtfg/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 回归树的核心思想; CART示例2;</figcaption></figure><p>我们知道了回归树其实是<strong>将输入空间划分为M个单元，每个区域的输出值是该区域内所有点y值的平均数</strong>。但我们希望构建最有效的回归树：预测值与真实值差异度最小。下面部分我们展开讲讲，回归树是如何生长的。</p><h3><span>2.启发式切分与最优属性选择</span></h3><h4><span>1）回归树模型示例</span></h4><p>我们用一个经典的棒球案例来解释回归树：根据从业年限和表现，去预估棒球运动员的工资。如下所示，有1987个数据样本，包含322个棒球运动员（数据同一个运动员在不同年份的数据）：</p><ul><li><p><strong>横坐标</strong>是年限。</p></li><li><p><strong>纵坐标</strong>是表现。</p></li><li><p><strong>红黄</strong>表示高收入，<strong>蓝绿</strong>表示低收入。</p></li></ul><p><img data-galleryid="" data-imgfileid="100005813" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoJFvQIibdwVMgxGTLQ1sPxogq3Rq28pNo6wRVNy79OwTdLlfsMzbHiblg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoJFvQIibdwVMgxGTLQ1sPxogq3Rq28pNo6wRVNy79OwTdLlfsMzbHiblg/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 回归树模型示例; 经典的棒球案例;</figcaption></figure><p>这个简单案例中，每个样本数据有<strong>两个特征</strong>：<strong>从业年限years</strong> 和<strong>成绩表现hits</strong>，回归树的决策过程由最终生成的回归树决定，如右图所示：</p><p><img data-galleryid="" data-imgfileid="100005814" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoibrmbtsoF3oPsakziblMpXohcmuuO4G3NSecPLWa1kUwmGOFkxDye7RQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoibrmbtsoF3oPsakziblMpXohcmuuO4G3NSecPLWa1kUwmGOFkxDye7RQ/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 回归树模型示例; 经典的棒球案例;</figcaption></figure><ul><li><p><span>根决策节点是特征years，其划分阈值为4.5，years≤4.5的样本划分到左边，＞4.5的样本划分到右边</span></p></li><li><p><span>第二个决策节点的特征为hits，其划分阈值为117.5，hits≤117.5的样本划分到左边，＞117.5的样本划分到右边。</span></p></li><li><p><span>一个样本顺着决策树的决策条件，走到叶子节点，即可获得预测工资，这里的预测工资总共就3种取值，分别为5.11、6.00、6.74。</span></p></li></ul><p>我们来深入拆解和对应一下，其实回归树构建完成后，实现了对整个空间的划分，如下图所示：</p><p><img data-galleryid="" data-imgfileid="100005815" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFovSs79WL7t8bm2icicNv3dKe8Y5LMoRLjC3fBUy3wjDHtZIcCHw1kwib9g/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFovSs79WL7t8bm2icicNv3dKe8Y5LMoRLjC3fBUy3wjDHtZIcCHw1kwib9g/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 回归树模型示例; 经典的棒球案例;</figcaption></figure><p>实际预测时，<strong>新样本会按照回归树的决策过程，被划分到下图R1、R2、R3之中的一个区域 R<sub>i</sub></strong>，而<strong>这个新样本的预测值（本案例中为棒球运动员的工资）就是它所在区域中所有训练样本的工资平均值</strong>。</p><h4><span>2）回归树构建方法</span></h4><p>下面切到回归树构建的核心：<strong>切分方式</strong>与<strong>属性选择</strong>。</p><p>假设一回归问题，预估结果y∈R，特征向量为X=[x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, …, x<sub>p</sub>]，回归树2个步骤是：</p><ul><li><p>① 把整个特征空间 X 切分成 J 个没有重叠的区域R<sub>1</sub>, R<sub>2</sub>, R<sub>3</sub>, …, R<sub>J</sub></p></li><li><p>② 每个分化的区域中，所有样本我们都给一样的预测结果：<strong>区域中所有样本的平均值</strong>。</p></li></ul><h5><span>（1）贪婪式递归</span></h5><p>仔细观察一下上面的过程，实际上我们希望能找到如下的 RSS 最小的化划分方式 R<sub>1</sub>, R<sub>2</sub>, R<sub>3</sub>, …, R<sub>J</sub>：</p><p><img data-galleryid="" data-imgfileid="100005816" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoQB7b0Qj27t4vWEgMyMAMSAoxyjibDJZfEEuYCjbic8bwCsQCpBBW9egg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoQB7b0Qj27t4vWEgMyMAMSAoxyjibDJZfEEuYCjbic8bwCsQCpBBW9egg/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 回归树构建方法; 切分方式 / 属性选择;</figcaption></figure><ul><li><p>y：每个训练样本的标签构成的标签向量，向量中的每个元素 y<sub>i</sub> 对应的是每个样本的标签。</p></li><li><p>X：特征的集合，x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, …, x<sub>p</sub> 为第1个特征到第p个特征。</p></li><li><p>R<sub>1</sub>, R<sub>2</sub>, R<sub>3</sub>, …, R<sub>J</sub>：整个特征空间划分得来的 J 个不重叠的区域。</p></li><li><p>y<sup>~</sup><sub>R<sub>j</sub></sub>：划分到第 j 个区域 R<sub>j</sub> 的所有样本平均标签值，用这个值作为该区域的预测值，即如果有一个测试样本在测试时落入到该区域，就将该样本的标签值预测为 y<sup>~</sup><sub>R<sub>j</sub></sub> 。</p></li></ul><p>回归树采用的是「自顶向下的<strong>贪婪式</strong>递归方案」。这里的贪婪，指的是<strong>每一次的划分，只考虑当前最优</strong>，而不回头考虑之前的划分。</p><p>从数学上定义，即选择切分的特征 x<sub>j</sub> 以及切分点 s 使得划分后的树<strong>RSS结果最小</strong>，公式如下所示：</p><p><img data-galleryid="" data-imgfileid="100005817" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoGex7F2W3c2AH2wUvGP8XUM3Uicpp4wMtOBgSriabicYm1slNqoBYgnxaw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoGex7F2W3c2AH2wUvGP8XUM3Uicpp4wMtOBgSriabicYm1slNqoBYgnxaw/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 回归树构建方法; 切分方式 / 属性选择;</figcaption></figure><p>但是这个最小化和探索的过程，<strong>计算量是非常非常大的</strong>。我们采用「<strong>探索式的递归二分</strong>」来尝试解决这个问题。</p><h5><span>（2）递归二分</span></h5><p>我们再来看看「<strong>递归切分</strong>」。下方有两个对比图，其中左图是非递归方式切分得到的，而右图是二分递归的方式切分得到的空间划分结果（<strong>下一次划分一定是在之前的划分基础上将某个区域一份为二</strong>）。</p><p><img data-galleryid="" data-imgfileid="100005818" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoWDibIIuxCNNrZVV75FcGEtic3gev4JXicX0VibOicYicdic4tbTL0icarnia1Ew/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoWDibIIuxCNNrZVV75FcGEtic3gev4JXicX0VibOicYicdic4tbTL0icarnia1Ew/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 回归树构建方法; 切分方式 / 属性选择;</figcaption></figure><p><strong>两种方式的差别是</strong>：</p><ul><li><p><span>递归切分一定可以找到一个较优的解</span></p></li><li><p><span>非递归切分穷举不了所有情况，算法上无法实现，可能无法得到一个较好的解。</span></p></li></ul><p><img data-galleryid="" data-imgfileid="100005819" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFo4jAfd7iaAiaEo9wGF9a1g6b2eMTVus1Mc0M1z04GcacoxaHkcEdbR3Ow/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFo4jAfd7iaAiaEo9wGF9a1g6b2eMTVus1Mc0M1z04GcacoxaHkcEdbR3Ow/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 回归树构建方法; 切分方式 / 属性选择;</figcaption></figure><p><strong>回归树总体流程类似于分类树</strong>：分枝时穷举每一个特征可能的划分阈值，来寻找最优切分特征和最优切分点阈值，衡量的方法是平方误差最小化。分枝直到达到预设的终止条件（如叶子个数上限）就停止。</p><blockquote><p>这部分内容感觉没有说清楚关于贪婪式递归和递归二分具体的差异，似乎只说明了贪婪式递归的方法。不过这也足够了。</p><p>关于贪婪式递归的计算方法，详细可以参考：https://zhuanlan.zhihu.com/p/82054400</p></blockquote><p>但通常在处理具体问题时，单一的回归树模型能力有限且有可能陷入过拟合，我们经常会利用集成学习中的Boosting思想，对回归树进行增强，得到的新模型就是提升树（Boosting Decision Tree），进一步，可以得到<strong>梯度提升树</strong>（Gradient Boosting Decision Tree，GBDT），再进一步可以升级到<strong>XGBoost</strong>。通过多棵回归树拟合残差，不断减小预测值与标签值的偏差，从而达到精准预测的目的，在后面我们会介绍这些高级算法。</p><h3><span>3.过拟合与正则化</span></h3><h4><span>1）过拟合问题</span></h4><p>决策树模型存在过拟合风险，通常情况下：</p><ul><li><p>树的规模<strong>太小</strong>会导致模型<strong>效果不佳</strong>。</p></li><li><p>树的规模太大就会造成模型<strong>过拟合</strong>。</p></li></ul><h4><span>2）过拟合问题处理</span></h4><p>对于决策树，我们通常有如下一些策略可以用于缓解过拟合：</p><p><img data-galleryid="" data-imgfileid="100005820" data-ratio="0.38055555555555554" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFojf167QWc73lHNFmgulgvZjJxSvCT0tm6fUNnP9XqP7wrjUA0J2cdYQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFojf167QWc73lHNFmgulgvZjJxSvCT0tm6fUNnP9XqP7wrjUA0J2cdYQ/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>回归树模型详解; 过拟合与正则化; 过拟合问题处理;</figcaption></figure><h5><span>（1）约束控制树的过度生长</span></h5><ul><li><p><strong>限制树的深度</strong>：当达到设置好的最大深度时结束树的生长。</p></li><li><p><strong>分类误差法</strong>：当树继续生长无法得到客观的分类误差减小，就停止生长。</p></li><li><p><strong>叶子节点最小数据量限制</strong>：一个叶子节点的数据量过小，树停止生长。</p></li></ul><h5><span>（2）剪枝</span></h5><p><strong>约束树生长的缺点就是提前扼杀了其他可能性</strong>，过早地终止了树的生长。</p><p>我们也可以等待树生长完成以后再进行剪枝，即所谓的<strong>后剪枝</strong>，而后剪枝算法主要有以下几种：</p><ul><li><p>Reduced-Error Pruning（REP，<strong>错误率降低剪枝</strong>）。</p></li><li><p>Pesimistic-Error Pruning（PEP，<strong>悲观错误剪枝</strong>）。</p></li><li><p>Cost-Complexity Pruning（CCP，<strong>代价复杂度剪枝</strong>）。</p></li><li><p>Error-Based Pruning（EBP，<strong>基于错误的剪枝</strong>）。</p></li></ul><h4><span>3）正则化</span></h4><p>对于回归树而言，在剪枝过程中我们会添加正则化项衡量。如下所示，考虑剪枝后得到的子树｛T<sub>α</sub>｝，其中 α 是正则化项的系数。当固定住 α 之后，最佳的 T<sub>α</sub> 就是使得下列式子值最小的子树。</p><p><img data-galleryid="" data-imgfileid="100005821" data-ratio="0.2600732600732601" data-s="300,640" data-src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoAQY7DOt8QINrV1EsJU2D4wZy2JBWJR6X7XKuvdAKU9vToMiahywtwHw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="546" src="https://mmbiz.qpic.cn/sz_mmbiz_png/fOo3cmIWvJrermpP7nV5ziacqwIyiaqicFoAQY7DOt8QINrV1EsJU2D4wZy2JBWJR6X7XKuvdAKU9vToMiahywtwHw/640?wx_fmt=png&amp;from=appmsg"></p><figure><figcaption>正则化公式</figcaption></figure><ul><li><p><span>|T| 是回归树叶子节点的个数。</span></p></li><li><p><span>α 可以通过交叉验证去选择。</span></p></li></ul><h3><span>后记</span></h3><p>这部分内容详细介绍了回归树的算法模型原理，在理解了决策树的基础上，还是比较容易理解，不过关于贪婪式递归和递归二分具体的差异没有解释的非常清楚，但是不影响我们的使用。这对于后面的机器学习真的很有帮助，再次感叹 <strong>ShowMeAI</strong> 论坛整理的真的非常棒，对数学能力不太高的我真的非常友善了！</p><blockquote><p>https://www.showmeai.tech/tutorials/34?articleId=192</p></blockquote></section><p><br></p><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/bHb0zPwYEDshGjYunxFbgA",target="_blank" rel="noopener noreferrer">原文链接</a>
