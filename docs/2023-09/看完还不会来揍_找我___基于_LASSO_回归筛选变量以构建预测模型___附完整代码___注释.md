---
title: "看完还不会来揍/找我 | 基于 LASSO 回归筛选变量以构建预测模型 | 附完整代码 + 注释"
date: 2023-09-05T09:12:58Z
draft: ["false"]
tags: [
  "fetched",
  "生信小白要知道"
]
categories: ["Acdemic"]
---
看完还不会来揍/找我 | 基于 LASSO 回归筛选变量以构建预测模型 | 附完整代码 + 注释 by 生信小白要知道
------
<div><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com"><h1 data-tool="mdnice编辑器"><span></span><span>基于 LASSO 回归筛选变量以构建预测模型</span></h1><blockquote data-tool="mdnice编辑器"><p>在构建疾病风险评估和预测模型时，我们会想要纳入尽可能多的指标去构建模型。这么做虽然会更准确，但却不实用。因为医生不可能让所有刚入院的患者把所有的检查都做一遍。我们更希望在尽可能知道较少指标的情况下而不损失风险评估准确性，这样才可以实现效益和效率最大化。又或者，在大量基因中选取可能对疾病有重要影响的基因（比如我们前面在<a target="_blank" href="http://mp.weixin.qq.com/s?__biz=Mzg5NjE1NTc1OA==&amp;mid=2247485273&amp;idx=1&amp;sn=484818f313c169582472c7bb60e7ae3f&amp;chksm=c0042e58f773a74e421508a43fbb631b2d69a1ec586daf730653e89dba463d669d946ff67f76&amp;scene=21#wechat_redirect" textvalue="看完还不会来揍我 | 差异分析三巨头 —— DESeq2、edgeR 和 limma 包 | 附完整代码 + 注释" linktype="text" imgurl="" imgdata="null" data-itemshowtype="0" tab="innerlink" data-linktype="2">看完还不会来揍我 | 差异分析三巨头 —— DESeq2、edgeR 和 limma 包 | 附完整代码 + 注释</a>中得到的差异基因），用于构建风险预测模型。这个时候，我们就可以使用 LASSO 回归在众多变量中进行筛选，获取重要变量，也就是特征，用于构建模型啦！今天我们就来分享一下具体要怎么做喽！</p></blockquote><p data-tool="mdnice编辑器">我们先介绍一下Lasso回归的原理，大家如果不需要的话，也可以跳过这部分直接去看用法和代码部分哟！</p><p data-tool="mdnice编辑器"><strong>Lasso回归（Least Absolute Shrinkage and Selection Operator）</strong>是一种用于线性回归问题的正则化方法，它可以用来降低模型的复杂性，防止过拟合，并选择重要的特征变量。</p><h2 data-tool="mdnice编辑器"><span></span><span>LASSO 回归的原理</span></h2><p data-tool="mdnice编辑器">Lasso回归的原理基于在线性回归（想深入了解线性回归的小伙伴们，详见<a href="https://mp.weixin.qq.com/mp/appmsgalbum?__biz=Mzg5NjE1NTc1OA==&amp;action=getalbum&amp;album_id=3026152859105230849&amp;scene=173&amp;from_msgid=2247484608&amp;from_itemidx=1&amp;count=3&amp;nolastread=1#wechat_redirect" data-linktype="2">跟我一起啃西瓜书</a>系列中的第三章<strong>线性模型</strong>3.2小节）的基础上添加一个<strong>L1正则化项</strong>（不要慌！后面会讲！），以约束模型的参数估计。L1正则化项是参数向量中绝对值之和，它的存在<strong>迫使许多参数变为零</strong>，从而实现了<strong>变量选择（特征筛选）</strong>的功能。Lasso回归的优化目标是最小化以下损失函数：</p><span data-tool="mdnice编辑器"><section role="presentation" data-formula="L(\beta) = \frac{1}{2N}\sum_{i=1}^{N}(y_i - \beta_0 - \sum_{j=1}^{p}x_{ij}\beta_j)^2 + \lambda\sum_{j=1}^{p}|\beta_j|" data-formula-type="block-equation"><embed src="https://mmbiz.qpic.cn/mmbiz_svg/LIND77SSexiboh1N9bqDWMTgc0bL6k96r9sG77UnOHmFHgpagFzOShz5mRgQvFTCzFvibmt4ibuibQYSibkMg9UxJwKh8buKT8ZRD/0?wx_fmt=svg" data-type="svg+xml"></embed></section></span><p data-tool="mdnice编辑器">其中：</p><ul data-tool="mdnice编辑器"><li><section><span><span role="presentation" data-formula="N" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -683 888 683" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="4E" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g></svg></span></span> 是样本数量。</section></li><li><section><span><span role="presentation" data-formula="p" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -442 503 636" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="70" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g></g></g></svg></span></span> 是特征变量的数量。</section></li><li><section><span><span role="presentation" data-formula="y_i" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -442 784 647" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="79" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></span></span> 是第 <span><span role="presentation" data-formula="i" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -661 345 672" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></span></span> 个观测值的目标值。</section></li><li><section><span><span role="presentation" data-formula="x_{ij}" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -442 1157.3 736.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></g></svg></span></span> 是第 <span><span role="presentation" data-formula="i" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -661 345 672" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></span></span> 个观测值的第 <span><span role="presentation" data-formula="j" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -661 412 865" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></svg></span></span> 个特征变量的值。</section></li><li><section><span><span role="presentation" data-formula="\beta_j" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -705 907.3 999.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mi" transform="translate(566, -150) scale(0.707)"><path data-c="6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></span></span> 是第 <span><span role="presentation" data-formula="j" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -661 412 865" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></svg></span></span> 个特征变量的系数，当其为 0 时，<span><span role="presentation" data-formula="x_{ij}" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -442 1157.3 736.2" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="78" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></g></svg></span></span> 就被踢出方程咯，这样就达到筛选变量的效果啦！</section></li><li><section><span><span role="presentation" data-formula="\beta_0" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -705 969.6 899" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mn" transform="translate(566, -150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></span></span> 是截距项。</section></li><li><section><span><span role="presentation" data-formula="\lambda" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -694 583 706" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="3BB" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g></g></svg></span></span> 是正则化参数，用于控制正则化的强度。较大的 <span><span role="presentation" data-formula="\lambda" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -694 583 706" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="3BB" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g></g></svg></span></span> 值将导致更多的参数估计为零。</section></li></ul><blockquote data-tool="mdnice编辑器"><p><strong>简单介绍一下正则化。</strong></p><p><strong>正则化（Regularization）</strong>是一种在机器学习和统计建模中常用的技术，旨在帮助模型更好地泛化到未见过的数据，避免过拟合问题。<strong>过拟合（overfitting）</strong>是指模型在训练数据上表现得非常好，但在未见过的测试数据上表现较差的情况。简单来说，模型过度“记忆”了训练数据的细节和噪声，导致在新数据上的预测表现不佳。过拟合通常出现在模型过于复杂或者训练数据较少的情况下。就好像是一个学生过度死记硬背了大量的题目答案，但实际上并没有理解问题的本质。因此，当遇到新的类似问题时，虽然能够准确回答原有题目，但对于新问题的推广能力较差。</p><figure><img data-ratio="0.381057268722467" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BiaO7Q1W1s9VCKbGMTPucRyQYGyN2biaqxWR98XkHlDV1MC180AIu60bw/640?wx_fmt=png" data-type="png" data-w="908" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BiaO7Q1W1s9VCKbGMTPucRyQYGyN2biaqxWR98XkHlDV1MC180AIu60bw/640?wx_fmt=png"></figure><p><strong>正则化就好像是在机器学习中的模型训练过程中施加的一种规则，它会对模型进行“限制”或“惩罚”，以确保它不会变得过于复杂。</strong></p><p>想象一下，你正在尝试通过拟合一条曲线来拟合一组散点数据，但你担心曲线会过于弯曲，导致在散点之间出现怪异的起伏。正则化就像是在你的曲线拟合问题中引入一个规则，告诉你不要让曲线变得太弯曲。<strong>岭回归（L2正则化）和Lasso回归（L1正则化）</strong>就是为了解决线性回归中的过拟合问题而出现的，<strong>弹性网络是L1和L2正则化的结合</strong>，可以平衡两者的影响。</p><ul><li><section><strong>L1正则化</strong>（也称为Lasso正则化）：它告诉你，如果曲线中有一些不太重要的部分，就将它们收缩到零，从而削减模型的复杂度。这相当于在模型中删除不必要的特征或参数，使模型更简单。</section></li><li><section><strong>L2正则化</strong>（也称为岭正则化）：它告诉你，如果曲线的某些部分变得过于陡峭，就要降低它们的陡峭程度。这使得曲线的参数不能太大，从而控制了模型的复杂度（在本文的后半部分，L2正则化马上就要登场啦）。</section></li></ul><p>正则化的目标是在模型的损失函数中添加一个额外的项，这个项与模型参数相关，通过这个项来限制参数的大小或稀疏性，从而防止模型在训练数据上过度拟合。这种“限制”或“惩罚”有助于模型更好地泛化到新数据，降低了过拟合的风险。</p></blockquote><h2 data-tool="mdnice编辑器"><span></span><span>LASSO 回归与岭回归、弹性网络的区别</span></h2><ol data-tool="mdnice编辑器"><li><section><p><strong>Lasso回归（Least Absolute Shrinkage and Selection Operator）</strong>：Lasso回归引入了L1正则化项，通过约束参数向量的绝对值之和来实现稀疏性。这使得Lasso能够执行特征选择，将许多参数估计为零，从而简化模型。Lasso回归倾向于在高维数据中选择少数关键特征，提高了模型的可解释性。</p><p>Lasso回归的优化目标如下：</p><span><section role="presentation" data-formula="L(\beta) = \frac{1}{2N}\sum_{i=1}^{N}(y_i - \beta_0 - \sum_{j=1}^{p}x_{ij}\beta_j)^2 + \lambda\sum_{j=1}^{p}|\beta_j|" data-formula-type="block-equation"><embed src="https://mmbiz.qpic.cn/mmbiz_svg/LIND77SSexiboh1N9bqDWMTgc0bL6k96r9sG77UnOHmFHgpagFzOShz5mRgQvFTCzFvibmt4ibuibQYSibkMg9UxJwKh8buKT8ZRD/0?wx_fmt=svg" data-type="svg+xml"></embed></section></span></section></li><li><section><p><strong>岭回归（Ridge Regression）</strong>：与Lasso不同，岭回归使用L2正则化项，其目标是最小化参数的平方和，而不是绝对值和。这意味着岭回归不会将参数估计为零，但会减小参数的绝对值，从而减小参数的大小。因此岭回归适合在存在多个相关特征的情况下稳定模型，但不执行特征选择。</p><p>岭回归的优化目标如下：</p><span><section role="presentation" data-formula="L(\beta) = \frac{1}{2N}\sum_{i=1}^{N}(y_i - \beta_0 - \sum_{j=1}^{p}x_{ij}\beta_j)^2 + \lambda\sum_{j=1}^{p}\beta_j^2" data-formula-type="block-equation"><embed src="https://mmbiz.qpic.cn/mmbiz_svg/LIND77SSexiboh1N9bqDWMTgc0bL6k96ric6m5Ong0b9AicWW5ib3AUB9A48IRhg2hbzt2oWkIcygUcBAwE8DWoUlTUdmvcic61BZ/0?wx_fmt=svg" data-type="svg+xml"></embed></section></span></section></li><li><section><p><strong>弹性网络（Elastic Net）</strong>：弹性网络是L1和L2正则化的组合，综合了岭回归和Lasso的优点。它的损失函数包括L1和L2正则化项，可以同时控制参数的大小和稀疏性，在特征选择和参数缩减之间进行权衡。弹性网络适用于数据集中既存在高度相关特征又需要进行特征选择的情况。</p><p>弹性网络的优化目标如下：</p><span><section role="presentation" data-formula="L(\beta) = \frac{1}{2N}\sum_{i=1}^{N}(y_i - \beta_0 - \sum_{j=1}^{p}x_{ij}\beta_j)^2 + \lambda_1\sum_{j=1}^{p}|\beta_j| + \lambda_2\sum_{j=1}^{p}\beta_j^2" data-formula-type="block-equation"><embed src="https://mmbiz.qpic.cn/mmbiz_svg/LIND77SSexiboh1N9bqDWMTgc0bL6k96r9cO3a3nAm6299xmiclALKrbWNfZtJxrSMDiazzolNxANLyC7w96YoaY7TEkiaud4Qr1/0?wx_fmt=svg" data-type="svg+xml"></embed></section></span></section></li></ol><h2 data-tool="mdnice编辑器"><span></span><span>为何选择 LASSO 回归进行特征筛选</span></h2><p data-tool="mdnice编辑器">选择在什么时候使用岭回归、Lasso回归或弹性网络来筛选变量，其实取决于具体情况和数据性质，有一些指导原则大家可以了解一下。</p><ol data-tool="mdnice编辑器"><li><section><strong>当你怀疑有多个特征对目标变量有影响，而且这些特征可能存在高度相关性时，可以考虑使用岭回归</strong>：岭回归通过L2正则化可以减小参数的绝对值，但不会将参数估计为零，因此适合在存在多个相关特征的情况下稳定模型。它不会执行严格的特征选择，而是缩小了参数的幅度。</section></li><li><section><strong>当你怀疑只有少数几个特征对目标变量有显著影响，并且希望执行特征选择时，可以考虑使用Lasso回归</strong>：Lasso回归通过L1正则化倾向于将许多参数估计为零，从而实现了特征选择的功能。如果你想要简化模型并提高可解释性，Lasso通常是一个不错的选择。</section></li><li><section><strong>当你既想要特征选择，又想要保留某些相关特征时，可以考虑使用弹性网络</strong>：弹性网络结合了L1和L2正则化，允许你在特征选择和参数缩减之间进行权衡。这在存在多个相关特征，但你仍然希望进行一定程度的特征选择时非常有用。通过调整两个正则化参数的值，你可以控制模型的行为。</section></li></ol><p data-tool="mdnice编辑器">总的来说，选择哪种正则化方法应该基于以下考虑：</p><ul data-tool="mdnice编辑器"><li><section>数据性质：了解数据的特征和相关性，以确定哪种正则化方法更适合。</section></li><li><section>目标：确定你是否更关注特征选择、参数缩减或平衡两者。</section></li><li><section>交叉验证：通常需要使用交叉验证来选择适当的正则化参数，以确保模型的性能。</section></li></ul><p data-tool="mdnice编辑器">通过上面的介绍，大家应该对我们选择Lasso进行特征筛选有了一定了解。一般我们在临床预测模型构建过程中，都会期望找到相对较少的特征数量，希望用更少的特征就可以对目标变量产生较大影响，且希望模型更具可解释性。在临床预测中，可能存在大量的生物医学特征，其中一些可能不是很重要或相关。Lasso可以自动将某些特征的系数估计为零，从而实现特征选择，帮助减少模型中的不必要的特征，这样生成的模型更容易解释，只包含最重要的特征，更便于医生或临床决策者理解模型的预测结果。而且Lasso在样本数量相对较少的情况下也可以工作良好，尤其是在高维数据中，它有助于防止过拟合，即使数据点有限（试问大家能拿到多少有效样本哈哈哈哈哈哈哈呜呜呜呜呜呜呜呜呜呜，帮你们哭一下）。所以，我们见到的临床模型多数是基于Lasso回归构建的！</p><blockquote data-tool="mdnice编辑器"><p>当然！咱也不能无脑Lasso，还是要多尝试滴！万一其他模型更合适呢！</p></blockquote><p data-tool="mdnice编辑器">这个时候，可能会有小伙伴有疑问，那既然弹性网络是L1和L2正则化的组合，看起来似乎它考虑更全面呐，我们为什么不直接用它来进行特征筛选呢？</p><p data-tool="mdnice编辑器">其实，当数据集中存在高度相关的特征时，Lasso有可能随机选择其中的一个特征并将其他特征的系数设为零，这可能导致结果不稳定，因为具体选择哪个特征取决于模型的随机性。这在高维数据中尤其成问题。而弹性网络可以平衡这两种正则化的影响，既能够进行特征选择，又能够处理多重共线性问题。它不会像Lasso那样过于随机地选择特征，所以更加稳定。</p><p data-tool="mdnice编辑器">但是吧，弹性网络也不一定在所有数据中都表现最好。当数据量较大、质量较好，且问题适用于线性模型时，弹性网络可能表现良好。当数据量较小、数据质量较差，或问题具有高度非线性性质时，弹性网络的效果可能就会表现不如其他模型。</p><p data-tool="mdnice编辑器"><strong>在实际应用中，我们还是要尝试不同的模型（包括弹性网络、岭回归、Lasso等）并使用交叉验证来确定哪种模型在给定问题上表现最佳。</strong></p><h2 data-tool="mdnice编辑器"><span></span><span>LASSO 回归在预测模型中的用法</span></h2><h3 data-tool="mdnice编辑器"><span></span><span>用法一</span><span></span></h3><p data-tool="mdnice编辑器">这也是最常见的用法，采用Lasso进行回归，根据十折交叉验证，筛选得到候选预测因子。然后用候选预测因子继续做后续的多因素Logsitic回归或者多因素Cox回归，得到最终的临床预测模型，用于后续的Nomogram等分析。</p><p data-tool="mdnice编辑器">比如：Mo R, Shi R, Hu Y, Hu F. Nomogram-Based Prediction of the Risk of Diabetic Retinopathy: A Retrospective Study. J Diabetes Res. 2020 Jun 7;2020:7261047.<img data-ratio="0.43329532497149376" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BxS2icUczZYiatGKEO6RsO7zpPPyzibl8JeI3F0GrEI8afhY4rwYuvwGXA/640?wx_fmt=png" data-type="png" data-w="877" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BxS2icUczZYiatGKEO6RsO7zpPPyzibl8JeI3F0GrEI8afhY4rwYuvwGXA/640?wx_fmt=png"></p><p data-tool="mdnice编辑器">这篇文章采用Lasso回归，对19个预测因子进行筛选，最终选得7个预测因子。然后对7个预测因子构建多因素Logistic回归模型。<img data-ratio="0.6576576576576577" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220By0vqhJEPGHenj79ZJcicCPLRydnStRqOuBSnic0CsZWFpxcJ3SibaYf3Q/640?wx_fmt=png" data-type="png" data-w="444" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220By0vqhJEPGHenj79ZJcicCPLRydnStRqOuBSnic0CsZWFpxcJ3SibaYf3Q/640?wx_fmt=png"></p><p data-tool="mdnice编辑器">然后作者就基于这7个因素构建列线图（Nomogram），并进行了后续3个度的评价。<img data-ratio="0.4449489216799092" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220B5Xh8CUy8Yd8pf77pdibraEicI9QqnN1R6Rcff2qjC8HKyicvj8IKW8K2A/640?wx_fmt=png" data-type="png" data-w="881" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220B5Xh8CUy8Yd8pf77pdibraEicI9QqnN1R6Rcff2qjC8HKyicvj8IKW8K2A/640?wx_fmt=png"></p><blockquote data-tool="mdnice编辑器"><p>这些图的绘制，我们后续都会陆续更新哟！</p></blockquote><h3 data-tool="mdnice编辑器"><span></span><span>用法二</span><span></span></h3><p data-tool="mdnice编辑器">当我们研究的因素较多，可以先进行单因素Logistic或Cox回归，先筛选一批可能的预测因子;然后再采用Lasso回归进行筛选，将筛选得到的预测因子，再次进行多因素Logistic或Cox回归，确定最终模型。</p><p data-tool="mdnice编辑器">比如：Liu J, Huang X, Yang W, Li C, Li Z, Zhang C, Chen S, Wu G, Xie W, Wei C, Tian C, Huang L, Jeen F, Mo X, Tang W. Nomogram for predicting overall survival in stage II-III colorectal cancer. Cancer Med. 2020 Apr;9(7):2363-2371.</p><p data-tool="mdnice编辑器">这篇文章通过单变量分析发现59个因素，基于专业背景知识又加了5个 P &gt; 0.05 的因素（有的时候我们建模，发现某个专业背景上有意义的变量，却没有进入多因素分析阶段，就可以通过这样的表述把它加进去），共64个因素。然后对这个64因素进行Lasso筛选，发现了6个系数不为零，也就是有意义的预测因子。<img data-ratio="0.583710407239819" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BZ0peibH6icZwI24LpBYKQqlZuA02ULEva15WVyH9rJlNwa6RicfUQI4cw/640?wx_fmt=png" data-type="png" data-w="884" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BZ0peibH6icZwI24LpBYKQqlZuA02ULEva15WVyH9rJlNwa6RicfUQI4cw/640?wx_fmt=png"></p><p data-tool="mdnice编辑器">作者对这6个预测因子，做了多因素Cox回归，最终发现4个因子，并使用这4个因子构建最终的最优模型。<img data-ratio="0.5506912442396313" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220Bwqzvtsphdico9rEs2oGIgZsNVYiaj3ulUg4zkmCbiaXvRefnJiazSjPcFQ/640?wx_fmt=png" data-type="png" data-w="434" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220Bwqzvtsphdico9rEs2oGIgZsNVYiaj3ulUg4zkmCbiaXvRefnJiazSjPcFQ/640?wx_fmt=png"></p><p data-tool="mdnice编辑器">然后构建了4个因子的Nomo图以及后续的3个度的验证等等！<img data-ratio="0.3724373576309795" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BX4VibTQLTqhr1pyvmjWSkya0QiaU21oVNkNbO67F80CJDeInzUiaGdibPQ/640?wx_fmt=png" data-type="png" data-w="878" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BX4VibTQLTqhr1pyvmjWSkya0QiaU21oVNkNbO67F80CJDeInzUiaGdibPQ/640?wx_fmt=png"></p><h3 data-tool="mdnice编辑器"><span></span><span>用法三</span><span></span></h3><p data-tool="mdnice编辑器">这种用法相对少见，就是直接用Lasso-Logit或者Lasso-Cox进行变量筛选，筛选后，用最小误差解构建模型，无需再对Lasso筛选得到的多个因子，再进行多因素的Logistic或Cox进行分析。</p><h2 data-tool="mdnice编辑器"><span></span><span>LASSO 回归存在的问题</span></h2><p data-tool="mdnice编辑器">尽管Lasso回归应用广泛且优点多多，但不可避免的，它也存在一些问题和局限性：</p><ol data-tool="mdnice编辑器"><li><section>参数选择问题：选择合适的正则化参数 <span><span role="presentation" data-formula="\lambda" data-formula-type="inline-equation"><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -694 583 706" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="3BB" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g></g></svg></span></span> 是一个挑战，需要进行交叉验证或其他模型选择方法。</section></li><li><section>估计偏差：Lasso倾向于对参数进行严格的稀疏化，可能导致估计偏差，尤其是在特征之间存在高度相关性时。</section></li><li><section>难以处理大规模数据集：对于大规模数据集，Lasso的计算成本可能很高，因为要考虑所有特征的组合。</section></li></ol><h2 data-tool="mdnice编辑器"><span></span><span>LASSO 回归代码实现及结果解读</span></h2><p data-tool="mdnice编辑器">数据用到的是我们之前<a href="https://mp.weixin.qq.com/s?__biz=Mzg5NjE1NTc1OA==&amp;mid=2247485273&amp;idx=1&amp;sn=484818f313c169582472c7bb60e7ae3f&amp;scene=21#wechat_redirect" data-linktype="2">看完还不会来揍我 | 差异分析三巨头 —— DESeq2、edgeR 和 limma 包 | 附完整代码 + 注释</a>中得到的差异基因表达矩阵，进行Lasso回归需要样本分类信息，我们使用生存数据，大家可以通过 UCSC Xena（http://xena.ucsc.edu/）进行下载。从 UCSC Xena 下载数据的方法详见：<a href="https://mp.weixin.qq.com/s?__biz=Mzg5NjE1NTc1OA==&amp;mid=2247485084&amp;idx=1&amp;sn=9499751c3ca7adf2e679aefd4c0d2c5a&amp;scene=21#wechat_redirect" data-linktype="2">看完还不会来揍/找我 | TCGA 与 GTEx 数据库联合分析 | 附完整代码 + 注释</a>的<strong>数据准备</strong>部分。<img data-ratio="0.5351851851851852" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BAPGj6DDUIwxEbKTnGcu6vmibyVU0iaibDukFIdGKcVZkeV4SwhRP5rVcA/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BAPGj6DDUIwxEbKTnGcu6vmibyVU0iaibDukFIdGKcVZkeV4SwhRP5rVcA/640?wx_fmt=png"></p><blockquote data-tool="mdnice编辑器"><p>如果大家想快快实战，不想耗费时间去运行前面的差异分析步骤得到差异分析表达矩阵或者下载生存数据，完全没问题！我已经把用到的数据上传到了GitHub，大家可以在公众号后台回复<code>lasso回归</code>，即可得到存放这些数据的链接。</p></blockquote><p data-tool="mdnice编辑器">咱们正式开始！</p><p data-tool="mdnice编辑器">代码看起来有点长，但其实正式的Lasso回归部分一点都不长！前面大多是数据处理部分，大家如果有现成的数据，直接从”<strong>现在正式开始lasso回归！</strong>”这里开始看就好啦！</p><pre data-tool="mdnice编辑器"><span></span><code><span>################################# Lasso 回归 ###################################</span><span><br><br></span><span># 加载包</span><span><br></span><span># 如果大家在运行的时候发现某个函数不存在或者不能用，可能是忘记加载包啦</span><span><br></span><span># 我这里如果没有写全的话，先给大家说声抱歉，因为有时候我的环境里可能已经加载了就直接用了</span><span><br></span><span># 所以可能会忘记写某个包，大家不要慌，如果遇到了且不知道函数来自哪个包的时候</span><span><br></span><span><span># 咱们</span>去<span>百度谷歌噼里啪啦查一下就好！</span><span>不怕！</span><span>你是最棒的！</span></span><span><br></span><span>library</span><span>(glmnet)<br></span><span>library</span><span>(survival)<br></span><span>library</span><span>(dplyr)<br><br></span><span>### 加载数据</span><span><br><br></span><span># 咱就今天使用的数据是BRCA数据集中的癌症样本，如果大家想了解这个数据是如何得出的，</span><span><br></span><span># 可以查看：https://mp.weixin.qq.com/s/0DVzPXrNCQgG7kTtMw74jQ</span><span><br>brca_tumor &lt;- readRDS(</span><span>"./lasso_data/brca_tumor.rds"</span><span>)<br>head(brca_tumor)[</span><span>1</span><span>:</span><span>5</span><span>, </span><span>1</span><span>:</span><span>4</span><span>]<br></span><span>#          TCGA-E9-A1NI-01A TCGA-A1-A0SP-01A TCGA-BH-A201-01A TCGA-E2-A14T-01A</span><span><br></span><span># TSPAN6           8.787903        12.064743        11.801304        10.723661</span><span><br></span><span># TNMD             0.000000         2.807355         4.954196         6.658211</span><span><br></span><span># DPM1            11.054604        11.292897        11.314017        11.214926</span><span><br></span><span># SCYL3           10.246741         9.905387        11.117643        12.093748</span><span><br></span><span># C1orf112         8.965784        10.053926         9.957102         9.503826</span><span><br><br></span><span># deseq2得到的差异基因</span><span><br>load(</span><span>"./lasso_data/DEG_deseq2.Rdata"</span><span>)<br>logFC = </span><span>2.5</span><span><br>P.Value = </span><span>0.01</span><span><br>k1 &lt;- (DEG_deseq2$pvalue &lt; P.Value) &amp; (DEG_deseq2$log2FoldChange &lt; -logFC)<br>k2 &lt;- (DEG_deseq2$pvalue &lt; P.Value) &amp; (DEG_deseq2$log2FoldChange &gt; logFC)<br>DEG_deseq2 &lt;- mutate(DEG_deseq2, change = ifelse(k1, </span><span>"down"</span><span>, ifelse(k2, </span><span>"up"</span><span>, </span><span>"stable"</span><span>)))<br>table(DEG_deseq2$change)<br></span><span># down stable     up </span><span><br></span><span>#  693  43491   1339 </span><span><br><br></span><span># edgeR得到的差异基因</span><span><br>load(</span><span>"./lasso_data/DEG_edgeR.Rdata"</span><span>)<br>logFC = </span><span>2.5</span><span><br>P.Value = </span><span>0.01</span><span><br>k1 &lt;- (DEG_edgeR$PValue &lt; P.Value) &amp; (DEG_edgeR$logFC &lt; -logFC)<br>k2 &lt;- (DEG_edgeR$PValue &lt; P.Value) &amp; (DEG_edgeR$logFC &gt; logFC)<br>DEG_edgeR &lt;- mutate(DEG_edgeR, change = ifelse(k1, </span><span>"down"</span><span>, ifelse(k2, </span><span>"up"</span><span>, </span><span>"stable"</span><span>)))<br>table(DEG_edgeR$change)<br></span><span># down stable     up </span><span><br></span><span>#  634  30531   1904 </span><span><br><br></span><span># limma得到的差异基因</span><span><br>load(</span><span>"./lasso_data/DEG_limma_voom.Rdata"</span><span>)<br>logFC = </span><span>2.5</span><span><br>P.Value = </span><span>0.01</span><span><br>k1 &lt;- (DEG_limma_voom$P.Value &lt; P.Value) &amp; (DEG_limma_voom$logFC &lt; -logFC)<br>k2 &lt;- (DEG_limma_voom$P.Value &lt; P.Value) &amp; (DEG_limma_voom$logFC &gt; logFC)<br>DEG_limma_voom &lt;- mutate(DEG_limma_voom, change = ifelse(k1, </span><span>"down"</span><span>, ifelse(k2, </span><span>"up"</span><span>, </span><span>"stable"</span><span>)))<br>table(DEG_limma_voom$change)<br></span><span># down stable     up </span><span><br></span><span>#  404  17753    248 </span><span><br><br></span><span># 定义函数挑选差异基因</span><span><br>deg_filter &lt;- </span><span>function</span><span>(df){<br>  rownames(df)[df$change != </span><span>"stable"</span><span>]<br>}<br><br></span><span># 取交集</span><span><br>all_degs &lt;- intersect(intersect(deg_filter(DEG_deseq2), deg_filter(DEG_edgeR)), deg_filter(DEG_limma_voom))<br>length(all_degs)<br></span><span># [1] 442</span><span><br><br></span><span># 三个包的结果取交集得到442个差异基因</span><span><br><br></span><span># 获取取交集得到的差异基因的表达矩阵</span><span><br>exp_brca_final &lt;- exp_brca %&gt;% filter(rownames(exp_brca) %</span><span>in</span><span>% all_degs)<br>dim(exp_brca_final)<br></span><span># [1]  442 1217</span><span><br><br>head(exp_brca_final)[</span><span>1</span><span>:</span><span>5</span><span>, </span><span>1</span><span>:</span><span>4</span><span>]<br></span><span>#        TCGA-E9-A1NI-01A TCGA-A1-A0SP-01A TCGA-BH-A201-01A TCGA-E2-A14T-01A</span><span><br></span><span># KLHL13         6.044394         7.734710        10.702173         7.321928</span><span><br></span><span># HSPB6          6.189825         8.312883         9.709084        10.121534</span><span><br></span><span># PDK4           9.306062        11.454299        11.350387        11.959640</span><span><br></span><span># MEOX1          3.700440         7.599913         8.455327         8.199672</span><span><br></span><span># SCN4A          3.169925         4.459432         7.515700         6.643856</span><span><br><br></span><span># 加载生存数据</span><span><br>sur_brac &lt;- read.table(</span><span>"./lasso_data/TCGA-BRCA.survival.tsv"</span><span>, header = </span><span>T</span><span>)<br>head(sur_brac)<br></span><span>#              sample OS     _PATIENT OS.time</span><span><br></span><span># 1: TCGA-C8-A275-01A  0 TCGA-C8-A275       1</span><span><br></span><span># 2: TCGA-BH-A1F8-11B  1 TCGA-BH-A1F8       1</span><span><br></span><span># 3: TCGA-BH-A1F8-01A  1 TCGA-BH-A1F8       1</span><span><br></span><span># 4: TCGA-AC-A7VC-01A  0 TCGA-AC-A7VC       1</span><span><br></span><span># 5: TCGA-AN-A0AM-01A  0 TCGA-AN-A0AM       5</span><span><br></span><span># 6: TCGA-C8-A1HJ-01A  0 TCGA-C8-A1HJ       5</span><span><br><br></span><span># 我们再看一下表达数据</span><span><br>head(exp_brca_final)[</span><span>1</span><span>:</span><span>5</span><span>, </span><span>1</span><span>:</span><span>4</span><span>]<br></span><span>#        TCGA-E9-A1NI-01A TCGA-A1-A0SP-01A TCGA-BH-A201-01A TCGA-E2-A14T-01A</span><span><br></span><span># KLHL13         6.044394         7.734710        10.702173         7.321928</span><span><br></span><span># HSPB6          6.189825         8.312883         9.709084        10.121534</span><span><br></span><span># PDK4           9.306062        11.454299        11.350387        11.959640</span><span><br></span><span># MEOX1          3.700440         7.599913         8.455327         8.199672</span><span><br></span><span># SCN4A          3.169925         4.459432         7.515700         6.643856</span><span><br><br></span><span># 咱们需要转置一下，然后合并！</span><span><br><br></span><span># 整合表达数据和生存数据</span><span><br>exp_brca_final &lt;- as.data.frame(t(exp_brca_final))<br>exp_brca_final$sample &lt;- rownames(exp_brca_final)<br>lasso_data &lt;- left_join(sur_brac, exp_brca_final, by = </span><span>"sample"</span><span>)<br>head(lasso_data)[</span><span>1</span><span>:</span><span>5</span><span>, </span><span>1</span><span>:</span><span>8</span><span>]<br></span><span>#              sample OS     _PATIENT OS.time    KLHL13     HSPB6      PDK4     MEOX1</span><span><br></span><span># 1: TCGA-C8-A275-01A  0 TCGA-C8-A275       1  6.714246  6.409391 10.108524  8.339850</span><span><br></span><span># 2: TCGA-BH-A1F8-11B  1 TCGA-BH-A1F8       1 11.071462 12.134105 13.323336 10.804131</span><span><br></span><span># 3: TCGA-BH-A1F8-01A  1 TCGA-BH-A1F8       1  7.483816  7.228819  9.620220  5.459432</span><span><br></span><span># 4: TCGA-AC-A7VC-01A  0 TCGA-AC-A7VC       1  7.098032  7.523562  9.552669  5.930737</span><span><br></span><span># 5: TCGA-AN-A0AM-01A  0 TCGA-AN-A0AM       5  7.330917  6.491853  7.209453  4.392317</span><span><br><br></span><span># 咱们整理一下下，样本名设为行名，剔除没用的列</span><span><br>rownames(lasso_data) &lt;- lasso_data$sample<br>lasso_data &lt;- lasso_data[ , -c(</span><span>1</span><span>, </span><span>3</span><span>)]<br>lasso_data &lt;- na.omit(lasso_data)<br>head(lasso_data)[</span><span>1</span><span>:</span><span>5</span><span>, </span><span>1</span><span>:</span><span>8</span><span>]<br></span><span>#                  OS OS.time    KLHL13     HSPB6      PDK4     MEOX1    SCN4A      E2F2</span><span><br></span><span># TCGA-C8-A275-01A  0       1  6.714246  6.409391 10.108524  8.339850 5.044394 10.727920</span><span><br></span><span># TCGA-BH-A1F8-11B  1       1 11.071462 12.134105 13.323336 10.804131 9.079485  7.011227</span><span><br></span><span># TCGA-BH-A1F8-01A  1       1  7.483816  7.228819  9.620220  5.459432 5.169925  8.791163</span><span><br></span><span># TCGA-AC-A7VC-01A  0       1  7.098032  7.523562  9.552669  5.930737 4.169925  9.202124</span><span><br></span><span># TCGA-AN-A0AM-01A  0       5  7.330917  6.491853  7.209453  4.392317 3.584963 10.099348</span><span><br><br>dim(lasso_data)<br></span><span># [1] 1194  444</span><span><br><br></span><span># 现在的数据有1194行（样本）、444列（基因），第一列为OS（样本结局，1为死亡，0为生存）</span><span><br></span><span># 第二列为OS.time（生存时间，单位为day），后面的442列为基因的表达量。</span><span><br><br></span><span># 现在正式开始lasso回归！</span><span><br><br></span><span># 把生存时间单位从day转换为year</span><span><br>lasso_data$OS.time &lt;- lasso_data$OS.time / </span><span>365</span><span><br>head(lasso_data)[</span><span>1</span><span>:</span><span>5</span><span>, </span><span>1</span><span>:</span><span>8</span><span>]<br></span><span>#                  OS     OS.time    KLHL13     HSPB6      PDK4     MEOX1    SCN4A      E2F2</span><span><br></span><span># TCGA-C8-A275-01A  0 0.002739726  6.714246  6.409391 10.108524  8.339850 5.044394 10.727920</span><span><br></span><span># TCGA-BH-A1F8-11B  1 0.002739726 11.071462 12.134105 13.323336 10.804131 9.079485  7.011227</span><span><br></span><span># TCGA-BH-A1F8-01A  1 0.002739726  7.483816  7.228819  9.620220  5.459432 5.169925  8.791163</span><span><br></span><span># TCGA-AC-A7VC-01A  0 0.002739726  7.098032  7.523562  9.552669  5.930737 4.169925  9.202124</span><span><br></span><span># TCGA-AN-A0AM-01A  0 0.013698630  7.330917  6.491853  7.209453  4.392317 3.584963 10.099348</span><span><br><br></span><span># 设置随机种子，方便大家重复！</span><span><br>set.seed(</span><span>1234</span><span>)<br><br></span><span># 设置自变量和因变量</span><span><br>x &lt;- as.matrix(lasso_data[ , c(</span><span>3</span><span>:ncol(lasso_data))])<br>y &lt;- as.matrix(Surv(lasso_data$OS.time, lasso_data$OS))<br><br></span><span># x为自变量，矩阵中是特征数据，用于构建lasso回归模型</span><span><br></span><span># y为因变量，矩阵中中是生存分析中的生存对象</span><span><br></span><span># 因为咱们要构建生存风险模型，如果只是普通的分类模型，y里面是样本的分类信息就好啦！</span><span><br></span><span># 假设我们以生存/死亡为分类依据（或者疾病/正常，都行！），那么y按照下面这样写就好啦！</span><span><br></span><span># y &lt;- as.matrix(lasso_data$OS)</span><span><br><br></span><span># 使用glmnet()建模，参数解释见下文</span><span><br>alpha1_fit &lt;- glmnet(x, y, alpha = </span><span>1</span><span>, family = </span><span>"cox"</span><span>, nlambda = </span><span>100</span><span>)</span><span><br></span></code></pre><p data-tool="mdnice编辑器">解释一下<code>glmnet</code>函数各参数的意义：</p><ol data-tool="mdnice编辑器"><li><section><code>x</code>: 是一个矩阵或数据框，包含自变量（特征）的观测值。模型将使用这些自变量来预测因变量 <code>y</code>。在Cox比例风险模型中，这些自变量通常是影响生存时间的因素，我们今天用的都是基因表达量。</section></li><li><section><code>y</code>: 这是因变量的向量或矩阵，它包含了我们想要预测或建模的目标变量的观测值。我们这里用的是<code>cox</code>，表示用于生存分析，因变量是生存时间（或事件发生时间）和事件状态（如生存与死亡）。</section></li><li><section><code>alpha</code>: 这是一个介于0和1之间的参数，用于控制Lasso回归和Ridge回归之间的权衡。<strong>当 <code>alpha = 1</code> 时，表示使用Lasso回归（L1正则化），当 <code>alpha = 0</code> 时，表示使用Ridge回归（L2正则化）。如果alpha在0~1之间，则为弹性网络。</strong>在这里，我们使用<code>alpha = 1</code>，因为我们希望使用Lasso正则化。</section></li><li><section><code>family</code>参数用于指定模型所假设的概率分布族，这将影响到模型的形式和假设。不同的问题类型和数据类型需要选择不同的分布族。下面咱们解释几种参数值：</section></li><ol><li><section><code>family = "cox"</code> 表示我们正在构建一个Cox比例风险模型，用于生存分析。Cox模型是用于研究事件发生时间与各种协变量之间的关系的模型。</section></li><li><section><code>"gaussian"</code>：用于连续的数值型因变量，假设因变量服从正态分布，这是线性回归的经典用途，也叫做普通最小二乘回归（Ordinary Least Squares，OLS）。</section></li><li><section><code>"binomial"</code>：用于二元分类问题，假设因变量是二元的，表示患病与正常或1与0的概率，这是逻辑回归（logistic distribution）的常见用途。</section></li><li><section><code>"poisson"</code>：用于计数数据，假设因变量是计数数据，且服从泊松分布，常用于描述事件在一段时间或空间内的发生次数。</section></li><li><section><code>"multinomial"</code>：用于多类别分类问题，假设因变量有多个类别，通常用于多类别逻辑回归或多类别分类问题。</section></li><li><section><code>"mgaussian"</code>：用于多元高斯分布，假设因变量是多维连续数值型数据，且服从多元高斯分布。这可以用于多元回归问题。</section></li></ol><li><section><code>nlambda</code>: 这个参数表示要拟合的lambda（正则化参数）的数量。Lambda是一个控制正则化强度的参数，它控制着Lasso回归中特征的稀疏性程度。<code>nlambda</code> 指定了在不同lambda值上进行模型拟合的次数，以便找到合适的正则化强度。一般我们使用默认值100就可以啦！</section></li></ol><pre data-tool="mdnice编辑器"><span></span><code><span># 绘图</span><br>plot(alpha1_fit, xvar = <span>"lambda"</span>, label = <span>TRUE</span>)<br></code></pre><p data-tool="mdnice编辑器"><img data-ratio="0.6518518518518519" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BBmhpvTRMkHWY49o5IprfCRQPhsqaQLTUoKAoJE9icj88by8DAibfhNOQ/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BBmhpvTRMkHWY49o5IprfCRQPhsqaQLTUoKAoJE9icj88by8DAibfhNOQ/640?wx_fmt=png">这是<strong>回归系数路径图</strong>。</p><p data-tool="mdnice编辑器">图中的每一条曲线上都有变量编号，即每一条曲线代表了每一个自变量系数的变化轨迹，纵坐标是系数的值，下横坐标是log λ，上横坐标是此时模型中非零系数的个数。</p><p data-tool="mdnice编辑器">我们可以看到，随着参数log λ增大，回归系数（即纵坐标值）不断收敛，最终收敛成0，最终系数被压缩为0的变量，说明它比较重要。例如，最上面那条代表的自变量135（绿色）在λ值很大时就有非零的系数，然后随着λ值变大不断变小。</p><p data-tool="mdnice编辑器">接下来，我们进行交叉验证（cross-validation），默认<code>nfolds = 10</code>，也就是十折交叉验证，大家可以按需修改！</p><pre data-tool="mdnice编辑器"><span></span><code><span># 交叉验证，参数解释见下文</span><br>set.seed(<span>1234</span>)<br>alpha1.fit.cv &lt;- cv.glmnet(x, y, type.measure = <span>"deviance"</span>, alpha = <span>1</span>, family = <span>"cox"</span>)<br></code></pre><p data-tool="mdnice编辑器"><code>type.measure</code> 参数用于指定在进行交叉验证时用于度量模型性能的指标。咱们的<code>type.measure</code> 被设置为 "<code>deviance</code>"，这是用于分类问题的一种常见指标，特别是对于逻辑回归等模型。它表示对数似然的负二倍，可用于评估分类模型的拟合。除了 "<code>deviance</code>"，<code>cv.glmnet</code> 函数还支持其他一些常见的度量指标，具体取决于问题类型。下面解释几种 <code>type.measure</code>的参数值：</p><ol data-tool="mdnice编辑器"><li><section>"mae"（Mean Absolute Error）: 这是用于回归问题的指标，衡量了模型预测值与真实值之间的绝对差异的平均值。</section></li><li><section>"auc"（Area Under the ROC Curve）: 这是用于二元分类问题的指标，度量了模型的分类性能，尤其是对正类别的分类准确性。</section></li><li><section>"class"：这是另一个用于分类问题的度量指标，通常与"deviance"或"auc"一起使用，以提供更多的分类性能信息。</section></li><li><section>"mse"（Mean Squared Error）: 这是用于回归问题的度量指标，衡量了模型预测值与真实值之间的平方差的平均值。</section></li></ol><pre data-tool="mdnice编辑器"><span></span><code><span># 绘图</span><br>plot(alpha1.fit.cv)<br></code></pre><p data-tool="mdnice编辑器"><img data-ratio="0.6546296296296297" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BRceNP36XgXKp8vgCKS5ArPtdLvc02QZwqKpfq7yIRx7oKfVNMwa1Cg/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpwWd8ugEkMcBcpVUVp220BRceNP36XgXKp8vgCKS5ArPtdLvc02QZwqKpfq7yIRx7oKfVNMwa1Cg/640?wx_fmt=png">介个是Lasso回归的<strong>交叉验证曲线</strong>。</p><p data-tool="mdnice编辑器">X轴是惩罚系数的对数 log λ，Y轴是似然偏差，Y轴越小说明方程的拟合效果越好。最上面的数字则为不同λ时，方程剩下的变量数。两条虚线代表两个特殊的lambda（λ）值。</p><p data-tool="mdnice编辑器">左边虚线为λ min，意思是偏差最小时的λ ，代表在该lambda取值下，模型拟合效果最高。变量数是29，相比λ-se，保留下来的变量更多。</p><p data-tool="mdnice编辑器">右边虚线为λ-se，意思是最小λ右侧的1个标准误。在该λ取值下，构建模型的拟合效果也很好，同时纳入方程的个数更少，模型更简单。因此，临床上一般会选择右侧的λ1-se作为最终方程筛选标准。我们今天选择的数据不太好哈，变量数为100以后拟合效果就都差不多啦！不过没事！咱主要是为了理解方法！不管结果好坏！</p><pre data-tool="mdnice编辑器"><span></span><code><span># 打印结果</span><br>print(alpha1.fit.cv)<br><span># Call:  cv.glmnet(x = x, y = y, type.measure = "deviance", alpha = 1,      family = "cox") </span><br><span># </span><br><span># Measure: Partial Likelihood Deviance </span><br><span># </span><br><span>#      Lambda Index Measure    SE Nonzero</span><br><span># min 0.01289    18   11.84 0.245      29</span><br><span># 1se 0.06267     1   12.03 0.238       0</span><br><br><span># 这里我们看最后一列，29是λ min时的变量数，那个0，理论上不能是0！</span><br><span># 应该是比29小的数字！这是个意外，要不咱们假装那里是6！继续往后讲！</span><br><br><span># 特征筛选，下面打印出来的不是我们的数据得到的结果，只是给大家展示一下</span><br><span># 第一行显示的442 x 1代表的就是442个变量，略多哈，放心！一般大家分析的时候应该会很少！</span><br>coef(alpha1.fit.cv, s = alpha1.fit.cv$lambda.1se)<br><span># 442 x 1 sparse Matrix of class "dgCMatrix"</span><br><span># s1</span><br><span># (Intercept) -13.320908508</span><br><span># CSMD3         .          </span><br><span># KCNIP3        0.302591830</span><br><span># GNAL          0.518595837</span><br><span># KCNB1         0.432713282</span><br><span># SPHKAP        .          </span><br><span># CRTAC1        .          </span><br><span># CDHR1         0.143984185</span><br><span># TNR           0.009118150</span><br><span># PTPRT         .          </span><br><span># GABRG1        0.062742617</span><br><span># ELFN2         .          </span><br><span># RTP5          .          </span><br><br><span># 点点就代表系数为0，系数不为0的就是最终纳入的变量</span><br><br><span># 提取特征</span><br>feature_all &lt;- as.data.frame(as.matrix(coef(alpha1.fit, s = alpha1.fit$lambda.1se)))<br>colnames(feature_all) &lt;- <span>"coff"</span><br>feature_opt &lt;-  feature_all %&gt;% filter(abs(coff) &gt; <span>0</span>)<br>rownames(feature_opt)<br><span># [1] "NKAIN1"   "IL4I1"    "LEPR"     "FOXJ1"    "LYVE1"    "CRHBP"    "S100B"   </span><br><span># [8] "FREM1"    "CST1"     "HTR1D"    "WT1"      "TNFRSF18"</span><br><br><span># 然后我们就成功筛选出候选变量啦！接下来就可以进行后续的分析啦！</span><br><span># 比如进一步筛选变量、构建预测模型等等。</span><br></code></pre><p data-tool="mdnice编辑器">所以通常文章中会放入这两张图，表示进行了Lasso回归。</p><p data-tool="mdnice编辑器">在这之后，我们就可以进行后续的Cox回归分析、列线图、生存分析、风险因子三图联动等等步骤啦！像下面这样！</p><p><img data-galleryid="" data-ratio="0.6648148148148149" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLob1A5HicAJNiaQxFeT4Dqvzic1K6GWTbXPGUXsUG9OSn6XtcAbTPJYyF5uU1IzxwgGz8FGRZMhbYlvw/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLob1A5HicAJNiaQxFeT4Dqvzic1K6GWTbXPGUXsUG9OSn6XtcAbTPJYyF5uU1IzxwgGz8FGRZMhbYlvw/640?wx_fmt=png"></p><section><span>图片来源：</span><span>Li Y, Mo H, Wu S, Liu X, Tu K. A Novel Lactate Metabolism-Related Gene Signature for Predicting Clinical Outcome and Tumor Microenvironment in Hepatocellular Carcinoma. Front Cell Dev Biol. 2022 Jan 3;9:801959.</span></section><blockquote data-tool="mdnice编辑器"><p><span>这</span><span>些我后面应该都会陆续更新分享，大家可以慢慢学习哟！</span><span>不过总会收到小伙伴询问接不接生信分析的单子，主要是考虑到近期有点忙，没精力接很多任务，所以这里先回复一下 —— 暂时不接。</span><span>我还是比较建议大家学习一下我的保姆级教程哈哈哈哈哈哈哈，感觉应该可以包会！</span><span>当然，也有一些小伙伴们可能希望有人指导或者协助完成生信分析工作，如果急需的话，也可以找我，我们可以协商一下是否合适！</span><span>最后，真心希望可以帮到大家！</span><span>    </span><span>                       </span><span>—— 一只小蛮要</span></p></blockquote><h2 data-tool="mdnice编辑器"><span></span><span>结语</span></h2><p data-tool="mdnice编辑器">好啦！以上就是Lasso回归的介绍啦！大家一定收获满满吧！</p><p data-tool="mdnice编辑器">其实我们在分析中可能会先进行单因素Cox回归，然后依据剩下的特征数去选择后续分析方法。如果单因素Cox回归后剩下的特征还很多，那可以用Lasso进行再次筛选，然后再进行多因素Cox回归分析，得到最终的特征用于构建风险模型。当然，如果剩下的特征非常少，一般就不进行Lasso啦！大家可以依据实际情况灵活调整！</p><p data-tool="mdnice编辑器">在之后的分享中，我们还会介绍单因素和多因素Cox回归分析哟！敬请期待！</p><p data-tool="mdnice编辑器"><strong>如果大家觉得我分享的内容对你有一丢丢帮助的话，可以关注点赞收藏分享阿巴阿巴来一波哟！</strong><br></p><section><mp-common-profile data-pluginname="mpprofile" data-id="Mzg5NjE1NTc1OA==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/QQjtfWKmicLpicdicvWNnpcaTbejCVgKe4FsicHqo48FOkqJTkziaengudibCF8AW86uyjgee0pxhmhM8JoylYDkSXug/0?wx_fmt=png" data-nickname="生信小白要知道" data-alias="Bioidiot" data-signature="主打小白保姆级教程，因为自己淋过雨，所以想给大家撑把伞！ 记录从小白到现在小灰的过程，希望以后可以成为小黑！" data-from="0" data-is_biz_ban="0"></mp-common-profile></section><h1 data-tool="mdnice编辑器"><span>参考资料</span></h1><ol data-tool="mdnice编辑器"><li><section>https://zhuanlan.zhihu.com/p/426162272</section></li><li><section>https://zhuanlan.zhihu.com/p/509021086</section></li><li><section>https://www.jianshu.com/p/617db057df37</section></li><li><section>https://www.cnblogs.com/zingp/p/10375691.html</section></li><li><section>https://www.bilibili.com/video/BV1kX4y1g7jp</section></li><li><section><a href="https://mp.weixin.qq.com/s?__biz=MzIxNjY0ODA4OQ==&amp;mid=2247497683&amp;idx=1&amp;sn=c9ffd712fd4f13af251f1107d86443df&amp;scene=21#wechat_redirect" data-linktype="2">https://mp.weixin.qq.com/s/aabknP4xq2j5JyKWCSEiUQ</a></section></li><li><section>https://www.hindawi.com/journals/jdr/2020/7261047/</section></li><li><section>https://onlinelibrary.wiley.com/doi/10.1002/cam4.2896</section></li><li><section><a href="https://mp.weixin.qq.com/s?__biz=MzUxNjgzNjU0Mw==&amp;mid=2247485117&amp;idx=1&amp;sn=e32088d7571b486f720ca2cb5c4a3367&amp;scene=21#wechat_redirect" data-linktype="2">https://mp.weixin.qq.com/s/EpVcu7x-EI2bzGjH5lG19Q</a></section></li><li><section><a href="https://mp.weixin.qq.com/s?__biz=Mzg2NjYzNjQ4Ng==&amp;mid=2247484429&amp;idx=1&amp;sn=14117457db375fad9b139ca67926768f&amp;scene=21#wechat_redirect" data-linktype="2">https://mp.weixin.qq.com/s/sCDEP0QmdYRp70LUMPcXIw</a></section></li><li><section><a href="https://mp.weixin.qq.com/s?__biz=MzA3OTE5MTc2Ng==&amp;mid=2247488513&amp;idx=1&amp;sn=36c81d7fecdd2504e230221c6574da85&amp;scene=21#wechat_redirect" data-linktype="2">https://mp.weixin.qq.com/s/5E1WXTawQPd4JsaShAWrxA</a></section></li></ol></section><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/kSrr6regfAtX4Bw6gSvmgw",target="_blank" rel="noopener noreferrer">原文链接</a>
