---
title: "R语言xgboost超参数调优"
date: 2024-01-04T03:42:05Z
draft: ["false"]
tags: [
  "fetched",
  "医学和生信笔记"
]
categories: ["Acdemic"]
---
R语言xgboost超参数调优 by 医学和生信笔记
------
<div><section><span>关注公众号，发送</span><strong>R语言</strong><span>或</span><strong>python</strong><span>，可获取资料</span><span></span></section><section><mp-common-profile data-pluginname="mpprofile" data-id="MzUzOTQzNzU0NA==" data-headimg="http://mmbiz.qpic.cn/mmbiz_png/tpAC6lR84R9YDc8IDhqWAHTrZsMuhDpFlw4scqOl1ZVWpeY77cdibaSzPeGALfkEhdVpwHzVibHCRSYZg4csB43g/0?wx_fmt=png" data-nickname="医学和生信笔记" data-alias="yxhsxbj" data-signature="外科医生👨‍⚕️的R语言和生信学习🔖" data-from="2" data-is_biz_ban="0" data-weui-theme="light"></mp-common-profile></section><section data-role="outer" label="edit by 135editor"><section data-role="paragraph"><section data-role="outer" label="edit by 135editor"><section data-role="paragraph"><section data-role="outer" label="edit by 135editor"><section data-role="paragraph"><section data-role="outer"><section data-role="outer" label="edit by 135editor"><section data-tools="135编辑器" data-id="28"><p data-brushtype="text" hm_fix="440:185"><span>💡专注R语言在🩺生物医学中的使用</span></p></section></section></section></section></section><hr><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com"><p data-tool="mdnice编辑器"><span><span>设为“</span><strong><span>星标</span></strong><span>”，精彩不错过</span></span><br></p><hr><p data-tool="mdnice编辑器"><span><strong></strong></span></p><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com"><p data-tool="mdnice编辑器"><code>xgboost</code>作为一种基于GBDT（Gradient Boosting Decision Tree，梯度提升树）发展而来的提升算法，其优秀之处不必多说，我们上次已经介绍了它在R语言中的简单使用，这次我们介绍下xgboost的各种参数的意义以及超参数调优。</p><p data-tool="mdnice编辑器">xgboost本身是基于梯度提升树（GBDT）实现的集成算法，所以它的参数整体来说可以分为三个部分：集成算法本身，用于集成的弱评估器（决策树），以及应用中的其他过程。</p><h2 data-tool="mdnice编辑器"><span></span><span>准备数据</span><span></span></h2><p data-tool="mdnice编辑器">使用皮玛印第安人糖尿病数据集。</p><pre data-tool="mdnice编辑器"><code>rm(list = ls())<br><span>library</span>(MASS)<br><span>library</span>(xgboost)<br><br>data(<span>"Pima.tr"</span>)<br>data(<span>"Pima.te"</span>)<br>pima &lt;- rbind(Pima.tr, Pima.te)<br><br>set.seed(<span>502</span>)<br>ind &lt;- sample(<span>2</span>, nrow(pima), replace = <span>T</span>, prob = c(<span>0.7</span>, <span>0.3</span>))<br>pima.train &lt;- pima[ind == <span>1</span>,]<br>pima.test &lt;- pima[ind == <span>2</span>,]<br><br>dim(pima.train)<br><span>## [1] 385   8</span><br>dim(pima.test)<br><span>## [1] 147   8</span><br></code></pre><p data-tool="mdnice编辑器">训练集有385行，8列，测试集有147行，8列，其中<code>type</code>列是结果变量，<code>Yes</code>表示有糖尿病，<code>No</code>表示没有糖尿病。</p><h2 data-tool="mdnice编辑器"><span></span><span>参数解释</span><span></span></h2><p data-tool="mdnice编辑器"><code>xgboost()</code>是<code>xgb.train()</code>的简单封装，<code>xgb.train()</code>是训练xgboost模型的高级接口。xgboost模型的参数非常多，我们介绍其中一部分，有些参数在上次的推文介绍过了。</p><ul data-tool="mdnice编辑器"><li><section>nrounds：最大迭代次数（最终模型中树的数量）。</section></li><li><section>early_stopping_rounds：一个正整数，表示在验证集中经过K次训练如果模型表现还是没有提高就停止训练。</section></li><li><section>print_every_n：如果verbose&gt;0，这个参数表示每多少次迭代打印一次日志信息。</section></li></ul><p data-tool="mdnice编辑器"><code>params</code>是<code>xgb.train()</code>中最重要的参数了，<code>params</code>接受一个列表，列表内包含超多参数，这些参数主要分为3大类，也是我们调参需要重点关注的参数：</p><ol data-tool="mdnice编辑器"><li><section>通用参数</section></li></ol><ul data-tool="mdnice编辑器"><li><section><code>booster</code>：提升器类型，<code>gbtree</code>(默认)或者<code>gblinear</code>。多数情况下都是<code>gbtree</code>的效果更好，但是如果你的预测变量和结果变量呈现明显的线性关系，可能<code>gblinear</code>更好，但也不是绝对的，开发者建议都试一下。</section></li></ul><ol start="2" data-tool="mdnice编辑器"><li><section>booster相关的参数2.1 tree booster相关的参数</section></li></ol><ul><li><section><code>eta</code>：学习率η，每棵树在最终解中的贡献，默认为0.3。</section></li><li><section><code>gamma</code>：在树中新增一个叶子分区时所需的最小减损。</section></li><li><section><code>max_depth</code>：单个树的最大深度。</section></li><li><section><code>min_child_weight</code>：对树进行提升时使用的最小权重，默认为1。</section></li><li><section><code>subsample</code>：子样本数据占整个观测的比例，默认值为1（100%）。</section></li><li><section><code>colsample_bytree</code>：建立树时随机抽取的特征数量，用一个比率表示，默认值为1（使用100%的特征)。</section></li><li><section><code>lambda</code>：L2正则化的比例，默认是1，也就是lasso。</section></li><li><section><code>alpha</code>：L1正则化的比例，默认是0。</section></li><li><section>...2.2 linear booster相关的参数</section></li><li><section>...</section></li></ul><li><section>任务相关的参数</section></li><ul data-tool="mdnice编辑器"><li><section><code>objective</code>：指定任务类型和目标函数，支持自定义函数，默认的有以下类型，主要是回归、分类、生存、排序等：</section></li><ul><li><section><code>reg:squarederror</code>：均方根误差（默认值）。</section></li><li><section><code>reg:squaredlogerror</code>：均方根对数误差。</section></li><li><section><code>reg:logistic</code>：logistic函数。</section></li><li><section><code>reg:pseudohubererror</code>：Pseudo Huber损失函数。</section></li><li><section><code>binary:logistic</code>：二分类逻辑回归，输出概率值。</section></li><li><section><code>binary:logitraw</code>：二分类逻辑回归，输出logistic转换之前的值。</section></li><li><section><code>binary:hinge</code>：二分类hinge loss,输出0或者1。</section></li><li><section><code>count:poisson</code>：计数数据的泊松回归</section></li><li><section><code>survival:cox</code>：右删失生存数据的cox回归，返回风险比HR。</section></li><li><section><code>survival:aft</code>：加速失效模型。</section></li><li><section>...</section></li></ul><li><section><code>base_score</code>：</section></li><li><section><code>eval_metric</code>：验证集的评价指标。</section></li></ul><p data-tool="mdnice编辑器">下面我们使用默认参数拟合模型，看看模型效果。顺便学习下如果准备这些参数。</p><p data-tool="mdnice编辑器">注意，所有的预测变量都需要是数值型（这和我们前面介绍过的xgboost输入数据的格式有关，矩阵需要都是数值型的），所以分类变量需要进行一些转换，比如哑变量、独热编码等。</p><pre data-tool="mdnice编辑器"><code><span># 选择参数的值</span><br>param &lt;- list(objective = <span>"binary:logistic"</span>,<br>              booster = <span>"gbtree"</span>,<br>              eval_metric = <span>"error"</span>,<br>              eta = <span>0.3</span>,<br>              max_depth = <span>3</span>,<br>              subsample = <span>1</span>,<br>              colsample_bytree = <span>1</span>,<br>              gamma = <span>0.5</span>)<br><br><span># 准备预测变量和结果变量</span><br>x &lt;- as.matrix(pima.train[, <span>1</span>:<span>7</span>])<br>y &lt;- ifelse(pima.train$type == <span>"Yes"</span>, <span>1</span>, <span>0</span>)<br><br><span># 放进专用的格式中</span><br>train.mat &lt;- xgb.DMatrix(data = x, label = y)<br>train.mat<br><span>## xgb.DMatrix  dim: 385 x 7  info: label  colnames: yes</span><br></code></pre><p data-tool="mdnice编辑器">这样参数和数据就都准备好了，下面开始训练即可。</p><pre data-tool="mdnice编辑器"><code>set.seed(<span>1</span>)<br>xgb.fit &lt;- xgb.train(params = param, <br>                     data = train.mat, <br>                     nrounds = <span>100</span>)<br></code></pre><p data-tool="mdnice编辑器">有了这个结果后你可以查看变量重要性，查看每棵树的信息，得出预测类别的概率，画出ROC曲线等，详情请参考上一篇，这里就不再重复演示了。</p><h2 data-tool="mdnice编辑器"><span></span><span>超参数调优</span><span></span></h2><p data-tool="mdnice编辑器">下面就是对这些参数进行调整，我们就使用<code>caret</code>进行演示。</p><p data-tool="mdnice编辑器"><code>caret</code>作为R语言中经典的机器学习综合性R包，使用起来非常简单，我们也写过非常详细的系列教程了，后台回复<strong>caret</strong>即可获取<code>caret</code>系列推文合集。</p><pre data-tool="mdnice编辑器"><code><span>library</span>(caret)<br><br><span># 选择参数范围</span><br>grid &lt;- expand.grid(nrounds = c(<span>75</span>, <span>100</span>),<br>                    colsample_bytree = <span>1</span>,<br>                    min_child_weight = <span>1</span>,<br>                    eta = c(<span>0.01</span>, <span>0.1</span>, <span>0.3</span>),<br>                    gamma = c(<span>0.5</span>, <span>0.25</span>),<br>                    subsample = <span>0.5</span>,<br>                    max_depth = c(<span>2</span>, <span>3</span>))<br><br><span># 一些控制参数</span><br>cntrl &lt;- trainControl(method = <span>"cv"</span>,<br>                      number = <span>5</span>,<br>                      verboseIter = <span>F</span>,<br>                      returnData = <span>F</span>,<br>                      returnResamp = <span>"final"</span>)<br><br><span># 开始调优</span><br>set.seed(<span>1</span>)<br>train.xgb &lt;- train(x = pima.train[, <span>1</span>:<span>7</span>],<br>                   y = pima.train[, <span>8</span>],<br>                   trControl = cntrl,<br>                   tuneGrid = grid,<br>                   method = <span>"xgbTree"</span>)<br><br>train.xgb<br><span>## eXtreme Gradient Boosting </span><br><span>## </span><br><span>## No pre-processing</span><br><span>## Resampling: Cross-Validated (5 fold) </span><br><span>## Summary of sample sizes: 308, 309, 308, 307, 308 </span><br><span>## Resampling results across tuning parameters:</span><br><span>## </span><br><span>##   eta   max_depth  gamma  nrounds  Accuracy   Kappa    </span><br><span>##   0.01  2          0.25    75      0.7865932  0.4700801</span><br><span>##   0.01  2          0.25   100      0.7971195  0.5003081</span><br><span>##   0.01  2          0.50    75      0.7971528  0.4945394</span><br><span>##   0.01  2          0.50   100      0.8101407  0.5317302</span><br><span>##   0.01  3          0.25    75      0.7971537  0.5018986</span><br><span>##   0.01  3          0.25   100      0.7893948  0.4854910</span><br><span>##   0.01  3          0.50    75      0.8050476  0.5221906</span><br><span>##   0.01  3          0.50   100      0.7997853  0.5136332</span><br><span>##   0.10  2          0.25    75      0.7896980  0.5002401</span><br><span>##   0.10  2          0.25   100      0.7921605  0.5073063</span><br><span>##   0.10  2          0.50    75      0.7947579  0.5167947</span><br><span>##   0.10  2          0.50   100      0.7791717  0.4828563</span><br><span>##   0.10  3          0.25    75      0.7922612  0.5044835</span><br><span>##   0.10  3          0.25   100      0.7896297  0.5011073</span><br><span>##   0.10  3          0.50    75      0.7845023  0.4923603</span><br><span>##   0.10  3          0.50   100      0.7766418  0.4779629</span><br><span>##   0.30  2          0.25    75      0.7557592  0.4362591</span><br><span>##   0.30  2          0.25   100      0.7609198  0.4504284</span><br><span>##   0.30  2          0.50    75      0.7635864  0.4475264</span><br><span>##   0.30  2          0.50   100      0.7740093  0.4729577</span><br><span>##   0.30  3          0.25    75      0.7636881  0.4478178</span><br><span>##   0.30  3          0.25   100      0.7637547  0.4534616</span><br><span>##   0.30  3          0.50    75      0.7583574  0.4403953</span><br><span>##   0.30  3          0.50   100      0.7427721  0.3975159</span><br><span>## </span><br><span>## Tuning parameter 'colsample_bytree' was held constant at a value of 1</span><br><span>## </span><br><span>## Tuning parameter 'min_child_weight' was held constant at a value of 1</span><br><span>## </span><br><span>## Tuning parameter 'subsample' was held constant at a value of 0.5</span><br><span>## Accuracy was used to select the optimal model using the largest value.</span><br><span>## The final values used for the model were nrounds = 100, max_depth = 2, eta</span><br><span>##  = 0.01, gamma = 0.5, colsample_bytree = 1, min_child_weight = 1 and</span><br><span>##  subsample = 0.5.</span><br></code></pre><p data-tool="mdnice编辑器">结果中给出了最优的超参数：nrounds = 100, max_depth = 2, eta = 0.01, gamma = 0.5, colsample_bytree = 1, min_child_weight = 1, subsample = 0.5。</p><p data-tool="mdnice编辑器">这个结果可以探索可视化的地方非常多，比如：</p><pre data-tool="mdnice编辑器"><code>plot(train.xgb)<br></code></pre><figure data-tool="mdnice编辑器"><img data-imgfileid="100016944" data-ratio="1" data-src="https://mmbiz.qpic.cn/mmbiz_png/tpAC6lR84RibibH1tMwPlfcRp4hJWMO117D5iaiaKwd3CvOvVBhRcraOviatvX6dAdQIbIxagYP2DIjhBRb0T7TTS8g/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="504" src="https://mmbiz.qpic.cn/mmbiz_png/tpAC6lR84RibibH1tMwPlfcRp4hJWMO117D5iaiaKwd3CvOvVBhRcraOviatvX6dAdQIbIxagYP2DIjhBRb0T7TTS8g/640?wx_fmt=png&amp;from=appmsg"></figure><p data-tool="mdnice编辑器">也是支持<code>ggplot2</code>的。</p><pre data-tool="mdnice编辑器"><code>ggplot(train.xgb)<br></code></pre><figure data-tool="mdnice编辑器"><img data-imgfileid="100016947" data-ratio="1" data-src="https://mmbiz.qpic.cn/mmbiz_png/tpAC6lR84RibibH1tMwPlfcRp4hJWMO117faAd70pGxicicXeVb4eZJA1U3IaCRfiaInrYNiaepMBY84rypibtrKo2gSw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="504" src="https://mmbiz.qpic.cn/mmbiz_png/tpAC6lR84RibibH1tMwPlfcRp4hJWMO117faAd70pGxicicXeVb4eZJA1U3IaCRfiaInrYNiaepMBY84rypibtrKo2gSw/640?wx_fmt=png&amp;from=appmsg"></figure><p data-tool="mdnice编辑器">更多方法大家可以探索我们的<code>caret</code>合集。</p><h2 data-tool="mdnice编辑器"><span></span><span>模型拟合</span><span></span></h2><p data-tool="mdnice编辑器">接下来就是使用最优的超参数重新拟合模型。</p><pre data-tool="mdnice编辑器"><code><span># 选择最优的参数值</span><br>param &lt;- list(objective = <span>"binary:logistic"</span>,<br>              booster = <span>"gbtree"</span>,<br>              eval_metric = <span>"error"</span>,<br>              eta = <span>0.01</span>,<br>              max_depth = <span>2</span>,<br>              subsample = <span>0.5</span>,<br>              colsample_bytree = <span>1</span>,<br>              gamma = <span>0.5</span>)<br><br><br><span># 拟合模型</span><br>set.seed(<span>1</span>)<br>xgb.fit &lt;- xgb.train(params = param, <br>                     data = train.mat, <br>                     nrounds = <span>100</span>)<br></code></pre><p data-tool="mdnice编辑器">画个ROC曲线，先计算一下训练集的预测概率，再画ROC曲线即可，没有任何难度：</p><pre data-tool="mdnice编辑器"><code>pred_train &lt;- predict(xgb.fit, newdata = train.mat)<br>head(pred_train)<br><span>## [1] 0.3161996 0.3288908 0.2387750 0.5719132 0.5012382 0.5890657</span><br><br><span>library</span>(ROCR)<br>pred &lt;- prediction(pred_train, pima.train$type)<br>perf &lt;- performance(pred, <span>"tpr"</span>, <span>"fpr"</span>)<br>auc &lt;- round(performance(pred, <span>"auc"</span>)@y.values[[<span>1</span>]],digits = <span>4</span>)<br><br>plot(perf, <br>     main = paste(<span>"ROC curve ("</span>, <span>"AUC = "</span>,auc,<span>")"</span>), <br>     col = <span>2</span>, <br>     lwd = <span>2</span>)<br>abline(<span>0</span>,<span>1</span>, lty = <span>2</span>, lwd = <span>2</span>)<br></code></pre><figure data-tool="mdnice编辑器"><img data-imgfileid="100016945" data-ratio="1" data-src="https://mmbiz.qpic.cn/mmbiz_png/tpAC6lR84RibibH1tMwPlfcRp4hJWMO1170NrrNhM95Xd8fPsqPk1BuQufUcIpY7icibgEDP5X8p9W493iaKfskHkeg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="504" src="https://mmbiz.qpic.cn/mmbiz_png/tpAC6lR84RibibH1tMwPlfcRp4hJWMO1170NrrNhM95Xd8fPsqPk1BuQufUcIpY7icibgEDP5X8p9W493iaKfskHkeg/640?wx_fmt=png&amp;from=appmsg"><figcaption>plot of chunk unnamed-chunk-8</figcaption></figure><p data-tool="mdnice编辑器">后台回复<strong>ROC</strong>即可获取ROC曲线合集，回复<strong>最佳截点</strong>即可获取ROC曲线的最佳截点合集。</p><p data-tool="mdnice编辑器">计算混淆矩阵等请参考上一篇关于xgboost的推文，无非就是把概率转换为硬类别而已。</p><h2 data-tool="mdnice编辑器"><span></span><span>测试集</span><span></span></h2><p data-tool="mdnice编辑器">首先需要把测试集的格式转换一下。</p><pre data-tool="mdnice编辑器"><code><span># 专用的格式中</span><br>test.mat &lt;- xgb.DMatrix(data = as.matrix(pima.test[, <span>1</span>:<span>7</span>]), <br>                         label = ifelse(pima.test$type == <span>"Yes"</span>, <span>1</span>, <span>0</span>))<br><br>pred_test &lt;- predict(xgb.fit, newdata = test.mat)<br>head(pred_test)<br><span>## [1] 0.2405169 0.6161979 0.6299443 0.2367187 0.5688603 0.5854614</span><br><br><span>library</span>(ROCR)<br>pred &lt;- prediction(pred_test, pima.test$type)<br>perf &lt;- performance(pred, <span>"tpr"</span>, <span>"fpr"</span>)<br>auc &lt;- round(performance(pred, <span>"auc"</span>)@y.values[[<span>1</span>]],digits = <span>4</span>)<br><br>plot(perf, <br>     main = paste(<span>"ROC curve ("</span>, <span>"AUC = "</span>,auc,<span>")"</span>), <br>     col = <span>2</span>, <br>     lwd = <span>2</span>)<br>abline(<span>0</span>,<span>1</span>, lty = <span>2</span>, lwd = <span>2</span>)<br></code></pre><figure data-tool="mdnice编辑器"><img data-imgfileid="100016946" data-ratio="1" data-src="https://mmbiz.qpic.cn/mmbiz_png/tpAC6lR84RibibH1tMwPlfcRp4hJWMO117Q10wxSaico99UibKaE8jY4ib4n8Meic45uycDkx8Gyk8QsxSku3zY7ibyIQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="504" src="https://mmbiz.qpic.cn/mmbiz_png/tpAC6lR84RibibH1tMwPlfcRp4hJWMO117Q10wxSaico99UibKaE8jY4ib4n8Meic45uycDkx8Gyk8QsxSku3zY7ibyIQ/640?wx_fmt=png&amp;from=appmsg"><figcaption>plot of chunk unnamed-chunk-9</figcaption></figure><p data-tool="mdnice编辑器">easy！</p><p data-tool="mdnice编辑器">有些指标是基于预测概率的，有些指标是基于预测列别的，xgboost只能给出预测概率，我们自己转换一下即可计算各种基于类别的指标了。</p><h2 data-tool="mdnice编辑器"><span></span><span>参考资料</span><span></span></h2><ol data-tool="mdnice编辑器"><li><section>帮助文档</section></li><li><section>https://blog.csdn.net/weixin_43217641/article/details/126599474</section></li><li><section>精通机器学习基于R</section></li></ol></section><p><span><br></span></p><hr><blockquote><p><span><strong>联系我们，关注我们</strong></span></p><ol><li><section>免费QQ交流群1：613637742（已满）</section></li><li><section>免费QQ交流群2：608720452</section></li><li><section>公众号消息界面关于作者获取联系方式</section></li><li><section>知乎、CSDN、简书同名账号</section></li><li><section>哔哩哔哩：阿越就是我</section></li></ol></blockquote></section></section></section></section></section><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/dBXgGGsEsGhKmfFIITlTSw",target="_blank" rel="noopener noreferrer">原文链接</a>
