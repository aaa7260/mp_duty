---
title: "空间转录组数据整合方法-3 STAligner模型的原理介绍"
date: 2024-05-06T02:08:16Z
draft: ["false"]
tags: [
  "fetched",
  "TOP生物信息"
]
categories: ["Acdemic"]
---
空间转录组数据整合方法-3 STAligner模型的原理介绍 by TOP生物信息
------
<div><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com"><section data-tool="mdnice编辑器" data-website="https://www.mdnice.com"><h2 data-tool="mdnice编辑器"><span>文章简介</span></h2><p data-tool="mdnice编辑器"><span>2</span><span>02</span><span>3年10月12日，中科院数学所的张世华研究员在期刊</span><strong><span>Nature Computational Science</span></strong><span>上发表了研究论文《Integrating spatial transcriptomics data across different conditions, technologies and developmental stages》，该工作提出了一个称为</span><strong><span>STAligner</span></strong><span>的图注意力神经网络模型，</span><span>用于</span><strong><span>整合不同条件（样本条件），技术（测序手段），发育阶段（针对发育数据集）的空间转录组数据集</span><span>。</span></strong><span></span></p><hr data-tool="mdnice编辑器"><h2 data-tool="mdnice编辑器"><span></span><span>任务定义-空间转录组数据整合</span></h2><p data-tool="mdnice编辑器"><span>空间转录组学数据整合是指将来自不同样本、使用不同技术平台获取的空间转录组数据集进行综合分析，</span><strong><span>消除批次效应</span><span>，</span><span>融合不同分辨率和不同实验手段和测序平台数据</span><span>的过程。</span></strong><span></span></p><p data-tool="mdnice编辑器"><span>具体来说，在深度学习层面，空间转录组学数据整合任务就是：</span><strong><span>深度</span><span>学习模型学习将不同批次来源的数据，并映射到一个共享的embedding空间中，同时消除不同数据集之间的批次差异，分辨率差异，生成保留生物学差异的embedding。</span></strong><span></span></p><p><img data-imgfileid="100007826" data-ratio="0.4650751547303271" data-src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2bLrkDkqsTSqLL02r3mibYrHYyFp0lh6nxr88o22JmKfFndpWW1Es9DEne74MncXwqJmyIngaf0FXg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1131" src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2bLrkDkqsTSqLL02r3mibYrHYyFp0lh6nxr88o22JmKfFndpWW1Es9DEne74MncXwqJmyIngaf0FXg/640?wx_fmt=png&amp;from=appmsg"><br></p><hr data-tool="mdnice编辑器"><h2 data-tool="mdnice编辑器"><span>STAligner模型介绍</span></h2><h3 data-tool="mdnice编辑器"><span>图注意力自编码器介绍</span></h3><p data-tool="mdnice编辑器"><strong><span>图注意力自编码器</span></strong><span>是一种无监督的表示学习框架，它同时使用图结构信息和</span><span>节点属性。它</span><span>采用图注意力机制来确定空间邻近节点对学习节点embedding的影响，并减少表达数据中噪声和缺失的影响，<strong>生成节点embedding。</strong></span></p><p data-tool="mdnice编辑器"><span></span><strong><span>图注意力自编码器将空间图作为输入</span></strong><span>，该图的</span><strong><span>节点代表空间转录组数据</span><span>中的s</span><span>pots，边表示spots之间的空间邻近性</span></strong><span>，作者使用KNN或者指定半径的方法来计算不同spots之间的空间邻近性。此外，<strong>空间图中的每个spots都有相关的feature，通常是基因表达数据</strong>，这些数据被用来学习节点的embedding。</span></p><p data-tool="mdnice编辑器"><span></span><span>图注意力自编码器使用注意力机制来确定图中每个节点的邻居节点对其embedding的影响。这种机制允许模型动态地为每个邻居分配不同的重要性权重（即下述公式）。该公式中，</span><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -1060.7 1608.5 1498.8" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="3B1" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g><g data-mml-node="TeXAtom" transform="translate(640, 530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="6B" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(910, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(640, -293.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="6A" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></g></svg><span>即节点i和节点j在第k层神经网络中的注意力权重，通过sigmoid函数计算，</span><svg xmlns="http://www.w3.org/2000/svg" role="img" focusable="false" viewbox="0 -1060.7 1479.5 1362.3" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="7A" d="M48 262Q48 264 54 349T60 436V444H252Q289 444 336 444T394 445Q441 445 450 441T459 418Q459 406 458 404Q456 399 327 229T194 55H237Q260 56 268 56T297 58T325 65T348 77T370 98T384 128T395 170Q400 197 400 216Q400 217 431 217H462V211Q461 208 453 108T444 6V0H245Q46 0 43 2Q32 7 32 28V33Q32 41 40 52T84 112Q129 170 164 217L298 393H256Q189 392 165 380Q124 360 115 303Q110 280 110 256Q110 254 79 254H48V262Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(511, 530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="6B" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(910, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(511, -293.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg><strong><span>即第k层神经网络输出的节点embedding。</span></strong><span></span></p><p><img data-galleryid="" data-imgfileid="100007830" data-ratio="0.25196850393700787" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xseKEib3jZvhkribHLVZso938RIF86mricY9d4K3kDb3ibl4iaU8NOWUfnzscQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="762" src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xseKEib3jZvhkribHLVZso938RIF86mricY9d4K3kDb3ibl4iaU8NOWUfnzscQ/640?wx_fmt=png&amp;from=appmsg"></p><p data-tool="mdnice编辑器"><span>接下来，作者通过解码器，将编码器的输出重</span><span>构回原始的基因表达空间，通过最小化重构损失（即下面的损失函数，编码器输出和解码器重构之间的差异），来保证模型可以学习到数据的有</span><span>效embedding。</span><span></span></p><p><img data-galleryid="" data-imgfileid="100007831" data-ratio="0.24867724867724866" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xseibqUvzfkYUPLJaw4Sia0d3yPbf1OjlpUFxCG9wFna6WuTWsLdOkJaG2A/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="378" src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xseibqUvzfkYUPLJaw4Sia0d3yPbf1OjlpUFxCG9wFna6WuTWsLdOkJaG2A/640?wx_fmt=png&amp;from=appmsg"><strong></strong></p><p><span></span></p><h3 data-tool="mdnice编辑器"><span></span><span>三元组损失函数介绍</span></h3><p data-tool="mdnice编辑器"><span>基</span><span>但是，单纯的图注意力自编码器只能学习单张切片上所有spots的有效embedding，<strong>无法去除不同切片之间存在的批次效应</strong>。作者在这里使用三元组损失函数来去除不同切片之间的批次效应，<strong>生成去除批次效应的节点embedding。 </strong></span></p><p data-tool="mdnice编辑器"><span>作者</span><span>首先定义了<strong>一组锚点样本，正样本和负样本</strong>。三元损失函数的核心思想是<strong>保持锚点样本和正样本之间的距离较小，同时增大锚点样本和负样本之间的距离。</strong></span></p><p data-tool="mdnice编辑器"><img data-imgfileid="100007832" data-ratio="0.5691906005221932" data-src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xseQhcaUwxeAnfLPicEFreqHTibIQaJxU8CP7ibP05ST4ubydjSnRunx6wicA/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="766" src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xseQhcaUwxeAnfLPicEFreqHTibIQaJxU8CP7ibP05ST4ubydjSnRunx6wicA/640?wx_fmt=png&amp;from=appmsg"></p><p data-tool="mdnice编辑器"><span>具体来说，在embedding空间，作者使用MNN 方法（相互最近邻）来定义具有相似基因表达但来自两个不同切片的锚点样本和正样本。</span><span>通过从锚点样本所在的切片进行随机抽样来获得负样本。</span><span>作者利用三元组损失函数（下述公式），<strong>在embedding空间中，最小化锚点样本-正样本之间的距离，最大化锚点样本-负样本之间的距离，来去除不同样本之间的批次效应。</strong></span></p><p><img data-galleryid="" data-imgfileid="100007833" data-ratio="0.11421628189550426" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xseqlgKLriadMjY1VgPDCODgGtic70YiaBOPs0ibhf621junKeG9TzoZibsjjg/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="823" src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xseqlgKLriadMjY1VgPDCODgGtic70YiaBOPs0ibhf621junKeG9TzoZibsjjg/640?wx_fmt=png&amp;from=appmsg"><span></span></p><p data-tool="mdnice编辑器"><span>最终作者提出了下述损失函数，α是一个控制重构损失和批次对齐损失（三元组损失函数）权重的超参数，默认大小是0.5。</span></p><p><img data-galleryid="" data-imgfileid="100007834" data-ratio="0.19333333333333333" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xsezqHic1ynJQX546qZalf0DFx6kpia7drsuYlCuqJ4Auj5febcfoqrmC3A/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="300" src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xsezqHic1ynJQX546qZalf0DFx6kpia7drsuYlCuqJ4Auj5febcfoqrmC3A/640?wx_fmt=png&amp;from=appmsg"><span></span></p><hr data-tool="mdnice编辑器"><h2 data-tool="mdnice编辑器"><span>STAligner部分实验结果介绍</span></h2><p data-tool="mdnice编辑器"><span>作者应用 STAligner 算法，来分析s通过 Slide-seq和 Stereo-seq平台进行测序的两个小鼠嗅球的空间转录组学数据。</span><span>小</span><span>鼠嗅球因为具有清晰的组织结构和已知的细胞类型，所以非常适合作为验证数据集。</span></p><p data-tool="mdnice编辑器"><span>此外Slide-seq的spots大小为10微米，而stereo-seq的spots大小为0.5微米，分辨率的差异和测序手段的不同使得这两张空间转录组切片之间存在显著的批次效应，非常适合检验STAligner算法去除批次效应的能力。</span></p><p data-tool="mdnice编辑器"><span><img data-imgfileid="100007835" data-ratio="0.5082335329341318" data-src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xsebBo5y87sp3dmCUGHE6VlkPR3k3nrFpVoewXiboYaUQkibTJibBc9RMpNw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1336" src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xsebBo5y87sp3dmCUGHE6VlkPR3k3nrFpVoewXiboYaUQkibTJibBc9RMpNw/640?wx_fmt=png&amp;from=appmsg"></span></p><p data-tool="mdnice编辑器"><span>整合结果显示，相较于传统用于整合scRNA-seq数据的Harmony算法，之前发表的空间转录组学整合算法SEDR，STAligner有效去除了两种测序技术分辨率不同的差异，更好的识别了组织中的空间结构。</span><span>例如在Slide-seqV2数据中识别出了附属嗅球（accessory olfactory bulb, AOB）和附属嗅球颗粒层（granular layer of the accessory olfactory bulb, AOBgr），而这些结构在stereo-seq切片中是不存在的。</span></p><p data-tool="mdnice编辑器"><span><img data-imgfileid="100007836" data-ratio="0.5210038188761593" data-src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xseoJFIfia0pfQ6ia5icEtNjAjMH3Bjo4AhJyFryLGFOxtiaCVJGugibqUT0vQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1833" src="https://mmbiz.qpic.cn/mmbiz_png/WThoCmvVu2YQYaqpm68F0EibUZuFc9xseoJFIfia0pfQ6ia5icEtNjAjMH3Bjo4AhJyFryLGFOxtiaCVJGugibqUT0vQ/640?wx_fmt=png&amp;from=appmsg"></span></p></section><hr data-tool="mdnice编辑器"><h2 data-tool="mdnice编辑器"><span>总结</span></h2><p data-tool="mdnice编辑器"><span>总之，STAligner模型使用了图注意力自编码器对空间转录组学数据进行建模，使用三元组损失函数来消除不同空间转录组学之间的批次效应。这种对空间转录组学数据进行建模的思想非常值得我们学习。</span><strong><span></span></strong></p><p data-tool="mdnice编辑器"><strong><span>目前STAligner已经被整合进python包Omicverse的工作流，大家可以在以下链接找到相关教程和代码</span></strong>https://omicverse.readthedocs.io/en/latest/Tutorials-space/t_staligner/</p><p data-tool="mdnice编辑器"><strong><span>在此感谢董弘禹师兄对本文的修改建议和指正！</span></strong><span></span></p><h3 data-tool="mdnice编辑器"><hr data-tool="mdnice编辑器"></h3><h2 data-tool="mdnice编辑器" data-remoteid="c1711332481222"><span>参考资料</span></h2><p><span>Zhou X, Dong K, Zhang S. Integrating spatial transcriptomics data across different conditions, technologies and developmental stages[J]. Nature Computational Science, 2023, 3(10): 894-906.</span><span></span></p></section><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/P7WBKqLgXCCu96o_hia_OQ",target="_blank" rel="noopener noreferrer">原文链接</a>
