---
title: "特征筛选还在用XGB的Feature Importance？试试Permutation Importance"
date: 2022-10-02T13:38:50Z
draft: ["false"]
tags: [
  "fetched",
  "PaperWeekly"
]
categories: ["Acdemic"]
---
特征筛选还在用XGB的Feature Importance？试试Permutation Importance by PaperWeekly
------
<div><p data-mpa-powered-by="yiban.io"><img data-backh="100" data-backw="578" data-galleryid="" data-ratio="0.1732283464566929" data-src="https://mmbiz.qpic.cn/mmbiz_gif/Psho9dm7oDHKVtfYDubjKdZRUjAfBQQicXjoZWJ3qnK42ooD4eeJUfJBM4SSZVa2RE5lO0j6rWwzliby0j9u4bDg/640?wx_fmt=gif" data-type="gif" data-w="635" src="https://mmbiz.qpic.cn/mmbiz_gif/Psho9dm7oDHKVtfYDubjKdZRUjAfBQQicXjoZWJ3qnK42ooD4eeJUfJBM4SSZVa2RE5lO0j6rWwzliby0j9u4bDg/640?wx_fmt=gif"></p><p><br></p><p><strong><span>©作者 |</span></strong><span> 刘秋言</span></p><p><strong><span>研究方向 |</span></strong><span> 机器学习</span></p><p><span><br></span></p><p><span><br></span></p><section><span>特征筛选是建模过程中的重要一环。</span></section><section><span><br></span></section><section><span>基于决策树的算法，如 Random Forest，Lightgbm, Xgboost，都能返回模型默认的 Feature Importance，但诸多研究都表明该重要性是存在偏差的。</span></section><section><span><br></span></section><section><span>是否有更好的方法来筛选特征呢？Kaggle 上很多大师级的选手通常采用的一个方法是 Permutation Importance。这个想法最早是由 Breiman（2001）</span><span>[1]</span><span> 提出，后来由 Fisher，Rudin，and Dominici（2018）改进 </span><span>[2]</span><span>。</span></section><section><span><br></span></section><section><span>通过本文，你将通过一个 Kaggle Amex</span> <span>真实数据了解到，模型默认的 Feature Importance 存在什么问题，什么是 Permutation Importance，它的优劣势分别是什么，以及具体代码如何实现和使用。</span></section><section><span><br></span></section><section><span><strong><span>本文完整代码：</span></strong></span></section><section><span><em><span>https://github.com/Qiuyan918/Permutation_Importance_Experiment</span></em></span></section><p><span><br></span></p><section><span>另外，本文代码使用 GPU 来完成 dataframe 的处理，以及 XGB 模型的训练和预测。相较于 CPU 的版本，可以提升 10 到 100 倍的速度。具体来说，使用 RAPIDS 的 CUDF 来处理数据。训练模型使用 XGB 的 GPU 版本，同时使用 DeviceQuantileDMatrix 作为 dataloader 来减少内存的占用。做预测时，则使用 RAPIDS 的 INF。</span></section><section><span><br></span></section><h2 data-into-catalog-status=""><span>本文目录</span></h2><section><span><br></span></section><ul><li><p><span>模型默认的 Feature Importance 存在什么问题？</span></p></li><li><p><span>什么是 Permutation Importance？</span></p></li><li><p><span>它的优劣势是什么？</span></p></li><li><p><span>Amex 数据实例验证</span></p></li></ul><p><br></p><section><br></section><p><img data-backh="84" data-backw="578" data-galleryid="" data-ratio="0.14577777777777778" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wulOVRfC18yCkd6xXqGq22h6QUk8chptF0fnQ4uXeZtAktYMrWwG2SyQ/640?wx_fmt=png" data-type="png" data-w="4500" src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wulOVRfC18yCkd6xXqGq22h6QUk8chptF0fnQ4uXeZtAktYMrWwG2SyQ/640?wx_fmt=png"></p><h2 data-into-catalog-status=""><span><br></span></h2><h2 data-into-catalog-status=""><span><strong><span>模型默认的Feature Importance存在什么问题？</span></strong></span></h2><p><span><br></span></p><section><span>Strobl et al </span><span>[3]</span> <span>在 2007 年就提出模型默认的 Feature Importance 会偏好连续型变量或高基数（high cardinality）的类型型变量。这也很好理解，因为连续型变量或高基数的类型变量在树节点上更容易找到一个切分点，换言之更容易过拟合。</span></section><section><span><br></span></section><section><span>另外一个问题是，Feature Importance 的本质是训练好的模型对变量的依赖程度，它不代表变量在 unseen data（比如测试集）上的泛化能力。特别当训练集和测试集的分布发生偏移时，模型默认的 Feature Importance 的偏差会更严重。</span></section><section><span><br></span></section><section><span>举一个极端的例子，如果我们随机生成一些 X 和二分类标签 y，并用 XGB 不断迭代。随着迭代次数的增加，训练集的 AUC 将接近 1，但是验证集上的 AUC 仍然会在 0.5 附近徘徊。这时模型默认的 Feature Importance 仍然会有一些变量的重要性特别高。这些变量帮助模型过拟合，从而在训练集上实现了接近 1 的 AUC。但实际上这些变量还是无意义的。</span></section><section><span><br></span></section><p><br></p><p><img data-backh="84" data-backw="578" data-galleryid="" data-ratio="0.14555555555555555" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wuhfgUpIfdPSqH8YjjHbCUiaaKsMA36bIMsMtGNKoBcus5py06M0fvx3A/640?wx_fmt=png" data-type="png" data-w="4500" src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wuhfgUpIfdPSqH8YjjHbCUiaaKsMA36bIMsMtGNKoBcus5py06M0fvx3A/640?wx_fmt=png"></p><h2 data-into-catalog-status=""><span><br></span></h2><h2 data-into-catalog-status=""><span><strong><span>什么是Permutation Importance？</span></strong></span></h2><p><span><br></span></p><section><span>Permutation Importance 是一种变量筛选的方法。它有效地解决了上述提到的两个问题。</span></section><section><span><br></span></section><section><span>Permutation Importance 将变量随机打乱来破坏变量和 y 原有的关系。如果打乱一个变量显著增加了模型在验证集上的loss，说明该变量很重要。如果打乱一个变量对模型在验证集上的 loss 没有影响，甚至还降低了 loss，那么说明该变量对模型不重要，甚至是有害的。</span></section><section><span><br></span></section><figure data-size="normal"><p><img data-backh="285" data-backw="410" data-ratio="0.6951219512195121" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJ2qxynj658m0BibuXb67UKhYo73vcjrLiaq4bGnuX5s7iaibqAjYRkMfw4g/640?wx_fmt=png" data-type="png" data-w="410" src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJ2qxynj658m0BibuXb67UKhYo73vcjrLiaq4bGnuX5s7iaibqAjYRkMfw4g/640?wx_fmt=png"></p><section><span>▲ 打乱变量示例</span></section><section><span><br></span></section></figure><section><span>变量重要性的具体计算步骤如下：</span></section><section><span><br></span></section><ul><li><section><span>1. 将数据分为 train 和 validation 两个数据集</span></section></li><li><section><span>2. 在 train 上训练模型，在 validation 上做预测，并评价模型（如计算 AUC）</span></section></li><li><section><span>3. 循环计算每个变量的重要性：</span></section></li><li><section><span>（3.1） 在 validation 上对单个变量随机打乱；</span></section></li><li><section><span>（3.2）使用第 2 步训练好的模型，重新在 validation 做预测，并评价模型；</span></section></li><li><p><span>（3.3）计算第 2 步和第 3.2 步 validation 上模型评价的差异，得到该变量的重要性指标</span></p></li></ul><p><br></p><section><span>Python 代码步骤（model 表示已经训练好的模型）：</span></section><section><span><br></span></section><section><pre><code><span><span>def</span> <span>permutation_importances</span><span>(model, X, y, metric)</span>:</span><br>    baseline = metric(model, X, y)<br>    imp = []<br>    <span>for</span> col <span>in</span> X.columns:<br>        save = X[col].copy()<br>        X[col] = np.random.permutation(X[col])<br>        m = metric(model, X, y)<br>        X[col] = save<br>        imp.append(baseline - m)<br>    <span>return</span> np.array(imp)<br></code></pre></section><section><br></section><p><br></p><p><img data-backh="84" data-backw="578" data-galleryid="" data-ratio="0.14555555555555555" data-s="300,640" data-type="png" data-w="4500" data-src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wukOjHSmSsEuRCB0fJu69CtdNgLnvFPDUCgeicOppBKuDvniaD3q8XWQ0Q/640?wx_fmt=png" src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wukOjHSmSsEuRCB0fJu69CtdNgLnvFPDUCgeicOppBKuDvniaD3q8XWQ0Q/640?wx_fmt=png"></p><h2 data-into-catalog-status=""><span><br></span></h2><h2 data-into-catalog-status=""><span><strong><span>Permutation Importance的优劣势是什么？</span></strong></span></h2><section><span><strong><span><br></span></strong></span></section><h3 data-into-catalog-status=""><span><strong><span>3.1 优势</span></strong></span></h3><section><br></section><ul><li><section><span>可以在任何模型上使用。不只是在基于决策树的模型，在线性回归，神经网络，任何模型上都可以使用。</span></section></li><li><section><span>不存在对连续型变量或高基数类别型变量的偏好。</span></section></li><li><section><span>体现了变量的泛化能力，当数据发生偏移时会特别有价值。</span></section></li><li><p><span>相较于循环的增加或剔除变量，不需要对模型重新训练，极大地降低了成本。但是循环地对模型做预测仍然会花费不少时间。</span></p></li></ul><h3 data-into-catalog-status=""><span><br></span></h3><h3 data-into-catalog-status=""><span><strong><span>3.2 劣势</span></strong></span></h3><section><span><strong><span><br></span></strong></span></section><ul><li><section><span>对变量的打乱存在随机性。这就要求随机打乱需要重复多次，以保证统计的显著性。</span></section></li><li><p><span>对相关性高的变量会低估重要性，模型默认的 Feature Importance 同样存在该问题。</span></p></li></ul><h2 data-into-catalog-status=""><br></h2><p><br></p><p><img data-backh="84" data-backw="578" data-galleryid="" data-ratio="0.14577777777777778" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wuiaLfO9V4lkD8cXK7ImEicqib5bPGH6syOrWzicR2KaqPyAicMccs8icC03Gw/640?wx_fmt=png" data-type="png" data-w="4500" src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDGhKg9nnSz5qQrwKvXibt3wuiaLfO9V4lkD8cXK7ImEicqib5bPGH6syOrWzicR2KaqPyAicMccs8icC03Gw/640?wx_fmt=png"></p><h2 data-into-catalog-status=""><span><br></span></h2><h2 data-into-catalog-status=""><span><strong><span>Amex数据实例验证</span></strong></span></h2><p><span><br></span></p><figure data-size="normal"><p><img data-backh="109" data-backw="578" data-ratio="0.1892116182572614" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJoUQyicaByib16zJgk3DbeiaaVlmnHdIssj5aOFovE8t4jTLgvTaIibMkkw/640?wx_fmt=png" data-type="png" data-w="1205" src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJoUQyicaByib16zJgk3DbeiaaVlmnHdIssj5aOFovE8t4jTLgvTaIibMkkw/640?wx_fmt=png"></p><section><span>▲ Kaggle Amex逾期预测比赛</span></section><section><span><br></span></section></figure><section><span>理论听起来可能有点头痛，我们直接以 Kaggle 的 Amex 数据作为实例，验证下 Permutation Importance 的效果。</span></section><section><span><br></span></section><section><span>考虑到 Permutation Importance 的随机性，我们将数据划分为 10 个 fold，并且每个变量随机打乱 10 次，所以每个变量总共打乱 100 次，再计算打乱后模型评价差异的平均值，以保证统计上的显著性。之后我们可以再谈谈如何更好地随机打乱以节省资源。</span></section><section><span><br></span></section><section><span>这里我们分别对比3种变量重要性的排序：</span></section><section><span><br></span></section><ol><li><section><span>模型默认的 Feature Importance</span></section></li><li><section><span>Permutation Importance</span></section></li><li><p><span>标准化后的 Permutation Importance：Permutation Importance / 随机 100 次的标准差。这考虑到了随机性。如果在 Permutation Importance 差不多的情况下，标准差更小，说明该变量的重要性结果是更稳定的。</span></p></li></ol><section><br></section><section><span>为了简化实验，这里随机筛选了总共 300 个变量。使用 300 个变量时，模型 10 fold AUC 为 0.9597。</span></section><section><span><br></span></section><section><span>下表是不同变化重要性排序下，模型 AUC 随着变量个数增加的变化情况：</span></section><section><span><br></span></section><figure data-size="normal"><p><img data-backh="320" data-backw="519" data-ratio="0.6165703275529865" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJLHKuposjgkICuFx9kspcxuf0ZjylNibjLCichO4HZYiamQW4qCeKq0uWw/640?wx_fmt=png" data-type="png" data-w="519" src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJLHKuposjgkICuFx9kspcxuf0ZjylNibjLCichO4HZYiamQW4qCeKq0uWw/640?wx_fmt=png"></p><section><span>▲ 不同变量重要性排序下，模型效果的变化情况</span></section><section><span><br></span></section></figure><section><span>由此我们可以得到如下结论：</span></section><section><span><br></span></section><ol><li><section><span>Permutation Importance 相较于模型默认的 Feature Importance 具有更好的排序性。当变量个数小于 250 个时，使用 Permutation Importance 排序的变量模型效果都更好。</span></section></li><li><section><span>随着变量个数的增加，模型默认的 Feature Importance 和 Permutation Importance 两种排序的模型 AUC 差异逐渐减小。这也间接说明 Permutation Importance 的重要性排序更好。因为在变量个数少的时候，两种排序筛选的变量差异会更大。随着变量的增加，两种排序下变量的重合逐渐增加，差异逐渐减小。</span></section></li><li><p><span>标准化后的 Permutation Importance 效果仅略微好于 Permutation Importance，这在真实业务场景意义较低，在建模比赛中有一定价值。</span></p></li></ol><h2><br></h2><h2><br></h2><p><br></p><section><section powered-by="xiumi.us"><section><section powered-by="xiumi.us"><section><section><svg viewbox="0 0 1 1"></svg></section><section><section opera-tn-ra-cell="_$.pages:0.layers:0.comps:0.col1:0.col1"><section powered-by="xiumi.us"><section><section powered-by="xiumi.us"><section><section powered-by="xiumi.us"><section><img data-ratio="0.5514706" data-type="svg" data-w="272" data-src="https://mmbiz.qpic.cn/mmbiz_svg/lpHDr05YrIRWFnyDtmhYYO1U5LiaVVY7Y8c8f0tL24zFvQIWjSicsTLcKFEgzogDib9lwzoqCxB4O6iaYj5iaFudxRTyvZeRBj4LZ/640?wx_fmt=svg" src="https://mmbiz.qpic.cn/mmbiz_svg/lpHDr05YrIRWFnyDtmhYYO1U5LiaVVY7Y8c8f0tL24zFvQIWjSicsTLcKFEgzogDib9lwzoqCxB4O6iaYj5iaFudxRTyvZeRBj4LZ/640?wx_fmt=svg"></section></section></section></section></section><section><section powered-by="xiumi.us"><p><strong>参考文献</strong></p></section></section><section><section powered-by="xiumi.us"><section><section powered-by="xiumi.us"><section><img data-ratio="0.5514706" data-type="svg" data-w="272" data-src="https://mmbiz.qpic.cn/mmbiz_svg/lpHDr05YrIRWFnyDtmhYYO1U5LiaVVY7Y8c8f0tL24zFvQIWjSicsTLcKFEgzogDib9lwzoqCxB4O6iaYj5iaFudxRTyvZeRBj4LZ/640?wx_fmt=svg" src="https://mmbiz.qpic.cn/mmbiz_svg/lpHDr05YrIRWFnyDtmhYYO1U5LiaVVY7Y8c8f0tL24zFvQIWjSicsTLcKFEgzogDib9lwzoqCxB4O6iaYj5iaFudxRTyvZeRBj4LZ/640?wx_fmt=svg"></section></section></section></section></section></section></section></section><section><svg viewbox="0 0 1 1"></svg></section></section></section></section></section></section><p><span>[1] Breiman, Leo.“Random Forests.” Machine Learning 45 (1). Springer: 5-32 (2001).</span></p><p><span>[2] Fisher, Aaron, Cynthia Rudin, and Francesca Dominici. “All models are wrong, but many are useful: Learning a variable’s importance by studying an entire class of prediction models simultaneously.” http://arxiv.org/abs/1801.01489 (2018).</span></p><p><span>[3] Strobl C, et al. Bias in random forest variable importance measures: Illustrations, sources and a solution, BMC Bioinformatics, 2007, vol. 8 pg. 25</span></p><p><br></p><p><br></p><section><section powered-by="xiumi.us"><section><section><svg viewbox="0 0 1 1"></svg></section><section><p><strong>更多阅读</strong></p></section><section><svg viewbox="0 0 1 1"></svg></section></section></section></section><p><a target="_blank" href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247593088&amp;idx=1&amp;sn=482ec0afda1fa16c516e62db5db91b16&amp;chksm=96ebb300a19c3a1694ee9d64c4db671bc4a335c27cebf000e880748f414a5999658b90f9112b&amp;scene=21#wechat_redirect" textvalue="你已选中了添加链接的内容" linktype="text" imgurl="" imgdata="null" tab="innerlink" data-linktype="1"><span data-positionback="static"><img data-backh="126" data-backw="578" data-ratio="0.21851851851851853" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJ7fXicoibia9RKF1P1V6HzfMZd419ZKTerkMjPiaIX2QicPPAiaIYCrXQUialQ/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJ7fXicoibia9RKF1P1V6HzfMZd419ZKTerkMjPiaIX2QicPPAiaIYCrXQUialQ/640?wx_fmt=png"></span></a></p><section><a target="_blank" href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247593289&amp;idx=2&amp;sn=6f1cc80c1201f15361ea2769d3f95b6a&amp;chksm=96ebb2c9a19c3bdf936f3e2f40233e05b1b5b4d449cadcdf2e9a29f6f907da885091d854e8c0&amp;scene=21#wechat_redirect" textvalue="你已选中了添加链接的内容" linktype="text" imgurl="" imgdata="null" tab="innerlink" data-linktype="1"><span data-positionback="static"><img data-backh="126" data-backw="578" data-ratio="0.21851851851851853" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJhlgQcDHkYChlicYFgxmQqVVvNGYW3fibRMmv4WDNbArAXuxNTUbChtZg/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJhlgQcDHkYChlicYFgxmQqVVvNGYW3fibRMmv4WDNbArAXuxNTUbChtZg/640?wx_fmt=png"></span></a></section><p><a target="_blank" href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247592483&amp;idx=3&amp;sn=3a8585b7e259410d2ff2744f6576741d&amp;chksm=96ebb5a3a19c3cb57d5cbae16101be7c75fccbd5af255a9bf325c0189c23da920eb56141d459&amp;scene=21#wechat_redirect" textvalue="你已选中了添加链接的内容" linktype="text" imgurl="" imgdata="null" tab="innerlink" data-linktype="1"><span data-positionback="static"><img data-ratio="0.21851851851851853" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJxAf6GXtrRDqibwbERric0XVe6wj8bPRuDWFLogiaIc3owG0hvj5f48QcA/640?wx_fmt=png" data-type="png" data-w="1080" src="https://mmbiz.qpic.cn/mmbiz_png/Psho9dm7oDF7dRuk86RJ3icZGT7TZkdBJxAf6GXtrRDqibwbERric0XVe6wj8bPRuDWFLogiaIc3owG0hvj5f48QcA/640?wx_fmt=png"></span></a></p><p><br></p><section><section powered-by="xiumi.us"><section><section><br></section></section></section></section><section><section powered-by="xiumi.us"><section><section powered-by="xiumi.us"><section><img data-ratio="0.125" data-src="https://mmbiz.qpic.cn/mmbiz_gif/Psho9dm7oDHHMXQ2IicFvJwssWxgWhKuK7ulQVyw7gPTxZia00vCxia2vzhRH6pGq8t1FN1zY48ibULAEZpic41k6eg/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1" data-type="gif" data-w="640" src="https://mmbiz.qpic.cn/mmbiz_gif/Psho9dm7oDHHMXQ2IicFvJwssWxgWhKuK7ulQVyw7gPTxZia00vCxia2vzhRH6pGq8t1FN1zY48ibULAEZpic41k6eg/640?wx_fmt=gif&amp;wxfrom=5&amp;wx_lazy=1"></section></section><section powered-by="xiumi.us"><p><strong>#<span>投 稿 通 道</span>#</strong></p></section><section powered-by="xiumi.us"><section><p><strong><span> 让你的文字被更多人看到 </span></strong></p></section></section><section powered-by="xiumi.us"><p><br></p></section><section powered-by="xiumi.us"><section><section><svg viewbox="0 0 1 1"></svg></section></section></section><section powered-by="xiumi.us"><p><br></p></section><section powered-by="xiumi.us"><p><span>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？<strong>答案就是：你不认识的人。</strong></span></p><p><br></p><p><span>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。 </span></p><p><br></p><p><span>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<strong>最新论文解读</strong>，也可以是<strong>学术热点剖析</strong>、<strong>科研心得</strong>或<strong>竞赛经验讲解</strong>等。我们的目的只有一个，让知识真正流动起来。</span></p><p><br></p><p><span>📝 <span><strong>稿件基本要求：</strong></span></span></p><p><span>• 文章确系个人<strong>原创作品</strong>，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注 </span></p><p><span>• 稿件建议以 <strong>markdown</strong> 格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题</span></p><p><span>• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供<strong>业内具有竞争力稿酬</strong>，具体依据文章阅读量和文章质量阶梯制结算</span></p><p><br></p><p><span>📬 <span><strong>投稿通道：</strong></span></span></p><p><span>• 投稿邮箱：</span><span>hr@paperweekly.site </span></p><p><span>• 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者</span></p><p><span>• 您也可以直接添加小编微信（<strong>pwbot02</strong>）快速投稿，备注：姓名-投稿</span></p><p><br></p><p><img data-ratio="0.9888641425389755" data-s="300,640" data-src="https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmic1CRCSOKfDibC3dZ4BaJuYyYTWJyw8gFxqon34STk3icf9aJbY4rqMpmhNjTGJXIGGFsCdTBHy3Tw/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" data-type="png" data-w="898" src="https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgmic1CRCSOKfDibC3dZ4BaJuYyYTWJyw8gFxqon34STk3icf9aJbY4rqMpmhNjTGJXIGGFsCdTBHy3Tw/640?wx_fmt=png&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1"></p><p><strong><span>△长按添加PaperWeekly小编</span></strong></p><p><br></p></section></section></section></section><section><br></section><section><br></section><section><span>🔍</span></section><section><br></section><section><span>现在，在<strong>「知乎」</strong>也能找到我们了</span></section><section><span>进入知乎首页搜索<strong>「PaperWeekly」</strong></span></section><section><span>点击<strong>「关注」</strong>订阅我们的专栏吧</span></section><p><br></p><section>·</section><section data-copyright="135编辑器"><section><section data-id="63" data-plugin="officialaccountcard" data-template="%7B%22block_id%22%3A1627203151678%2C%22padding%22%3A%7B%22top%22%3A0%2C%22bottom%22%3A0%2C%22left%22%3A0%2C%22right%22%3A0%7D%2C%22margin%22%3A%7B%22top%22%3A0%2C%22bottom%22%3A0%2C%22left%22%3A10%2C%22right%22%3A10%7D%2C%22plugin%22%3A%22officialaccountcard%22%2C%22template%22%3A%7B%22cate_id%22%3A%221%22%2C%22sub_cate_id%22%3A%22%2C13%2C%22%2C%22id%22%3A%2263%22%2C%22name%22%3A%22%E5%85%AC%E4%BC%97%E5%8F%B7%E5%90%8D%E7%89%87%22%7D%2C%22custom%22%3A%7B%22imgs%22%3A%5B%7B%22u%22%3A%22https%3A%2F%2F135editor.cdn.bcebos.com%2Ffiles%2Fusers%2F330%2F3304508%2F202107%2FYzEbV2DZ_IXbx.png%22%2C%22w%22%3A1280%2C%22h%22%3A1111%7D%5D%2C%22html%22%3A%22%3Cmpprofile%20class%3D%5C%22js_uneditable%20custom_select_card%20mp_profile_iframe%5C%22%20data-pluginname%3D%5C%22mpprofile%5C%22%20data-id%3D%5C%22MzIwMTc4ODE0Mw%3D%3D%5C%22%20data-headimg%3D%5C%22http%3A%2F%2Fimage2.135editor.com%2Fcache%2Fremote%2FaHR0cHM6Ly9tbWJpei5xbG9nby5jbi9tbWJpel9wbmcvVkJjRDAyakZoZ2tsT3NKS2ZOS0NZRkNpYUJPalZpYWFyaWIzNTJ2amRRYzJ2dmNWN0JFaWNkRXNaSkVvblRrZVpNc3FoM254MnMxTnpBVW1zUk5ITTdPZzNRLzA%2Fd3hfZm10PXBuZw%3D%3D%5C%22%20data-nickname%3D%5C%22PaperWeekly%5C%22%20data-alias%3D%5C%22paperweekly%5C%22%20data-signature%3D%5C%22PaperWeekly%E6%98%AF%E4%B8%80%E4%B8%AA%E6%8E%A8%E8%8D%90%E3%80%81%E8%A7%A3%E8%AF%BB%E3%80%81%E8%AE%A8%E8%AE%BA%E5%92%8C%E6%8A%A5%E9%81%93%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87%E6%88%90%E6%9E%9C%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%B9%B3%E5%8F%B0%EF%BC%8C%E8%87%B4%E5%8A%9B%E4%BA%8E%E8%AE%A9%E5%9B%BD%E5%86%85%E5%A4%96%E4%BC%98%E7%A7%80%E7%A7%91%E7%A0%94%E5%B7%A5%E4%BD%9C%E5%BE%97%E5%88%B0%E6%9B%B4%E4%B8%BA%E5%B9%BF%E6%B3%9B%E7%9A%84%E4%BC%A0%E6%92%AD%E5%92%8C%E8%AE%A4%E5%8F%AF%E3%80%82%E7%A4%BE%E5%8C%BA%EF%BC%9Ahttp%3A%2F%2Fpaperweek.ly%20%7C%20%E5%BE%AE%E5%8D%9A%EF%BC%9A%40PaperWeekly%5C%22%20data-from%3D%5C%220%5C%22%3E%3C%2Fmpprofile%3E%22%2C%22id%22%3A%22MzIwMTc4ODE0Mw%3D%3D%22%2C%22nickname%22%3A%22PaperWeekly%22%2C%22headimg%22%3A%22http%3A%2F%2Fimage2.135editor.com%2Fcache%2Fremote%2FaHR0cHM6Ly9tbWJpei5xbG9nby5jbi9tbWJpel9wbmcvVkJjRDAyakZoZ2tsT3NKS2ZOS0NZRkNpYUJPalZpYWFyaWIzNTJ2amRRYzJ2dmNWN0JFaWNkRXNaSkVvblRrZVpNc3FoM254MnMxTnpBVW1zUk5ITTdPZzNRLzA%2Fd3hfZm10PXBuZw%3D%3D%22%2C%22alias%22%3A%22paperweekly%22%2C%22signature%22%3A%22PaperWeekly%E6%98%AF%E4%B8%80%E4%B8%AA%E6%8E%A8%E8%8D%90%E3%80%81%E8%A7%A3%E8%AF%BB%E3%80%81%E8%AE%A8%E8%AE%BA%E5%92%8C%E6%8A%A5%E9%81%93%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%89%8D%E6%B2%BF%E8%AE%BA%E6%96%87%E6%88%90%E6%9E%9C%E7%9A%84%E5%AD%A6%E6%9C%AF%E5%B9%B3%E5%8F%B0%EF%BC%8C%E8%87%B4%E5%8A%9B%E4%BA%8E%E8%AE%A9%E5%9B%BD%E5%86%85%E5%A4%96%E4%BC%98%E7%A7%80%E7%A7%91%E7%A0%94%E5%B7%A5%E4%BD%9C%E5%BE%97%E5%88%B0%E6%9B%B4%E4%B8%BA%E5%B9%BF%E6%B3%9B%E7%9A%84%E4%BC%A0%E6%92%AD%E5%92%8C%E8%AE%A4%E5%8F%AF%E3%80%82%E7%A4%BE%E5%8C%BA%EF%BC%9Ahttp%3A%2F%2Fpaperweek.ly%20%7C%20%E5%BE%AE%E5%8D%9A%EF%BC%9A%40PaperWeekly%22%7D%7D" data-copyright="135编辑器"><section data-inner-id="63" data-inner-name="135editor-officialaccountcard"><section><section><section><section><mp-common-profile data-pluginname="mpprofile" data-id="MzIwMTc4ODE0Mw==" data-headimg="http://image2.135editor.com/cache/remote/aHR0cHM6Ly9tbWJpei5xbG9nby5jbi9tbWJpel9wbmcvVkJjRDAyakZoZ2tsT3NKS2ZOS0NZRkNpYUJPalZpYWFyaWIzNTJ2amRRYzJ2dmNWN0JFaWNkRXNaSkVvblRrZVpNc3FoM254MnMxTnpBVW1zUk5ITTdPZzNRLzA/d3hfZm10PXBuZw==" data-nickname="PaperWeekly" data-alias="paperweekly" data-signature="PaperWeekly是一个推荐、解读、讨论和报道人工智能前沿论文成果的学术平台，致力于让国内外优秀科研工作得到更为广泛的传播和认可。社区：http://paperweek.ly | 微博：@PaperWeekly"></mp-common-profile></section></section></section><section><section><svg viewbox="0 0 1280 1111" vsersion="3P3UV0QUR4508" xml=""></svg></section></section></section></section></section></section></section><section><br></section><section><img data-backh="61" data-backw="457" data-ratio="0.13333333333333333" data-type="jpeg" data-w="750" data-src="https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnZ3nlEAOI3MyTd7jqeD6cq8uTbkM2xZNpribyNr9liaPJ722zaHxd0YpQvib2nxOYmWibydCVY7W94ew/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" src="https://mmbiz.qpic.cn/mmbiz_png/VBcD02jFhgnZ3nlEAOI3MyTd7jqeD6cq8uTbkM2xZNpribyNr9liaPJ722zaHxd0YpQvib2nxOYmWibydCVY7W94ew/640?wx_fmt=jpeg&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1"></section></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/IBqBl6XROplWz5ECc0wTaA",target="_blank" rel="noopener noreferrer">原文链接</a>
