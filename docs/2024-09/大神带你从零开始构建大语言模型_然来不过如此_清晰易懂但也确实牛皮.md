---
title: "大神带你从零开始构建大语言模型，然来不过如此，清晰易懂但也确实牛皮"
date: 2024-09-01T00:57:49Z
draft: ["false"]
tags: [
  "fetched",
  "老码沉思录"
]
categories: ["Acdemic"]
---
大神带你从零开始构建大语言模型，然来不过如此，清晰易懂但也确实牛皮 by 老码沉思录
------
<div><br><p>当我们谈论大语言模型（LLM）时，可能会让人觉得高深莫测，仿佛只有专家才能涉足。然而，事实并非如此。</p><p>我在 GitHub 上看到了一个特别牛逼的项目，地址在此：</p><p>https://github.com/rasbt/LLMs-from-scratch</p><p>目前 star 25K！而且Sebastian Raschka大神还出书了</p><figure><img data-imgfileid="100001676" data-ratio="1.2531328320802004" data-src="https://mmbiz.qpic.cn/mmbiz_png/oXqG8ETvAemaicqmDfdLWCrheDSiav5jPS0dhfoVXqPY5WTGy0DCQFjaxSPHyDufz5G8aS6unjwLtd0FDMq8oGRQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1197" title="null" src="https://mmbiz.qpic.cn/mmbiz_png/oXqG8ETvAemaicqmDfdLWCrheDSiav5jPS0dhfoVXqPY5WTGy0DCQFjaxSPHyDufz5G8aS6unjwLtd0FDMq8oGRQ/640?wx_fmt=png&amp;from=appmsg"></figure><p>Sebastian Raschka 是一位在机器学习和数据科学领域非常知名的专家。他目前是人工智能领域的教授，同时也是一位在业界颇有影响力的开发者和教育者。他拥有丰富的学术背景，特别是在深度学习和大规模机器学习系统方面有深入研究。此外，他还是一位高产的作家和开源项目贡献者。</p><p>今天，我就来和你聊聊如何在短短三小时内，从零开始构建一个属于你自己的大语言模型。别担心，这不是什么遥不可及的任务，按照项目中的文章章节，你一步步走过来，跟搭积木一样简单。然后不要担心，代码都是只可可用的。还有不需要机器有 GPU，如果有的话更加好，代码会自动识别，抓住这次机会，这也许是离你自己构建一个大模型最近的一次体验机会。</p><h3>你和AI的距离，其实没那么远</h3><p>也许你已经听过很多关于大语言模型的神话，比如“训练一个大语言模型需要海量的数据和昂贵的算力”之类的传闻。作为一个开发者，你可能会想，这种事情是不是太遥远了？我想告诉你，其实并没有那么复杂。就像你第一次学写代码一样，构建大语言模型的过程，也是可以一步步来，慢慢积累的。今天，我们就从最基础的代码开始，一步步搭建出一个属于你的大语言模型。</p><h4>从基础概念开始——了解大语言模型的工作原理</h4><p>在开始写代码之前，咱们先得弄明白什么是大语言模型。简单来说，大语言模型就是通过大量的文本数据来“训练”一个模型，让它能够理解和生成自然语言。<strong>这个模型的核心就是所谓的“神经网络”，它通过学习大量的语料库，不断调整内部参数，最终能输出比较靠谱的文本内容</strong>。</p><p>如果你把大语言模型比作一个刚开始学说话的孩子，那些庞大的文本数据就是教科书。通过不断“阅读”，孩子逐渐掌握了如何说话，并且还能在不同的情境下说出合适的话。这就是大语言模型的基本原理。</p><figure><img data-imgfileid="100001674" data-ratio="0.4459745762711864" data-src="https://mmbiz.qpic.cn/mmbiz_png/oXqG8ETvAemaicqmDfdLWCrheDSiav5jPSNkYvlOKYAfYknLkcjcajsLBic2fWzg3CuMqBcp709FZKF7fUkYDEPqQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1888" title="null" src="https://mmbiz.qpic.cn/mmbiz_png/oXqG8ETvAemaicqmDfdLWCrheDSiav5jPSNkYvlOKYAfYknLkcjcajsLBic2fWzg3CuMqBcp709FZKF7fUkYDEPqQ/640?wx_fmt=png&amp;from=appmsg"></figure><h4>逐步搭建——从简单到复杂的构建过程</h4><p>现在，你可能会问：“知道了原理，我怎么开始开车呢？”其实，就像我们学任何新东西一样，最好的方法就是从简单的开始。你不需要一上来就构建一个和GPT-4一样复杂的模型。我们可以从一个简单的、只有几层的神经网络开始，慢慢增加复杂度。</p><p>比如说，最基础的代码可能就是这样：</p><pre><span></span><code><span>import</span> torch<br><span>import</span> torch.nn <span>as</span> nn<br><br><span>class</span> <span>SimpleNN</span>(nn.Module):<br>    <span>def</span> <span>__init__</span>(<span>self</span>):<br>        <span>super</span>(SimpleNN, <span>self</span>).__init__()<br>        <span>self</span>.fc1 = nn.Linear(<span>10</span>, <span>50</span>)<br>        <span>self</span>.fc2 = nn.Linear(<span>50</span>, <span>10</span>)<br><br>    <span>def</span> <span>forward</span>(<span>self, x</span>):<br>        x = torch.relu(<span>self</span>.fc1(x))<br>        x = <span>self</span>.fc2(x)<br>        <span>return</span> x<br><br>model = SimpleNN()</code></pre><p>当然GitHub 中的项目中不是这么简单，这里为了尽可能简单的解释而已！OK，这是一个非常简单的神经网络结构，它有两层全连接层，用于处理输入和输出。虽然看起来简单，但这就是大部分神经网络的基本构成。随着你对模型理解的深入，你可以在这个基础上添加更多层、调整参数，使模型变得更强大。</p><figure><img data-imgfileid="100001675" data-ratio="1.0200308166409862" data-src="https://mmbiz.qpic.cn/mmbiz_png/oXqG8ETvAemaicqmDfdLWCrheDSiav5jPSaofdbwicAYUNchh5oWsYRHCFnuSCvng5OgEL1lTtxBuqrH6aM0A0zaw/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1298" title="null" src="https://mmbiz.qpic.cn/mmbiz_png/oXqG8ETvAemaicqmDfdLWCrheDSiav5jPSaofdbwicAYUNchh5oWsYRHCFnuSCvng5OgEL1lTtxBuqrH6aM0A0zaw/640?wx_fmt=png&amp;from=appmsg"></figure><h4>数据的力量——如何选择和处理训练数据</h4><p>有了模型结构，接下来我们需要“喂”模型数据，让它“学会”我们想要的技能。数据的选择和处理非常重要，因为模型的表现好坏，往往取决于你给它的“食物”——也就是训练数据。</p><p>在选择数据时，<strong>你可以从公开的文本数据集开始，比如Wikipedia的文章、新闻报道或者技术文档</strong>。这些数据可以让模型学习到广泛的语言知识。不过，数据并不是越多越好，我们还需要对数据进行清洗，去掉那些无意义或者噪声太多的内容，这样模型才能更高效地学习。</p><p>数据处理的代码可能会长这样：</p><pre><span></span><code><span>import</span> re<br><br><span>def</span> <span>clean_text</span>(<span>text</span>):<br>    text = re.sub(<span>r'\s+'</span>, <span>' '</span>, text)<br>    text = re.sub(<span>r'[^\w\s]'</span>, <span>''</span>, text)<br>    <span>return</span> text.lower()<br><br>cleaned_text = clean_text(<span>"Hello World! This is an example text."</span>)</code></pre><p>这里我们做了一些简单的文本清洗操作，比如去掉多余的空格和标点符号，并将所有字符转换为小写。这些小操作能显著提高训练效果。</p><figure><img data-imgfileid="100001678" data-ratio="0.6348993288590604" data-src="https://mmbiz.qpic.cn/mmbiz_png/oXqG8ETvAemaicqmDfdLWCrheDSiav5jPSJKbwYbQ9AvWibE5HxXIWjiaYGymOfTsGiajjiak2VQxric8t0iauzY0OCm4Q/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1490" title="null" src="https://mmbiz.qpic.cn/mmbiz_png/oXqG8ETvAemaicqmDfdLWCrheDSiav5jPSJKbwYbQ9AvWibE5HxXIWjiaYGymOfTsGiajjiak2VQxric8t0iauzY0OCm4Q/640?wx_fmt=png&amp;from=appmsg"></figure><h4>迭代和优化——如何不断改进你的模型</h4><p>训练模型是一个不断迭代的过程。就像写代码调试一样，你需要通过多次实验，找出最适合你任务的模型架构和参数设置。这个过程中，你会遇到很多问题，比如模型过拟合、训练时间过长等等。但这些都不是问题，解决的过程反而会让你对模型理解得更深。</p><p>举个例子，如果你发现模型在训练数据上表现很好，但在新数据上表现糟糕，这可能是因为模型过拟合了。解决这个问题的方法有很多，比如使用正则化技术，或者增加训练数据的多样性。</p><p>优化模型的代码示例：</p><pre><span></span><code>optimizer = torch.optim.Adam(model.parameters(), lr=<span>0.001</span>)<br>loss_fn = nn.CrossEntropyLoss()<br><br><span>for</span> epoch <span>in</span> <span>range</span>(<span>100</span>):<br>    optimizer.zero_grad()<br>    outputs = model(inputs)<br>    loss = loss_fn(outputs, labels)<br>    loss.backward()<br>    optimizer.step()</code></pre><p>通过调整学习率（lr）、优化器的选择以及损失函数的定义，你可以不断改进模型的性能。</p><figure><img data-imgfileid="100001680" data-ratio="0.6786114221724524" data-src="https://mmbiz.qpic.cn/mmbiz_png/oXqG8ETvAemaicqmDfdLWCrheDSiav5jPSdWqCCH7MGL8U5t8qBLFtSicHA1gtrqqG9fyh3AHKu5VeomDyTaEr3DQ/640?wx_fmt=png&amp;from=appmsg" data-type="png" data-w="1786" title="null" src="https://mmbiz.qpic.cn/mmbiz_png/oXqG8ETvAemaicqmDfdLWCrheDSiav5jPSdWqCCH7MGL8U5t8qBLFtSicHA1gtrqqG9fyh3AHKu5VeomDyTaEr3DQ/640?wx_fmt=png&amp;from=appmsg"></figure><h3>额外思考</h3><p>构建大语言模型听起来可能很复杂，但只要你从基础开始，一步步来，你一定能成功。通过理解基本原理、逐步搭建模型、选择和处理数据、以及不断优化迭代，你就能在短短三小时内完成一个基础的大语言模型构建。下次，当别人提起大语言模型时，你可以自信地告诉他们：“自己做一个，好像也并不是什么难事！”，然后做出来效果不 OK，在补一句，“又不是不能用！”</p><p><br></p><p><mp-style-type data-value="3"></mp-style-type></p></div>  
<hr>
<a href="https://mp.weixin.qq.com/s/RR-urlj1aEgLIe3jnFhHdg",target="_blank" rel="noopener noreferrer">原文链接</a>
